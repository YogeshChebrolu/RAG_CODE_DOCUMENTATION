{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (0.3.21)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from langchain) (0.3.49)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from langchain) (0.3.7)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from langchain) (0.3.19)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from langchain) (2.11.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from langchain) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (4.13.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.0 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain) (3.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yogesh\\Documents\\Documentation RAG\\notebooks\\__file__\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.abspath(\"__file__\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\Yogesh\\Documents\\Documentation RAG\\Documents\\pydantic_processed2.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    content = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = content.split(\"Page \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'28 ---\\n\\n*  function  \\n  *  FunctionModel  \\n    *  __init__  \\n    *  model_name  \\n    *  system  \\n  *  AgentInfo  \\n    *  function_tools  \\n    *  allow_text_result  \\n    *  result_tools  \\n    *  model_settings  \\n  *  DeltaToolCall  \\n    *  name  \\n    *  json_args  \\n  *  DeltaToolCalls  \\n  *  FunctionDef  \\n  *  StreamFunctionDef  \\n  *  FunctionStreamedResponse  \\n    *  model_name  \\n    *  timestamp  \\n\\n  1.  Introduction  \\n  2.  API Reference  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# `pydantic_ai.models.function`\\nA model controlled by a local function.\\n`FunctionModel` is similar to `TestModel`, but allows greater control over the model\\'s behavior.\\nIts primary use case is for more advanced unit testing than is possible with `TestModel`.\\nHere\\'s a minimal example:\\nfunction_model_usage.py```\\nfrompydantic_aiimport Agent\\nfrompydantic_ai.messagesimport ModelMessage, ModelResponse, TextPart\\nfrompydantic_ai.models.functionimport FunctionModel, AgentInfo\\nmy_agent = Agent(\\'openai:gpt-4o\\')\\n\\nasync defmodel_function(\\n  messages: list[ModelMessage], info: AgentInfo\\n) -> ModelResponse:\\n  print(messages)\\n\"\"\"\\n  [\\n    ModelRequest(\\n      parts=[\\n        UserPromptPart(\\n          content=\\'Testing my agent...\\',\\n          timestamp=datetime.datetime(...),\\n          part_kind=\\'user-prompt\\',\\n        )\\n      ],\\n      kind=\\'request\\',\\n    )\\n  ]\\n  \"\"\"\\n  print(info)\\n\"\"\"\\n  AgentInfo(\\n    function_tools=[], allow_text_result=True, result_tools=[], model_settings=None\\n  )\\n  \"\"\"\\n  return ModelResponse(parts=[TextPart(\\'hello world\\')])\\n\\nasync deftest_my_agent():\\n\"\"\"Unit test for my_agent, to be run by pytest.\"\"\"\\n  with my_agent.override(model=FunctionModel(model_function)):\\n    result = await my_agent.run(\\'Testing my agent...\\')\\n    assert result.data == \\'hello world\\'\\n\\n```\\n\\nSee Unit testing with `FunctionModel` for detailed documentation.\\n###  FunctionModel `dataclass`\\nBases: `Model`\\nA model controlled by a local function.\\nApart from `__init__`, all methods are private or match those of the base class.\\nSource code in `pydantic_ai_slim/pydantic_ai/models/function.py`\\n```\\n\\n```\\n| ```\\n@dataclass(init=False)\\nclassFunctionModel(Model):\\n\"\"\"A model controlled by a local function.\\n  Apart from `__init__`, all methods are private or match those of the base class.\\n  \"\"\"\\n  function: FunctionDef | None = None\\n  stream_function: StreamFunctionDef | None = None\\n  _model_name: str = field(repr=False)\\n  _system: str | None = field(default=None, repr=False)\\n  @overload\\n  def__init__(self, function: FunctionDef) -> None: ...\\n  @overload\\n  def__init__(self, *, stream_function: StreamFunctionDef) -> None: ...\\n  @overload\\n  def__init__(self, function: FunctionDef, *, stream_function: StreamFunctionDef) -> None: ...\\n  def__init__(self, function: FunctionDef | None = None, *, stream_function: StreamFunctionDef | None = None):\\n\"\"\"Initialize a `FunctionModel`.\\n    Either `function` or `stream_function` must be provided, providing both is allowed.\\n    Args:\\n      function: The function to call for non-streamed requests.\\n      stream_function: The function to call for streamed requests.\\n    \"\"\"\\n    if function is None and stream_function is None:\\n      raise TypeError(\\'Either `function` or `stream_function` must be provided\\')\\n    self.function = function\\n    self.stream_function = stream_function\\n    function_name = self.function.__name__ if self.function is not None else \\'\\'\\n    stream_function_name = self.stream_function.__name__ if self.stream_function is not None else \\'\\'\\n    self._model_name = f\\'function:{function_name}:{stream_function_name}\\'\\n  async defrequest(\\n    self,\\n    messages: list[ModelMessage],\\n    model_settings: ModelSettings | None,\\n    model_request_parameters: ModelRequestParameters,\\n  ) -> tuple[ModelResponse, usage.Usage]:\\n    agent_info = AgentInfo(\\n      model_request_parameters.function_tools,\\n      model_request_parameters.allow_text_result,\\n      model_request_parameters.result_tools,\\n      model_settings,\\n    )\\n    assert self.function is not None, \\'FunctionModel must receive a `function` to support non-streamed requests\\'\\n    if inspect.iscoroutinefunction(self.function):\\n      response = await self.function(messages, agent_info)\\n    else:\\n      response_ = await _utils.run_in_executor(self.function, messages, agent_info)\\n      assert isinstance(response_, ModelResponse), response_\\n      response = response_\\n    response.model_name = f\\'function:{self.function.__name__}\\'\\n    # TODO is `messages` right here? Should it just be new messages?\\n    return response, _estimate_usage(chain(messages, [response]))\\n  @asynccontextmanager\\n  async defrequest_stream(\\n    self,\\n    messages: list[ModelMessage],\\n    model_settings: ModelSettings | None,\\n    model_request_parameters: ModelRequestParameters,\\n  ) -> AsyncIterator[StreamedResponse]:\\n    agent_info = AgentInfo(\\n      model_request_parameters.function_tools,\\n      model_request_parameters.allow_text_result,\\n      model_request_parameters.result_tools,\\n      model_settings,\\n    )\\n    assert self.stream_function is not None, (\\n      \\'FunctionModel must receive a `stream_function` to support streamed requests\\'\\n    )\\n    response_stream = PeekableAsyncStream(self.stream_function(messages, agent_info))\\n    first = await response_stream.peek()\\n    if isinstance(first, _utils.Unset):\\n      raise ValueError(\\'Stream function must return at least one item\\')\\n    yield FunctionStreamedResponse(_model_name=f\\'function:{self.stream_function.__name__}\\', _iter=response_stream)\\n  @property\\n  defmodel_name(self) -> str:\\n\"\"\"The model name.\"\"\"\\n    return self._model_name\\n  @property\\n  defsystem(self) -> str | None:\\n\"\"\"The system / model provider.\"\"\"\\n    return self._system\\n\\n```\\n\\n---|---  \\n####  __init__\\n```\\n__init__(function: FunctionDef) -> None\\n\\n```\\n\\n```\\n__init__(*, stream_function: StreamFunctionDef) -> None\\n\\n```\\n\\n```\\n__init__(\\n  function: FunctionDef,\\n  *,\\n  stream_function: StreamFunctionDef\\n) -> None\\n\\n```\\n\\n```\\n__init__(\\n  function: FunctionDef | None = None,\\n  *,\\n  stream_function: StreamFunctionDef | None = None\\n)\\n\\n```\\n\\nInitialize a `FunctionModel`.\\nEither `function` or `stream_function` must be provided, providing both is allowed.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`function` |  `FunctionDef | None` |  The function to call for non-streamed requests. |  `None`  \\n`stream_function` |  `StreamFunctionDef | None` |  The function to call for streamed requests. |  `None`  \\nSource code in `pydantic_ai_slim/pydantic_ai/models/function.py`\\n```\\n\\n```\\n| ```\\ndef__init__(self, function: FunctionDef | None = None, *, stream_function: StreamFunctionDef | None = None):\\n\"\"\"Initialize a `FunctionModel`.\\n  Either `function` or `stream_function` must be provided, providing both is allowed.\\n  Args:\\n    function: The function to call for non-streamed requests.\\n    stream_function: The function to call for streamed requests.\\n  \"\"\"\\n  if function is None and stream_function is None:\\n    raise TypeError(\\'Either `function` or `stream_function` must be provided\\')\\n  self.function = function\\n  self.stream_function = stream_function\\n  function_name = self.function.__name__ if self.function is not None else \\'\\'\\n  stream_function_name = self.stream_function.__name__ if self.stream_function is not None else \\'\\'\\n  self._model_name = f\\'function:{function_name}:{stream_function_name}\\'\\n\\n```\\n\\n---|---  \\n####  model_name `property`\\n```\\nmodel_name: str\\n\\n```\\n\\nThe model name.\\n####  system `property`\\n```\\nsystem: str | None\\n\\n```\\n\\nThe system / model provider.\\n###  AgentInfo `dataclass`\\nInformation about an agent.\\nThis is passed as the second to functions used within `FunctionModel`.\\nSource code in `pydantic_ai_slim/pydantic_ai/models/function.py`\\n```\\n\\n```\\n| ```\\n@dataclass(frozen=True)\\nclassAgentInfo:\\n\"\"\"Information about an agent.\\n  This is passed as the second to functions used within [`FunctionModel`][pydantic_ai.models.function.FunctionModel].\\n  \"\"\"\\n  function_tools: list[ToolDefinition]\\n\"\"\"The function tools available on this agent.\\n  These are the tools registered via the [`tool`][pydantic_ai.Agent.tool] and\\n  [`tool_plain`][pydantic_ai.Agent.tool_plain] decorators.\\n  \"\"\"\\n  allow_text_result: bool\\n\"\"\"Whether a plain text result is allowed.\"\"\"\\n  result_tools: list[ToolDefinition]\\n\"\"\"The tools that can called as the final result of the run.\"\"\"\\n  model_settings: ModelSettings | None\\n\"\"\"The model settings passed to the run call.\"\"\"\\n\\n```\\n\\n---|---  \\n####  function_tools `instance-attribute`\\n```\\nfunction_tools: listToolDefinition[]\\n\\n```\\n\\nThe function tools available on this agent.\\nThese are the tools registered via the `tool` and `tool_plain` decorators.\\n####  allow_text_result `instance-attribute`\\n```\\nallow_text_result: bool\\n\\n```\\n\\nWhether a plain text result is allowed.\\n####  result_tools `instance-attribute`\\n```\\nresult_tools: listToolDefinition[]\\n\\n```\\n\\nThe tools that can called as the final result of the run.\\n####  model_settings `instance-attribute`\\n```\\nmodel_settings: ModelSettings | None\\n\\n```\\n\\nThe model settings passed to the run call.\\n###  DeltaToolCall `dataclass`\\nIncremental change to a tool call.\\nUsed to describe a chunk when streaming structured responses.\\nSource code in `pydantic_ai_slim/pydantic_ai/models/function.py`\\n```\\n\\n```\\n| ```\\n@dataclass\\nclassDeltaToolCall:\\n\"\"\"Incremental change to a tool call.\\n  Used to describe a chunk when streaming structured responses.\\n  \"\"\"\\n  name: str | None = None\\n\"\"\"Incremental change to the name of the tool.\"\"\"\\n  json_args: str | None = None\\n\"\"\"Incremental change to the arguments as JSON\"\"\"\\n\\n```\\n\\n---|---  \\n####  name `class-attribute` `instance-attribute`\\n```\\nname: str | None = None\\n\\n```\\n\\nIncremental change to the name of the tool.\\n####  json_args `class-attribute` `instance-attribute`\\n```\\njson_args: str | None = None\\n\\n```\\n\\nIncremental change to the arguments as JSON\\n###  DeltaToolCalls `module-attribute`\\n```\\nDeltaToolCalls: TypeAlias = dictint[, DeltaToolCall]\\n\\n```\\n\\nA mapping of tool call IDs to incremental changes.\\n###  FunctionDef `module-attribute`\\n```\\nFunctionDef: TypeAlias = Callable[\\n  list[ModelMessage[], AgentInfo],\\n  UnionModelResponse[, AwaitableModelResponse[]],\\n]\\n\\n```\\n\\nA function used to generate a non-streamed response.\\n###  StreamFunctionDef `module-attribute`\\n```\\nStreamFunctionDef: TypeAlias = Callable[\\n  list[ModelMessage[], AgentInfo],\\n  AsyncIteratorUnion[str[, DeltaToolCalls]],\\n]\\n\\n```\\n\\nA function used to generate a streamed response.\\nWhile this is defined as having return type of `AsyncIterator[Union[str, DeltaToolCalls]]`, it should really be considered as `Union[AsyncIterator[str], AsyncIterator[DeltaToolCalls]`,\\nE.g. you need to yield all text or all `DeltaToolCalls`, not mix them.\\n###  FunctionStreamedResponse `dataclass`\\nBases: `StreamedResponse`\\nImplementation of `StreamedResponse` for FunctionModel.\\nSource code in `pydantic_ai_slim/pydantic_ai/models/function.py`\\n```\\n\\n```\\n| ```\\n@dataclass\\nclassFunctionStreamedResponse(StreamedResponse):\\n\"\"\"Implementation of `StreamedResponse` for [FunctionModel][pydantic_ai.models.function.FunctionModel].\"\"\"\\n  _model_name: str\\n  _iter: AsyncIterator[str | DeltaToolCalls]\\n  _timestamp: datetime = field(default_factory=_utils.now_utc)\\n  def__post_init__(self):\\n    self._usage += _estimate_usage([])\\n  async def_get_event_iterator(self) -> AsyncIterator[ModelResponseStreamEvent]:\\n    async for item in self._iter:\\n      if isinstance(item, str):\\n        response_tokens = _estimate_string_tokens(item)\\n        self._usage += usage.Usage(response_tokens=response_tokens, total_tokens=response_tokens)\\n        yield self._parts_manager.handle_text_delta(vendor_part_id=\\'content\\', content=item)\\n      else:\\n        delta_tool_calls = item\\n        for dtc_index, delta_tool_call in delta_tool_calls.items():\\n          if delta_tool_call.json_args:\\n            response_tokens = _estimate_string_tokens(delta_tool_call.json_args)\\n            self._usage += usage.Usage(response_tokens=response_tokens, total_tokens=response_tokens)\\n          maybe_event = self._parts_manager.handle_tool_call_delta(\\n            vendor_part_id=dtc_index,\\n            tool_name=delta_tool_call.name,\\n            args=delta_tool_call.json_args,\\n            tool_call_id=None,\\n          )\\n          if maybe_event is not None:\\n            yield maybe_event\\n  @property\\n  defmodel_name(self) -> str:\\n\"\"\"Get the model name of the response.\"\"\"\\n    return self._model_name\\n  @property\\n  deftimestamp(self) -> datetime:\\n\"\"\"Get the timestamp of the response.\"\"\"\\n    return self._timestamp\\n\\n```\\n\\n---|---  \\n####  model_name `property`\\n```\\nmodel_name: str\\n\\n```\\n\\nGet the model name of the response.\\n####  timestamp `property`\\n```\\ntimestamp: datetime\\n\\n```\\n\\nGet the timestamp of the response.\\n© Pydantic Services Inc. 2024 to present\\n\\n--- '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'38 ---\\n\\n*  nodes  \\n  *  GraphRunContext  \\n    *  state  \\n    *  deps  \\n  *  BaseNode  \\n    *  docstring_notes  \\n    *  run  \\n    *  get_id  \\n    *  get_note  \\n    *  get_node_def  \\n  *  End  \\n    *  data  \\n  *  Edge  \\n    *  label  \\n  *  DepsT  \\n  *  RunEndT  \\n  *  NodeRunEndT  \\n\\n  1.  Introduction  \\n  2.  API Reference  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# `pydantic_graph.nodes`\\n###  GraphRunContext `dataclass`\\nBases: `GenericStateT[, DepsT]`\\nContext for a graph.\\nSource code in `pydantic_graph/pydantic_graph/nodes.py`\\n```\\n\\n```\\n| ```\\n@dataclass\\nclassGraphRunContext(Generic[StateT, DepsT]):\\n\"\"\"Context for a graph.\"\"\"\\n  # TODO: Can we get rid of this struct and just pass both these things around..?\\n  state: StateT\\n\"\"\"The state of the graph.\"\"\"\\n  deps: DepsT\\n\"\"\"Dependencies for the graph.\"\"\"\\n\\n```\\n\\n---|---  \\n####  state `instance-attribute`\\n```\\nstate: StateT\\n\\n```\\n\\nThe state of the graph.\\n####  deps `instance-attribute`\\n```\\ndeps: DepsT\\n\\n```\\n\\nDependencies for the graph.\\n###  BaseNode\\nBases: `ABC`, `GenericStateT[, DepsT, NodeRunEndT]`\\nBase class for a node.\\nSource code in `pydantic_graph/pydantic_graph/nodes.py`\\n```\\n\\n```\\n| ```\\nclassBaseNode(ABC, Generic[StateT, DepsT, NodeRunEndT]):\\n\"\"\"Base class for a node.\"\"\"\\n  docstring_notes: ClassVar[bool] = False\\n\"\"\"Set to `True` to generate mermaid diagram notes from the class\\'s docstring.\\n  While this can add valuable information to the diagram, it can make diagrams harder to view, hence\\n  it is disabled by default. You can also customise notes overriding the\\n  [`get_note`][pydantic_graph.nodes.BaseNode.get_note] method.\\n  \"\"\"\\n  @abstractmethod\\n  async defrun(self, ctx: GraphRunContext[StateT, DepsT]) -> BaseNode[StateT, DepsT, Any] | End[NodeRunEndT]:\\n\"\"\"Run the node.\\n    This is an abstract method that must be implemented by subclasses.\\n    !!! note \"Return types used at runtime\"\\n      The return type of this method are read by `pydantic_graph` at runtime and used to define which\\n      nodes can be called next in the graph. This is displayed in [mermaid diagrams](mermaid.md)\\n      and enforced when running the graph.\\n    Args:\\n      ctx: The graph context.\\n    Returns:\\n      The next node to run or [`End`][pydantic_graph.nodes.End] to signal the end of the graph.\\n    \"\"\"\\n    ...\\n  @classmethod\\n  @cache\\n  defget_id(cls) -> str:\\n\"\"\"Get the ID of the node.\"\"\"\\n    return cls.__name__\\n  @classmethod\\n  defget_note(cls) -> str | None:\\n\"\"\"Get a note about the node to render on mermaid charts.\\n    By default, this returns a note only if [`docstring_notes`][pydantic_graph.nodes.BaseNode.docstring_notes]\\n    is `True`. You can override this method to customise the node notes.\\n    \"\"\"\\n    if not cls.docstring_notes:\\n      return None\\n    docstring = cls.__doc__\\n    # dataclasses get an automatic docstring which is just their signature, we don\\'t want that\\n    if docstring and is_dataclass(cls) and docstring.startswith(f\\'{cls.__name__}(\\'):\\n      docstring = None\\n    if docstring:\\n      # remove indentation from docstring\\n      importinspect\\n      docstring = inspect.cleandoc(docstring)\\n    return docstring\\n  @classmethod\\n  defget_node_def(cls, local_ns: dict[str, Any] | None) -> NodeDef[StateT, DepsT, NodeRunEndT]:\\n\"\"\"Get the node definition.\"\"\"\\n    type_hints = get_type_hints(cls.run, localns=local_ns, include_extras=True)\\n    try:\\n      return_hint = type_hints[\\'return\\']\\n    except KeyError as e:\\n      raise exceptions.GraphSetupError(f\\'Node {cls} is missing a return type hint on its `run` method\\') frome\\n    next_node_edges: dict[str, Edge] = {}\\n    end_edge: Edge | None = None\\n    returns_base_node: bool = False\\n    for return_type in _utils.get_union_args(return_hint):\\n      return_type, annotations = _utils.unpack_annotated(return_type)\\n      edge = next((a for a in annotations if isinstance(a, Edge)), Edge(None))\\n      return_type_origin = get_origin(return_type) or return_type\\n      if return_type_origin is End:\\n        end_edge = edge\\n      elif return_type_origin is BaseNode:\\n        # TODO: Should we disallow this?\\n        returns_base_node = True\\n      elif issubclass(return_type_origin, BaseNode):\\n        next_node_edges[return_type.get_id()] = edge\\n      else:\\n        raise exceptions.GraphSetupError(f\\'Invalid return type: {return_type}\\')\\n    return NodeDef(\\n      cls,\\n      cls.get_id(),\\n      cls.get_note(),\\n      next_node_edges,\\n      end_edge,\\n      returns_base_node,\\n    )\\n\\n```\\n\\n---|---  \\n####  docstring_notes `class-attribute`\\n```\\ndocstring_notes: bool = False\\n\\n```\\n\\nSet to `True` to generate mermaid diagram notes from the class\\'s docstring.\\nWhile this can add valuable information to the diagram, it can make diagrams harder to view, hence it is disabled by default. You can also customise notes overriding the `get_note` method.\\n####  run `abstractmethod` `async`\\n```\\nrun(\\n  ctx: GraphRunContextStateT[, DepsT],\\n) -> BaseNodeStateT[, DepsT, Any] | EndNodeRunEndT[]\\n\\n```\\n\\nRun the node.\\nThis is an abstract method that must be implemented by subclasses.\\nReturn types used at runtime\\nThe return type of this method are read by `pydantic_graph` at runtime and used to define which nodes can be called next in the graph. This is displayed in mermaid diagrams and enforced when running the graph.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`ctx` |  `GraphRunContextStateT[, DepsT]` |  The graph context. |  _required_  \\nReturns:\\nType | Description  \\n---|---  \\n`BaseNodeStateT[, DepsT, Any] | EndNodeRunEndT[]` |  The next node to run or `End` to signal the end of the graph.  \\nSource code in `pydantic_graph/pydantic_graph/nodes.py`\\n```\\n\\n```\\n| ```\\n@abstractmethod\\nasync defrun(self, ctx: GraphRunContext[StateT, DepsT]) -> BaseNode[StateT, DepsT, Any] | End[NodeRunEndT]:\\n\"\"\"Run the node.\\n  This is an abstract method that must be implemented by subclasses.\\n  !!! note \"Return types used at runtime\"\\n    The return type of this method are read by `pydantic_graph` at runtime and used to define which\\n    nodes can be called next in the graph. This is displayed in [mermaid diagrams](mermaid.md)\\n    and enforced when running the graph.\\n  Args:\\n    ctx: The graph context.\\n  Returns:\\n    The next node to run or [`End`][pydantic_graph.nodes.End] to signal the end of the graph.\\n  \"\"\"\\n  ...\\n\\n```\\n\\n---|---  \\n####  get_id `cached` `classmethod`\\n```\\nget_id() -> str\\n\\n```\\n\\nGet the ID of the node.\\nSource code in `pydantic_graph/pydantic_graph/nodes.py`\\n```\\n\\n```\\n| ```\\n@classmethod\\n@cache\\ndefget_id(cls) -> str:\\n\"\"\"Get the ID of the node.\"\"\"\\n  return cls.__name__\\n\\n```\\n\\n---|---  \\n####  get_note `classmethod`\\n```\\nget_note() -> str | None\\n\\n```\\n\\nGet a note about the node to render on mermaid charts.\\nBy default, this returns a note only if `docstring_notes` is `True`. You can override this method to customise the node notes.\\nSource code in `pydantic_graph/pydantic_graph/nodes.py`\\n```\\n\\n```\\n| ```\\n@classmethod\\ndefget_note(cls) -> str | None:\\n\"\"\"Get a note about the node to render on mermaid charts.\\n  By default, this returns a note only if [`docstring_notes`][pydantic_graph.nodes.BaseNode.docstring_notes]\\n  is `True`. You can override this method to customise the node notes.\\n  \"\"\"\\n  if not cls.docstring_notes:\\n    return None\\n  docstring = cls.__doc__\\n  # dataclasses get an automatic docstring which is just their signature, we don\\'t want that\\n  if docstring and is_dataclass(cls) and docstring.startswith(f\\'{cls.__name__}(\\'):\\n    docstring = None\\n  if docstring:\\n    # remove indentation from docstring\\n    importinspect\\n    docstring = inspect.cleandoc(docstring)\\n  return docstring\\n\\n```\\n\\n---|---  \\n####  get_node_def `classmethod`\\n```\\nget_node_def(\\n  local_ns: dictstr[, Any] | None,\\n) -> NodeDefStateT[, DepsT, NodeRunEndT]\\n\\n```\\n\\nGet the node definition.\\nSource code in `pydantic_graph/pydantic_graph/nodes.py`\\n```\\n\\n```\\n| ```\\n@classmethod\\ndefget_node_def(cls, local_ns: dict[str, Any] | None) -> NodeDef[StateT, DepsT, NodeRunEndT]:\\n\"\"\"Get the node definition.\"\"\"\\n  type_hints = get_type_hints(cls.run, localns=local_ns, include_extras=True)\\n  try:\\n    return_hint = type_hints[\\'return\\']\\n  except KeyError as e:\\n    raise exceptions.GraphSetupError(f\\'Node {cls} is missing a return type hint on its `run` method\\') frome\\n  next_node_edges: dict[str, Edge] = {}\\n  end_edge: Edge | None = None\\n  returns_base_node: bool = False\\n  for return_type in _utils.get_union_args(return_hint):\\n    return_type, annotations = _utils.unpack_annotated(return_type)\\n    edge = next((a for a in annotations if isinstance(a, Edge)), Edge(None))\\n    return_type_origin = get_origin(return_type) or return_type\\n    if return_type_origin is End:\\n      end_edge = edge\\n    elif return_type_origin is BaseNode:\\n      # TODO: Should we disallow this?\\n      returns_base_node = True\\n    elif issubclass(return_type_origin, BaseNode):\\n      next_node_edges[return_type.get_id()] = edge\\n    else:\\n      raise exceptions.GraphSetupError(f\\'Invalid return type: {return_type}\\')\\n  return NodeDef(\\n    cls,\\n    cls.get_id(),\\n    cls.get_note(),\\n    next_node_edges,\\n    end_edge,\\n    returns_base_node,\\n  )\\n\\n```\\n\\n---|---  \\n###  End `dataclass`\\nBases: `GenericRunEndT[]`\\nType to return from a node to signal the end of the graph.\\nSource code in `pydantic_graph/pydantic_graph/nodes.py`\\n```\\n\\n```\\n| ```\\n@dataclass\\nclassEnd(Generic[RunEndT]):\\n\"\"\"Type to return from a node to signal the end of the graph.\"\"\"\\n  data: RunEndT\\n\"\"\"Data to return from the graph.\"\"\"\\n\\n```\\n\\n---|---  \\n####  data `instance-attribute`\\n```\\ndata: RunEndT\\n\\n```\\n\\nData to return from the graph.\\n###  Edge `dataclass`\\nAnnotation to apply a label to an edge in a graph.\\nSource code in `pydantic_graph/pydantic_graph/nodes.py`\\n```\\n\\n```\\n| ```\\n@dataclass\\nclassEdge:\\n\"\"\"Annotation to apply a label to an edge in a graph.\"\"\"\\n  label: str | None\\n\"\"\"Label for the edge.\"\"\"\\n\\n```\\n\\n---|---  \\n####  label `instance-attribute`\\n```\\nlabel: str | None\\n\\n```\\n\\nLabel for the edge.\\n###  DepsT `module-attribute`\\n```\\nDepsT = TypeVar(\\'DepsT\\', default=None, contravariant=True)\\n\\n```\\n\\nType variable for the dependencies of a graph and node.\\n###  RunEndT `module-attribute`\\n```\\nRunEndT = TypeVar(\\'RunEndT\\', covariant=True, default=None)\\n\\n```\\n\\nCovariant type variable for the return type of a graph `run`.\\n###  NodeRunEndT `module-attribute`\\n```\\nNodeRunEndT = TypeVar(\\n  \"NodeRunEndT\", covariant=True, default=Never\\n)\\n\\n```\\n\\nCovariant type variable for the return type of a node `run`.\\n© Pydantic Services Inc. 2024 to present\\n\\n--- '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = []\n",
    "for i in content[2:]:\n",
    "    con.append(i[9:])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  nodes  \\n  *  GraphRunContext  \\n    *  state  \\n    *  deps  \\n  *  BaseNode  \\n    *  docstring_notes  \\n    *  run  \\n    *  get_id  \\n    *  get_note  \\n    *  get_node_def  \\n  *  End  \\n    *  data  \\n  *  Edge  \\n    *  label  \\n  *  DepsT  \\n  *  RunEndT  \\n  *  NodeRunEndT  \\n\\n  1.  Introduction  \\n  2.  API Reference  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# `pydantic_graph.nodes`\\n###  GraphRunContext `dataclass`\\nBases: `GenericStateT[, DepsT]`\\nContext for a graph.\\nSource code in `pydantic_graph/pydantic_graph/nodes.py`\\n```\\n\\n```\\n| ```\\n@dataclass\\nclassGraphRunContext(Generic[StateT, DepsT]):\\n\"\"\"Context for a graph.\"\"\"\\n  # TODO: Can we get rid of this struct and just pass both these things around..?\\n  state: StateT\\n\"\"\"The state of the graph.\"\"\"\\n  deps: DepsT\\n\"\"\"Dependencies for the graph.\"\"\"\\n\\n```\\n\\n---|---  \\n####  state `instance-attribute`\\n```\\nstate: StateT\\n\\n```\\n\\nThe state of the graph.\\n####  deps `instance-attribute`\\n```\\ndeps: DepsT\\n\\n```\\n\\nDependencies for the graph.\\n###  BaseNode\\nBases: `ABC`, `GenericStateT[, DepsT, NodeRunEndT]`\\nBase class for a node.\\nSource code in `pydantic_graph/pydantic_graph/nodes.py`\\n```\\n\\n```\\n| ```\\nclassBaseNode(ABC, Generic[StateT, DepsT, NodeRunEndT]):\\n\"\"\"Base class for a node.\"\"\"\\n  docstring_notes: ClassVar[bool] = False\\n\"\"\"Set to `True` to generate mermaid diagram notes from the class\\'s docstring.\\n  While this can add valuable information to the diagram, it can make diagrams harder to view, hence\\n  it is disabled by default. You can also customise notes overriding the\\n  [`get_note`][pydantic_graph.nodes.BaseNode.get_note] method.\\n  \"\"\"\\n  @abstractmethod\\n  async defrun(self, ctx: GraphRunContext[StateT, DepsT]) -> BaseNode[StateT, DepsT, Any] | End[NodeRunEndT]:\\n\"\"\"Run the node.\\n    This is an abstract method that must be implemented by subclasses.\\n    !!! note \"Return types used at runtime\"\\n      The return type of this method are read by `pydantic_graph` at runtime and used to define which\\n      nodes can be called next in the graph. This is displayed in [mermaid diagrams](mermaid.md)\\n      and enforced when running the graph.\\n    Args:\\n      ctx: The graph context.\\n    Returns:\\n      The next node to run or [`End`][pydantic_graph.nodes.End] to signal the end of the graph.\\n    \"\"\"\\n    ...\\n  @classmethod\\n  @cache\\n  defget_id(cls) -> str:\\n\"\"\"Get the ID of the node.\"\"\"\\n    return cls.__name__\\n  @classmethod\\n  defget_note(cls) -> str | None:\\n\"\"\"Get a note about the node to render on mermaid charts.\\n    By default, this returns a note only if [`docstring_notes`][pydantic_graph.nodes.BaseNode.docstring_notes]\\n    is `True`. You can override this method to customise the node notes.\\n    \"\"\"\\n    if not cls.docstring_notes:\\n      return None\\n    docstring = cls.__doc__\\n    # dataclasses get an automatic docstring which is just their signature, we don\\'t want that\\n    if docstring and is_dataclass(cls) and docstring.startswith(f\\'{cls.__name__}(\\'):\\n      docstring = None\\n    if docstring:\\n      # remove indentation from docstring\\n      importinspect\\n      docstring = inspect.cleandoc(docstring)\\n    return docstring\\n  @classmethod\\n  defget_node_def(cls, local_ns: dict[str, Any] | None) -> NodeDef[StateT, DepsT, NodeRunEndT]:\\n\"\"\"Get the node definition.\"\"\"\\n    type_hints = get_type_hints(cls.run, localns=local_ns, include_extras=True)\\n    try:\\n      return_hint = type_hints[\\'return\\']\\n    except KeyError as e:\\n      raise exceptions.GraphSetupError(f\\'Node {cls} is missing a return type hint on its `run` method\\') frome\\n    next_node_edges: dict[str, Edge] = {}\\n    end_edge: Edge | None = None\\n    returns_base_node: bool = False\\n    for return_type in _utils.get_union_args(return_hint):\\n      return_type, annotations = _utils.unpack_annotated(return_type)\\n      edge = next((a for a in annotations if isinstance(a, Edge)), Edge(None))\\n      return_type_origin = get_origin(return_type) or return_type\\n      if return_type_origin is End:\\n        end_edge = edge\\n      elif return_type_origin is BaseNode:\\n        # TODO: Should we disallow this?\\n        returns_base_node = True\\n      elif issubclass(return_type_origin, BaseNode):\\n        next_node_edges[return_type.get_id()] = edge\\n      else:\\n        raise exceptions.GraphSetupError(f\\'Invalid return type: {return_type}\\')\\n    return NodeDef(\\n      cls,\\n      cls.get_id(),\\n      cls.get_note(),\\n      next_node_edges,\\n      end_edge,\\n      returns_base_node,\\n    )\\n\\n```\\n\\n---|---  \\n####  docstring_notes `class-attribute`\\n```\\ndocstring_notes: bool = False\\n\\n```\\n\\nSet to `True` to generate mermaid diagram notes from the class\\'s docstring.\\nWhile this can add valuable information to the diagram, it can make diagrams harder to view, hence it is disabled by default. You can also customise notes overriding the `get_note` method.\\n####  run `abstractmethod` `async`\\n```\\nrun(\\n  ctx: GraphRunContextStateT[, DepsT],\\n) -> BaseNodeStateT[, DepsT, Any] | EndNodeRunEndT[]\\n\\n```\\n\\nRun the node.\\nThis is an abstract method that must be implemented by subclasses.\\nReturn types used at runtime\\nThe return type of this method are read by `pydantic_graph` at runtime and used to define which nodes can be called next in the graph. This is displayed in mermaid diagrams and enforced when running the graph.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`ctx` |  `GraphRunContextStateT[, DepsT]` |  The graph context. |  _required_  \\nReturns:\\nType | Description  \\n---|---  \\n`BaseNodeStateT[, DepsT, Any] | EndNodeRunEndT[]` |  The next node to run or `End` to signal the end of the graph.  \\nSource code in `pydantic_graph/pydantic_graph/nodes.py`\\n```\\n\\n```\\n| ```\\n@abstractmethod\\nasync defrun(self, ctx: GraphRunContext[StateT, DepsT]) -> BaseNode[StateT, DepsT, Any] | End[NodeRunEndT]:\\n\"\"\"Run the node.\\n  This is an abstract method that must be implemented by subclasses.\\n  !!! note \"Return types used at runtime\"\\n    The return type of this method are read by `pydantic_graph` at runtime and used to define which\\n    nodes can be called next in the graph. This is displayed in [mermaid diagrams](mermaid.md)\\n    and enforced when running the graph.\\n  Args:\\n    ctx: The graph context.\\n  Returns:\\n    The next node to run or [`End`][pydantic_graph.nodes.End] to signal the end of the graph.\\n  \"\"\"\\n  ...\\n\\n```\\n\\n---|---  \\n####  get_id `cached` `classmethod`\\n```\\nget_id() -> str\\n\\n```\\n\\nGet the ID of the node.\\nSource code in `pydantic_graph/pydantic_graph/nodes.py`\\n```\\n\\n```\\n| ```\\n@classmethod\\n@cache\\ndefget_id(cls) -> str:\\n\"\"\"Get the ID of the node.\"\"\"\\n  return cls.__name__\\n\\n```\\n\\n---|---  \\n####  get_note `classmethod`\\n```\\nget_note() -> str | None\\n\\n```\\n\\nGet a note about the node to render on mermaid charts.\\nBy default, this returns a note only if `docstring_notes` is `True`. You can override this method to customise the node notes.\\nSource code in `pydantic_graph/pydantic_graph/nodes.py`\\n```\\n\\n```\\n| ```\\n@classmethod\\ndefget_note(cls) -> str | None:\\n\"\"\"Get a note about the node to render on mermaid charts.\\n  By default, this returns a note only if [`docstring_notes`][pydantic_graph.nodes.BaseNode.docstring_notes]\\n  is `True`. You can override this method to customise the node notes.\\n  \"\"\"\\n  if not cls.docstring_notes:\\n    return None\\n  docstring = cls.__doc__\\n  # dataclasses get an automatic docstring which is just their signature, we don\\'t want that\\n  if docstring and is_dataclass(cls) and docstring.startswith(f\\'{cls.__name__}(\\'):\\n    docstring = None\\n  if docstring:\\n    # remove indentation from docstring\\n    importinspect\\n    docstring = inspect.cleandoc(docstring)\\n  return docstring\\n\\n```\\n\\n---|---  \\n####  get_node_def `classmethod`\\n```\\nget_node_def(\\n  local_ns: dictstr[, Any] | None,\\n) -> NodeDefStateT[, DepsT, NodeRunEndT]\\n\\n```\\n\\nGet the node definition.\\nSource code in `pydantic_graph/pydantic_graph/nodes.py`\\n```\\n\\n```\\n| ```\\n@classmethod\\ndefget_node_def(cls, local_ns: dict[str, Any] | None) -> NodeDef[StateT, DepsT, NodeRunEndT]:\\n\"\"\"Get the node definition.\"\"\"\\n  type_hints = get_type_hints(cls.run, localns=local_ns, include_extras=True)\\n  try:\\n    return_hint = type_hints[\\'return\\']\\n  except KeyError as e:\\n    raise exceptions.GraphSetupError(f\\'Node {cls} is missing a return type hint on its `run` method\\') frome\\n  next_node_edges: dict[str, Edge] = {}\\n  end_edge: Edge | None = None\\n  returns_base_node: bool = False\\n  for return_type in _utils.get_union_args(return_hint):\\n    return_type, annotations = _utils.unpack_annotated(return_type)\\n    edge = next((a for a in annotations if isinstance(a, Edge)), Edge(None))\\n    return_type_origin = get_origin(return_type) or return_type\\n    if return_type_origin is End:\\n      end_edge = edge\\n    elif return_type_origin is BaseNode:\\n      # TODO: Should we disallow this?\\n      returns_base_node = True\\n    elif issubclass(return_type_origin, BaseNode):\\n      next_node_edges[return_type.get_id()] = edge\\n    else:\\n      raise exceptions.GraphSetupError(f\\'Invalid return type: {return_type}\\')\\n  return NodeDef(\\n    cls,\\n    cls.get_id(),\\n    cls.get_note(),\\n    next_node_edges,\\n    end_edge,\\n    returns_base_node,\\n  )\\n\\n```\\n\\n---|---  \\n###  End `dataclass`\\nBases: `GenericRunEndT[]`\\nType to return from a node to signal the end of the graph.\\nSource code in `pydantic_graph/pydantic_graph/nodes.py`\\n```\\n\\n```\\n| ```\\n@dataclass\\nclassEnd(Generic[RunEndT]):\\n\"\"\"Type to return from a node to signal the end of the graph.\"\"\"\\n  data: RunEndT\\n\"\"\"Data to return from the graph.\"\"\"\\n\\n```\\n\\n---|---  \\n####  data `instance-attribute`\\n```\\ndata: RunEndT\\n\\n```\\n\\nData to return from the graph.\\n###  Edge `dataclass`\\nAnnotation to apply a label to an edge in a graph.\\nSource code in `pydantic_graph/pydantic_graph/nodes.py`\\n```\\n\\n```\\n| ```\\n@dataclass\\nclassEdge:\\n\"\"\"Annotation to apply a label to an edge in a graph.\"\"\"\\n  label: str | None\\n\"\"\"Label for the edge.\"\"\"\\n\\n```\\n\\n---|---  \\n####  label `instance-attribute`\\n```\\nlabel: str | None\\n\\n```\\n\\nLabel for the edge.\\n###  DepsT `module-attribute`\\n```\\nDepsT = TypeVar(\\'DepsT\\', default=None, contravariant=True)\\n\\n```\\n\\nType variable for the dependencies of a graph and node.\\n###  RunEndT `module-attribute`\\n```\\nRunEndT = TypeVar(\\'RunEndT\\', covariant=True, default=None)\\n\\n```\\n\\nCovariant type variable for the return type of a graph `run`.\\n###  NodeRunEndT `module-attribute`\\n```\\nNodeRunEndT = TypeVar(\\n  \"NodeRunEndT\", covariant=True, default=Never\\n)\\n\\n```\\n\\nCovariant type variable for the return type of a node `run`.\\n© Pydantic Services Inc. 2024 to present\\n\\n--- '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con[36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chunks import chunk_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Document(metadata={'chunk_id': 0, 'type': 'text', 'num_code_blocks': 8, 'code_block_ids': [0, 1, 2, 3, 4, 5, 6, 7], 'url': 'https://pydantic.com'}, page_content=\"nodes  \\n  *  GraphRunContext  \\n    *  state  \\n    *  deps  \\n  *  BaseNode  \\n    *  docstring_notes  \\n    *  run  \\n    *  get_id  \\n    *  get_note  \\n    *  get_node_def  \\n  *  End  \\n    *  data  \\n  *  Edge  \\n    *  label  \\n  *  DepsT  \\n  *  RunEndT  \\n  *  NodeRunEndT  \\n\\n  1.  Introduction  \\n  2.  API Reference  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# `pydantic_graph.nodes`\\n###  GraphRunContext `dataclass`\\nBases: `GenericStateT[, DepsT]`\\nContext for a graph.\\nSource code in `pydantic_graph/pydantic_graph/nodes.py`\\n\\n| \\n\\n---|---  \\n####  state `instance-attribute`\\n\\n\\nThe state of the graph.\\n####  deps `instance-attribute`\\n\\n\\nDependencies for the graph.\\n###  BaseNode\\nBases: `ABC`, `GenericStateT[, DepsT, NodeRunEndT]`\\nBase class for a node.\\nSource code in `pydantic_graph/pydantic_graph/nodes.py`\\n\\n| \\n\\n---|---  \\n####  docstring_notes `class-attribute`\\n\\n\\nSet to `True` to generate mermaid diagram notes from the class's docstring.\\nWhile this can add valuable information to the diagram, it can make diagrams harder to view, hence it is disabled by default. You can also customise notes overriding the `get_note` method.\\n####  run `abstractmethod` `async`\\n\\n\\nRun the node.\\nThis is an abstract method that must be implemented by subclasses.\\nReturn types used at runtime\\nThe return type of this method are read by `pydantic_graph` at runtime and used to define which nodes can be called next in the graph. This is displayed in mermaid diagrams and enforced when running the graph.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`ctx` |  `GraphRunContextStateT[, DepsT]` |  The graph context.\"),\n",
       "  Document(metadata={'chunk_id': 1, 'type': 'text', 'num_code_blocks': 20, 'code_block_ids': [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27], 'url': 'https://pydantic.com'}, page_content='|  _required_  \\nReturns:\\nType | Description  \\n---|---  \\n`BaseNodeStateT[, DepsT, Any] | EndNodeRunEndT[]` |  The next node to run or `End` to signal the end of the graph.  \\nSource code in `pydantic_graph/pydantic_graph/nodes.py`\\n\\n| \\n\\n---|---  \\n####  get_id `cached` `classmethod`\\n\\n\\nGet the ID of the node.\\nSource code in `pydantic_graph/pydantic_graph/nodes.py`\\n\\n| \\n\\n---|---  \\n####  get_note `classmethod`\\n\\n\\nGet a note about the node to render on mermaid charts.\\nBy default, this returns a note only if `docstring_notes` is `True`. You can override this method to customise the node notes.\\nSource code in `pydantic_graph/pydantic_graph/nodes.py`\\n\\n| \\n\\n---|---  \\n####  get_node_def `classmethod`\\n\\n\\nGet the node definition.\\nSource code in `pydantic_graph/pydantic_graph/nodes.py`\\n\\n| \\n\\n---|---  \\n###  End `dataclass`\\nBases: `GenericRunEndT[]`\\nType to return from a node to signal the end of the graph.\\nSource code in `pydantic_graph/pydantic_graph/nodes.py`\\n\\n| \\n\\n---|---  \\n####  data `instance-attribute`\\n\\n\\nData to return from the graph.\\n###  Edge `dataclass`\\nAnnotation to apply a label to an edge in a graph.\\nSource code in `pydantic_graph/pydantic_graph/nodes.py`\\n\\n| \\n\\n---|---  \\n####  label `instance-attribute`\\n\\n\\nLabel for the edge.\\n###  DepsT `module-attribute`\\n\\n\\nType variable for the dependencies of a graph and node.\\n###  RunEndT `module-attribute`\\n\\n\\nCovariant type variable for the return type of a graph `run`.\\n###  NodeRunEndT `module-attribute`\\n\\n\\nCovariant type variable for the return type of a node `run`.\\n© Pydantic Services Inc. 2024 to present\\n\\n---')],\n",
       " [Document(metadata={'chunk_id': 0, 'type': 'code', 'parent_text_id': 0, 'url': 'https://pydantic.com'}, page_content=''),\n",
       "  Document(metadata={'chunk_id': 0, 'type': 'code', 'parent_text_id': 0, 'url': 'https://pydantic.com'}, page_content='@dataclass\\nclassGraphRunContext(Generic[StateT, DepsT]):\\n\"\"\"Context for a graph.\"\"\"\\n  # TODO: Can we get rid of this struct and just pass both these things around..?\\n  state: StateT\\n\"\"\"The state of the graph.\"\"\"\\n  deps: DepsT\\n\"\"\"Dependencies for the graph.\"\"\"'),\n",
       "  Document(metadata={'chunk_id': 0, 'type': 'code', 'parent_text_id': 0, 'url': 'https://pydantic.com'}, page_content='state: StateT'),\n",
       "  Document(metadata={'chunk_id': 0, 'type': 'code', 'parent_text_id': 0, 'url': 'https://pydantic.com'}, page_content='deps: DepsT'),\n",
       "  Document(metadata={'chunk_id': 0, 'type': 'code', 'parent_text_id': 0, 'url': 'https://pydantic.com'}, page_content=''),\n",
       "  Document(metadata={'chunk_id': 0, 'type': 'code', 'parent_text_id': 0, 'url': 'https://pydantic.com'}, page_content='classBaseNode(ABC, Generic[StateT, DepsT, NodeRunEndT]):\\n\"\"\"Base class for a node.\"\"\"\\n  docstring_notes: ClassVar[bool] = False\\n\"\"\"Set to `True` to generate mermaid diagram notes from the class\\'s docstring.\\n  While this can add valuable information to the diagram, it can make diagrams harder to view, hence\\n  it is disabled by default. You can also customise notes overriding the\\n  [`get_note`][pydantic_graph.nodes.BaseNode.get_note] method.\\n  \"\"\"\\n  @abstractmethod\\n  async defrun(self, ctx: GraphRunContext[StateT, DepsT]) -> BaseNode[StateT, DepsT, Any] | End[NodeRunEndT]:\\n\"\"\"Run the node.\\n    This is an abstract method that must be implemented by subclasses.\\n    !!! note \"Return types used at runtime\"\\n      The return type of this method are read by `pydantic_graph` at runtime and used to define which\\n      nodes can be called next in the graph. This is displayed in [mermaid diagrams](mermaid.md)\\n      and enforced when running the graph.\\n    Args:\\n      ctx: The graph context.\\n    Returns:\\n      The next node to run or [`End`][pydantic_graph.nodes.End] to signal the end of the graph.\\n    \"\"\"\\n    ...\\n  @classmethod\\n  @cache\\n  defget_id(cls) -> str:\\n\"\"\"Get the ID of the node.\"\"\"\\n    return cls.__name__\\n  @classmethod\\n  defget_note(cls) -> str | None:\\n\"\"\"Get a note about the node to render on mermaid charts.\\n    By default, this returns a note only if [`docstring_notes`][pydantic_graph.nodes.BaseNode.docstring_notes]\\n    is `True`. You can override this method to customise the node notes.\\n    \"\"\"\\n    if not cls.docstring_notes:\\n      return None\\n    docstring = cls.__doc__\\n    # dataclasses get an automatic docstring which is just their signature, we don\\'t want that\\n    if docstring and is_dataclass(cls) and docstring.startswith(f\\'{cls.__name__}(\\'):\\n      docstring = None\\n    if docstring:\\n      # remove indentation from docstring\\n      importinspect\\n      docstring = inspect.cleandoc(docstring)\\n    return docstring\\n  @classmethod\\n  defget_node_def(cls, local_ns: dict[str, Any] | None) -> NodeDef[StateT, DepsT, NodeRunEndT]:\\n\"\"\"Get the node definition.\"\"\"\\n    type_hints = get_type_hints(cls.run, localns=local_ns, include_extras=True)\\n    try:\\n      return_hint = type_hints[\\'return\\']\\n    except KeyError as e:\\n      raise exceptions.GraphSetupError(f\\'Node {cls} is missing a return type hint on its `run` method\\') frome\\n    next_node_edges: dict[str, Edge] = {}\\n    end_edge: Edge | None = None\\n    returns_base_node: bool = False\\n    for return_type in _utils.get_union_args(return_hint):\\n      return_type, annotations = _utils.unpack_annotated(return_type)\\n      edge = next((a for a in annotations if isinstance(a, Edge)), Edge(None))\\n      return_type_origin = get_origin(return_type) or return_type\\n      if return_type_origin is End:\\n        end_edge = edge\\n      elif return_type_origin is BaseNode:\\n        # TODO: Should we disallow this?\\n        returns_base_node = True\\n      elif issubclass(return_type_origin, BaseNode):\\n        next_node_edges[return_type.get_id()] = edge\\n      else:\\n        raise exceptions.GraphSetupError(f\\'Invalid return type: {return_type}\\')\\n    return NodeDef(\\n      cls,\\n      cls.get_id(),\\n      cls.get_note(),\\n      next_node_edges,\\n      end_edge,\\n      returns_base_node,\\n    )'),\n",
       "  Document(metadata={'chunk_id': 0, 'type': 'code', 'parent_text_id': 0, 'url': 'https://pydantic.com'}, page_content='docstring_notes: bool = False'),\n",
       "  Document(metadata={'chunk_id': 0, 'type': 'code', 'parent_text_id': 0, 'url': 'https://pydantic.com'}, page_content='run(\\n  ctx: GraphRunContextStateT[, DepsT],\\n) -> BaseNodeStateT[, DepsT, Any] | EndNodeRunEndT[]'),\n",
       "  Document(metadata={'chunk_id': 1, 'type': 'code', 'parent_text_id': 1, 'url': 'https://pydantic.com'}, page_content=''),\n",
       "  Document(metadata={'chunk_id': 1, 'type': 'code', 'parent_text_id': 1, 'url': 'https://pydantic.com'}, page_content='@abstractmethod\\nasync defrun(self, ctx: GraphRunContext[StateT, DepsT]) -> BaseNode[StateT, DepsT, Any] | End[NodeRunEndT]:\\n\"\"\"Run the node.\\n  This is an abstract method that must be implemented by subclasses.\\n  !!! note \"Return types used at runtime\"\\n    The return type of this method are read by `pydantic_graph` at runtime and used to define which\\n    nodes can be called next in the graph. This is displayed in [mermaid diagrams](mermaid.md)\\n    and enforced when running the graph.\\n  Args:\\n    ctx: The graph context.\\n  Returns:\\n    The next node to run or [`End`][pydantic_graph.nodes.End] to signal the end of the graph.\\n  \"\"\"\\n  ...'),\n",
       "  Document(metadata={'chunk_id': 1, 'type': 'code', 'parent_text_id': 1, 'url': 'https://pydantic.com'}, page_content='get_id() -> str'),\n",
       "  Document(metadata={'chunk_id': 1, 'type': 'code', 'parent_text_id': 1, 'url': 'https://pydantic.com'}, page_content=''),\n",
       "  Document(metadata={'chunk_id': 1, 'type': 'code', 'parent_text_id': 1, 'url': 'https://pydantic.com'}, page_content='@classmethod\\n@cache\\ndefget_id(cls) -> str:\\n\"\"\"Get the ID of the node.\"\"\"\\n  return cls.__name__'),\n",
       "  Document(metadata={'chunk_id': 1, 'type': 'code', 'parent_text_id': 1, 'url': 'https://pydantic.com'}, page_content='get_note() -> str | None'),\n",
       "  Document(metadata={'chunk_id': 1, 'type': 'code', 'parent_text_id': 1, 'url': 'https://pydantic.com'}, page_content=''),\n",
       "  Document(metadata={'chunk_id': 1, 'type': 'code', 'parent_text_id': 1, 'url': 'https://pydantic.com'}, page_content='@classmethod\\ndefget_note(cls) -> str | None:\\n\"\"\"Get a note about the node to render on mermaid charts.\\n  By default, this returns a note only if [`docstring_notes`][pydantic_graph.nodes.BaseNode.docstring_notes]\\n  is `True`. You can override this method to customise the node notes.\\n  \"\"\"\\n  if not cls.docstring_notes:\\n    return None\\n  docstring = cls.__doc__\\n  # dataclasses get an automatic docstring which is just their signature, we don\\'t want that\\n  if docstring and is_dataclass(cls) and docstring.startswith(f\\'{cls.__name__}(\\'):\\n    docstring = None\\n  if docstring:\\n    # remove indentation from docstring\\n    importinspect\\n    docstring = inspect.cleandoc(docstring)\\n  return docstring'),\n",
       "  Document(metadata={'chunk_id': 1, 'type': 'code', 'parent_text_id': 1, 'url': 'https://pydantic.com'}, page_content='get_node_def(\\n  local_ns: dictstr[, Any] | None,\\n) -> NodeDefStateT[, DepsT, NodeRunEndT]'),\n",
       "  Document(metadata={'chunk_id': 1, 'type': 'code', 'parent_text_id': 1, 'url': 'https://pydantic.com'}, page_content=''),\n",
       "  Document(metadata={'chunk_id': 1, 'type': 'code', 'parent_text_id': 1, 'url': 'https://pydantic.com'}, page_content='@classmethod\\ndefget_node_def(cls, local_ns: dict[str, Any] | None) -> NodeDef[StateT, DepsT, NodeRunEndT]:\\n\"\"\"Get the node definition.\"\"\"\\n  type_hints = get_type_hints(cls.run, localns=local_ns, include_extras=True)\\n  try:\\n    return_hint = type_hints[\\'return\\']\\n  except KeyError as e:\\n    raise exceptions.GraphSetupError(f\\'Node {cls} is missing a return type hint on its `run` method\\') frome\\n  next_node_edges: dict[str, Edge] = {}\\n  end_edge: Edge | None = None\\n  returns_base_node: bool = False\\n  for return_type in _utils.get_union_args(return_hint):\\n    return_type, annotations = _utils.unpack_annotated(return_type)\\n    edge = next((a for a in annotations if isinstance(a, Edge)), Edge(None))\\n    return_type_origin = get_origin(return_type) or return_type\\n    if return_type_origin is End:\\n      end_edge = edge\\n    elif return_type_origin is BaseNode:\\n      # TODO: Should we disallow this?\\n      returns_base_node = True\\n    elif issubclass(return_type_origin, BaseNode):\\n      next_node_edges[return_type.get_id()] = edge\\n    else:\\n      raise exceptions.GraphSetupError(f\\'Invalid return type: {return_type}\\')\\n  return NodeDef(\\n    cls,\\n    cls.get_id(),\\n    cls.get_note(),\\n    next_node_edges,\\n    end_edge,\\n    returns_base_node,\\n  )'),\n",
       "  Document(metadata={'chunk_id': 1, 'type': 'code', 'parent_text_id': 1, 'url': 'https://pydantic.com'}, page_content=''),\n",
       "  Document(metadata={'chunk_id': 1, 'type': 'code', 'parent_text_id': 1, 'url': 'https://pydantic.com'}, page_content='@dataclass\\nclassEnd(Generic[RunEndT]):\\n\"\"\"Type to return from a node to signal the end of the graph.\"\"\"\\n  data: RunEndT\\n\"\"\"Data to return from the graph.\"\"\"'),\n",
       "  Document(metadata={'chunk_id': 1, 'type': 'code', 'parent_text_id': 1, 'url': 'https://pydantic.com'}, page_content='data: RunEndT'),\n",
       "  Document(metadata={'chunk_id': 1, 'type': 'code', 'parent_text_id': 1, 'url': 'https://pydantic.com'}, page_content=''),\n",
       "  Document(metadata={'chunk_id': 1, 'type': 'code', 'parent_text_id': 1, 'url': 'https://pydantic.com'}, page_content='@dataclass\\nclassEdge:\\n\"\"\"Annotation to apply a label to an edge in a graph.\"\"\"\\n  label: str | None\\n\"\"\"Label for the edge.\"\"\"'),\n",
       "  Document(metadata={'chunk_id': 1, 'type': 'code', 'parent_text_id': 1, 'url': 'https://pydantic.com'}, page_content='label: str | None'),\n",
       "  Document(metadata={'chunk_id': 1, 'type': 'code', 'parent_text_id': 1, 'url': 'https://pydantic.com'}, page_content=\"DepsT = TypeVar('DepsT', default=None, contravariant=True)\"),\n",
       "  Document(metadata={'chunk_id': 1, 'type': 'code', 'parent_text_id': 1, 'url': 'https://pydantic.com'}, page_content=\"RunEndT = TypeVar('RunEndT', covariant=True, default=None)\"),\n",
       "  Document(metadata={'chunk_id': 1, 'type': 'code', 'parent_text_id': 1, 'url': 'https://pydantic.com'}, page_content='NodeRunEndT = TypeVar(\\n  \"NodeRunEndT\", covariant=True, default=Never\\n)')])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_text(con[36])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_text_chunks = []\n",
    "total_code_chunks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  usage  \\n  *  Usage  \\n    *  requests  \\n    *  request_tokens  \\n    *  response_tokens  \\n    *  total_tokens  \\n    *  details  \\n    *  incr  \\n    *  __add__  \\n  *  UsageLimits  \\n    *  request_limit  \\n    *  request_tokens_limit  \\n    *  response_tokens_limit  \\n    *  total_tokens_limit  \\n    *  has_token_limits  \\n    *  check_before_request  \\n    *  check_tokens  \\n\\n  1.  Introduction  \\n  2.  API Reference  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# `pydantic_ai.usage`\\n###  Usage `dataclass`\\nLLM usage associated with a request or run.\\nResponsibility for calculating usage is on the model; PydanticAI simply sums the usage information across requests.\\nYou\\'ll need to look up the documentation of the model you\\'re using to convert usage to monetary costs.\\nSource code in `pydantic_ai_slim/pydantic_ai/usage.py`\\n```\\n\\n```\\n| ```\\n@dataclass\\nclassUsage:\\n\"\"\"LLM usage associated with a request or run.\\n  Responsibility for calculating usage is on the model; PydanticAI simply sums the usage information across requests.\\n  You\\'ll need to look up the documentation of the model you\\'re using to convert usage to monetary costs.\\n  \"\"\"\\n  requests: int = 0\\n\"\"\"Number of requests made to the LLM API.\"\"\"\\n  request_tokens: int | None = None\\n\"\"\"Tokens used in processing requests.\"\"\"\\n  response_tokens: int | None = None\\n\"\"\"Tokens used in generating responses.\"\"\"\\n  total_tokens: int | None = None\\n\"\"\"Total tokens used in the whole run, should generally be equal to `request_tokens + response_tokens`.\"\"\"\\n  details: dict[str, int] | None = None\\n\"\"\"Any extra details returned by the model.\"\"\"\\n  defincr(self, incr_usage: Usage, *, requests: int = 0) -> None:\\n\"\"\"Increment the usage in place.\\n    Args:\\n      incr_usage: The usage to increment by.\\n      requests: The number of requests to increment by in addition to `incr_usage.requests`.\\n    \"\"\"\\n    self.requests += requests\\n    for f in \\'requests\\', \\'request_tokens\\', \\'response_tokens\\', \\'total_tokens\\':\\n      self_value = getattr(self, f)\\n      other_value = getattr(incr_usage, f)\\n      if self_value is not None or other_value is not None:\\n        setattr(self, f, (self_value or 0) + (other_value or 0))\\n    if incr_usage.details:\\n      self.details = self.details or {}\\n      for key, value in incr_usage.details.items():\\n        self.details[key] = self.details.get(key, 0) + value\\n  def__add__(self, other: Usage) -> Usage:\\n\"\"\"Add two Usages together.\\n    This is provided so it\\'s trivial to sum usage information from multiple requests and runs.\\n    \"\"\"\\n    new_usage = copy(self)\\n    new_usage.incr(other)\\n    return new_usage\\n\\n```\\n\\n---|---  \\n####  requests `class-attribute` `instance-attribute`\\n```\\nrequests: int = 0\\n\\n```\\n\\nNumber of requests made to the LLM API.\\n####  request_tokens `class-attribute` `instance-attribute`\\n```\\nrequest_tokens: int | None = None\\n\\n```\\n\\nTokens used in processing requests.\\n####  response_tokens `class-attribute` `instance-attribute`\\n```\\nresponse_tokens: int | None = None\\n\\n```\\n\\nTokens used in generating responses.\\n####  total_tokens `class-attribute` `instance-attribute`\\n```\\ntotal_tokens: int | None = None\\n\\n```\\n\\nTotal tokens used in the whole run, should generally be equal to `request_tokens + response_tokens`.\\n####  details `class-attribute` `instance-attribute`\\n```\\ndetails: dictstr[, int] | None = None\\n\\n```\\n\\nAny extra details returned by the model.\\n####  incr\\n```\\nincr(incr_usage: Usage, *, requests: int = 0) -> None\\n\\n```\\n\\nIncrement the usage in place.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`incr_usage` |  `Usage` |  The usage to increment by. |  _required_  \\n`requests` |  `int` |  The number of requests to increment by in addition to `incr_usage.requests`. |  `0`  \\nSource code in `pydantic_ai_slim/pydantic_ai/usage.py`\\n```\\n\\n```\\n| ```\\ndefincr(self, incr_usage: Usage, *, requests: int = 0) -> None:\\n\"\"\"Increment the usage in place.\\n  Args:\\n    incr_usage: The usage to increment by.\\n    requests: The number of requests to increment by in addition to `incr_usage.requests`.\\n  \"\"\"\\n  self.requests += requests\\n  for f in \\'requests\\', \\'request_tokens\\', \\'response_tokens\\', \\'total_tokens\\':\\n    self_value = getattr(self, f)\\n    other_value = getattr(incr_usage, f)\\n    if self_value is not None or other_value is not None:\\n      setattr(self, f, (self_value or 0) + (other_value or 0))\\n  if incr_usage.details:\\n    self.details = self.details or {}\\n    for key, value in incr_usage.details.items():\\n      self.details[key] = self.details.get(key, 0) + value\\n\\n```\\n\\n---|---  \\n####  __add__\\n```\\n__add__(other: Usage) -> Usage\\n\\n```\\n\\nAdd two Usages together.\\nThis is provided so it\\'s trivial to sum usage information from multiple requests and runs.\\nSource code in `pydantic_ai_slim/pydantic_ai/usage.py`\\n```\\n\\n```\\n| ```\\ndef__add__(self, other: Usage) -> Usage:\\n\"\"\"Add two Usages together.\\n  This is provided so it\\'s trivial to sum usage information from multiple requests and runs.\\n  \"\"\"\\n  new_usage = copy(self)\\n  new_usage.incr(other)\\n  return new_usage\\n\\n```\\n\\n---|---  \\n###  UsageLimits `dataclass`\\nLimits on model usage.\\nThe request count is tracked by pydantic_ai, and the request limit is checked before each request to the model. Token counts are provided in responses from the model, and the token limits are checked after each response.\\nEach of the limits can be set to `None` to disable that limit.\\nSource code in `pydantic_ai_slim/pydantic_ai/usage.py`\\n```\\n\\n```\\n| ```\\n@dataclass\\nclassUsageLimits:\\n\"\"\"Limits on model usage.\\n  The request count is tracked by pydantic_ai, and the request limit is checked before each request to the model.\\n  Token counts are provided in responses from the model, and the token limits are checked after each response.\\n  Each of the limits can be set to `None` to disable that limit.\\n  \"\"\"\\n  request_limit: int | None = 50\\n\"\"\"The maximum number of requests allowed to the model.\"\"\"\\n  request_tokens_limit: int | None = None\\n\"\"\"The maximum number of tokens allowed in requests to the model.\"\"\"\\n  response_tokens_limit: int | None = None\\n\"\"\"The maximum number of tokens allowed in responses from the model.\"\"\"\\n  total_tokens_limit: int | None = None\\n\"\"\"The maximum number of tokens allowed in requests and responses combined.\"\"\"\\n  defhas_token_limits(self) -> bool:\\n\"\"\"Returns `True` if this instance places any limits on token counts.\\n    If this returns `False`, the `check_tokens` method will never raise an error.\\n    This is useful because if we have token limits, we need to check them after receiving each streamed message.\\n    If there are no limits, we can skip that processing in the streaming response iterator.\\n    \"\"\"\\n    return any(\\n      limit is not None\\n      for limit in (self.request_tokens_limit, self.response_tokens_limit, self.total_tokens_limit)\\n    )\\n  defcheck_before_request(self, usage: Usage) -> None:\\n\"\"\"Raises a `UsageLimitExceeded` exception if the next request would exceed the request_limit.\"\"\"\\n    request_limit = self.request_limit\\n    if request_limit is not None and usage.requests >= request_limit:\\n      raise UsageLimitExceeded(f\\'The next request would exceed the request_limit of {request_limit}\\')\\n  defcheck_tokens(self, usage: Usage) -> None:\\n\"\"\"Raises a `UsageLimitExceeded` exception if the usage exceeds any of the token limits.\"\"\"\\n    request_tokens = usage.request_tokens or 0\\n    if self.request_tokens_limit is not None and request_tokens > self.request_tokens_limit:\\n      raise UsageLimitExceeded(\\n        f\\'Exceeded the request_tokens_limit of {self.request_tokens_limit} ({request_tokens=})\\'\\n      )\\n    response_tokens = usage.response_tokens or 0\\n    if self.response_tokens_limit is not None and response_tokens > self.response_tokens_limit:\\n      raise UsageLimitExceeded(\\n        f\\'Exceeded the response_tokens_limit of {self.response_tokens_limit} ({response_tokens=})\\'\\n      )\\n    total_tokens = usage.total_tokens or 0\\n    if self.total_tokens_limit is not None and total_tokens > self.total_tokens_limit:\\n      raise UsageLimitExceeded(f\\'Exceeded the total_tokens_limit of {self.total_tokens_limit} ({total_tokens=})\\')\\n\\n```\\n\\n---|---  \\n####  request_limit `class-attribute` `instance-attribute`\\n```\\nrequest_limit: int | None = 50\\n\\n```\\n\\nThe maximum number of requests allowed to the model.\\n####  request_tokens_limit `class-attribute` `instance-attribute`\\n```\\nrequest_tokens_limit: int | None = None\\n\\n```\\n\\nThe maximum number of tokens allowed in requests to the model.\\n####  response_tokens_limit `class-attribute` `instance-attribute`\\n```\\nresponse_tokens_limit: int | None = None\\n\\n```\\n\\nThe maximum number of tokens allowed in responses from the model.\\n####  total_tokens_limit `class-attribute` `instance-attribute`\\n```\\ntotal_tokens_limit: int | None = None\\n\\n```\\n\\nThe maximum number of tokens allowed in requests and responses combined.\\n####  has_token_limits\\n```\\nhas_token_limits() -> bool\\n\\n```\\n\\nReturns `True` if this instance places any limits on token counts.\\nIf this returns `False`, the `check_tokens` method will never raise an error.\\nThis is useful because if we have token limits, we need to check them after receiving each streamed message. If there are no limits, we can skip that processing in the streaming response iterator.\\nSource code in `pydantic_ai_slim/pydantic_ai/usage.py`\\n```\\n\\n```\\n| ```\\ndefhas_token_limits(self) -> bool:\\n\"\"\"Returns `True` if this instance places any limits on token counts.\\n  If this returns `False`, the `check_tokens` method will never raise an error.\\n  This is useful because if we have token limits, we need to check them after receiving each streamed message.\\n  If there are no limits, we can skip that processing in the streaming response iterator.\\n  \"\"\"\\n  return any(\\n    limit is not None\\n    for limit in (self.request_tokens_limit, self.response_tokens_limit, self.total_tokens_limit)\\n  )\\n\\n```\\n\\n---|---  \\n####  check_before_request\\n```\\ncheck_before_request(usage: Usage) -> None\\n\\n```\\n\\nRaises a `UsageLimitExceeded` exception if the next request would exceed the request_limit.\\nSource code in `pydantic_ai_slim/pydantic_ai/usage.py`\\n```\\n\\n```\\n| ```\\ndefcheck_before_request(self, usage: Usage) -> None:\\n\"\"\"Raises a `UsageLimitExceeded` exception if the next request would exceed the request_limit.\"\"\"\\n  request_limit = self.request_limit\\n  if request_limit is not None and usage.requests >= request_limit:\\n    raise UsageLimitExceeded(f\\'The next request would exceed the request_limit of {request_limit}\\')\\n\\n```\\n\\n---|---  \\n####  check_tokens\\n```\\ncheck_tokens(usage: Usage) -> None\\n\\n```\\n\\nRaises a `UsageLimitExceeded` exception if the usage exceeds any of the token limits.\\nSource code in `pydantic_ai_slim/pydantic_ai/usage.py`\\n```\\n\\n```\\n| ```\\ndefcheck_tokens(self, usage: Usage) -> None:\\n\"\"\"Raises a `UsageLimitExceeded` exception if the usage exceeds any of the token limits.\"\"\"\\n  request_tokens = usage.request_tokens or 0\\n  if self.request_tokens_limit is not None and request_tokens > self.request_tokens_limit:\\n    raise UsageLimitExceeded(\\n      f\\'Exceeded the request_tokens_limit of {self.request_tokens_limit} ({request_tokens=})\\'\\n    )\\n  response_tokens = usage.response_tokens or 0\\n  if self.response_tokens_limit is not None and response_tokens > self.response_tokens_limit:\\n    raise UsageLimitExceeded(\\n      f\\'Exceeded the response_tokens_limit of {self.response_tokens_limit} ({response_tokens=})\\'\\n    )\\n  total_tokens = usage.total_tokens or 0\\n  if self.total_tokens_limit is not None and total_tokens > self.total_tokens_limit:\\n    raise UsageLimitExceeded(f\\'Exceeded the total_tokens_limit of {self.total_tokens_limit} ({total_tokens=})\\')\\n\\n```\\n\\n---|---  \\n© Pydantic Services Inc. 2024 to present\\n\\n--- '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'chunk_id': 0, 'type': 'code', 'parent_text_id': 0, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'chunk_id': 0, 'type': 'code', 'parent_text_id': 0, 'url': 'https://pydantic.com'}, page_content='@dataclass\\nclassUsage:\\n\"\"\"LLM usage associated with a request or run.\\n  Responsibility for calculating usage is on the model; PydanticAI simply sums the usage information across requests.\\n  You\\'ll need to look up the documentation of the model you\\'re using to convert usage to monetary costs.\\n  \"\"\"\\n  requests: int = 0\\n\"\"\"Number of requests made to the LLM API.\"\"\"\\n  request_tokens: int | None = None\\n\"\"\"Tokens used in processing requests.\"\"\"\\n  response_tokens: int | None = None\\n\"\"\"Tokens used in generating responses.\"\"\"\\n  total_tokens: int | None = None\\n\"\"\"Total tokens used in the whole run, should generally be equal to `request_tokens + response_tokens`.\"\"\"\\n  details: dict[str, int] | None = None\\n\"\"\"Any extra details returned by the model.\"\"\"\\n  defincr(self, incr_usage: Usage, *, requests: int = 0) -> None:\\n\"\"\"Increment the usage in place.\\n    Args:\\n      incr_usage: The usage to increment by.\\n      requests: The number of requests to increment by in addition to `incr_usage.requests`.\\n    \"\"\"\\n    self.requests += requests\\n    for f in \\'requests\\', \\'request_tokens\\', \\'response_tokens\\', \\'total_tokens\\':\\n      self_value = getattr(self, f)\\n      other_value = getattr(incr_usage, f)\\n      if self_value is not None or other_value is not None:\\n        setattr(self, f, (self_value or 0) + (other_value or 0))\\n    if incr_usage.details:\\n      self.details = self.details or {}\\n      for key, value in incr_usage.details.items():\\n        self.details[key] = self.details.get(key, 0) + value\\n  def__add__(self, other: Usage) -> Usage:\\n\"\"\"Add two Usages together.\\n    This is provided so it\\'s trivial to sum usage information from multiple requests and runs.\\n    \"\"\"\\n    new_usage = copy(self)\\n    new_usage.incr(other)\\n    return new_usage'),\n",
       " Document(metadata={'chunk_id': 0, 'type': 'code', 'parent_text_id': 0, 'url': 'https://pydantic.com'}, page_content='requests: int = 0'),\n",
       " Document(metadata={'chunk_id': 0, 'type': 'code', 'parent_text_id': 0, 'url': 'https://pydantic.com'}, page_content='request_tokens: int | None = None'),\n",
       " Document(metadata={'chunk_id': 0, 'type': 'code', 'parent_text_id': 0, 'url': 'https://pydantic.com'}, page_content='response_tokens: int | None = None'),\n",
       " Document(metadata={'chunk_id': 0, 'type': 'code', 'parent_text_id': 0, 'url': 'https://pydantic.com'}, page_content='total_tokens: int | None = None'),\n",
       " Document(metadata={'chunk_id': 0, 'type': 'code', 'parent_text_id': 0, 'url': 'https://pydantic.com'}, page_content='details: dictstr[, int] | None = None'),\n",
       " Document(metadata={'chunk_id': 0, 'type': 'code', 'parent_text_id': 0, 'url': 'https://pydantic.com'}, page_content='incr(incr_usage: Usage, *, requests: int = 0) -> None'),\n",
       " Document(metadata={'chunk_id': 0, 'type': 'code', 'parent_text_id': 0, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'chunk_id': 0, 'type': 'code', 'parent_text_id': 0, 'url': 'https://pydantic.com'}, page_content='defincr(self, incr_usage: Usage, *, requests: int = 0) -> None:\\n\"\"\"Increment the usage in place.\\n  Args:\\n    incr_usage: The usage to increment by.\\n    requests: The number of requests to increment by in addition to `incr_usage.requests`.\\n  \"\"\"\\n  self.requests += requests\\n  for f in \\'requests\\', \\'request_tokens\\', \\'response_tokens\\', \\'total_tokens\\':\\n    self_value = getattr(self, f)\\n    other_value = getattr(incr_usage, f)\\n    if self_value is not None or other_value is not None:\\n      setattr(self, f, (self_value or 0) + (other_value or 0))\\n  if incr_usage.details:\\n    self.details = self.details or {}\\n    for key, value in incr_usage.details.items():\\n      self.details[key] = self.details.get(key, 0) + value'),\n",
       " Document(metadata={'chunk_id': 1, 'type': 'code', 'parent_text_id': 1, 'url': 'https://pydantic.com'}, page_content='__add__(other: Usage) -> Usage'),\n",
       " Document(metadata={'chunk_id': 1, 'type': 'code', 'parent_text_id': 1, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'chunk_id': 1, 'type': 'code', 'parent_text_id': 1, 'url': 'https://pydantic.com'}, page_content='def__add__(self, other: Usage) -> Usage:\\n\"\"\"Add two Usages together.\\n  This is provided so it\\'s trivial to sum usage information from multiple requests and runs.\\n  \"\"\"\\n  new_usage = copy(self)\\n  new_usage.incr(other)\\n  return new_usage'),\n",
       " Document(metadata={'chunk_id': 1, 'type': 'code', 'parent_text_id': 1, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'chunk_id': 1, 'type': 'code', 'parent_text_id': 1, 'url': 'https://pydantic.com'}, page_content='@dataclass\\nclassUsageLimits:\\n\"\"\"Limits on model usage.\\n  The request count is tracked by pydantic_ai, and the request limit is checked before each request to the model.\\n  Token counts are provided in responses from the model, and the token limits are checked after each response.\\n  Each of the limits can be set to `None` to disable that limit.\\n  \"\"\"\\n  request_limit: int | None = 50\\n\"\"\"The maximum number of requests allowed to the model.\"\"\"\\n  request_tokens_limit: int | None = None\\n\"\"\"The maximum number of tokens allowed in requests to the model.\"\"\"\\n  response_tokens_limit: int | None = None\\n\"\"\"The maximum number of tokens allowed in responses from the model.\"\"\"\\n  total_tokens_limit: int | None = None\\n\"\"\"The maximum number of tokens allowed in requests and responses combined.\"\"\"\\n  defhas_token_limits(self) -> bool:\\n\"\"\"Returns `True` if this instance places any limits on token counts.\\n    If this returns `False`, the `check_tokens` method will never raise an error.\\n    This is useful because if we have token limits, we need to check them after receiving each streamed message.\\n    If there are no limits, we can skip that processing in the streaming response iterator.\\n    \"\"\"\\n    return any(\\n      limit is not None\\n      for limit in (self.request_tokens_limit, self.response_tokens_limit, self.total_tokens_limit)\\n    )\\n  defcheck_before_request(self, usage: Usage) -> None:\\n\"\"\"Raises a `UsageLimitExceeded` exception if the next request would exceed the request_limit.\"\"\"\\n    request_limit = self.request_limit\\n    if request_limit is not None and usage.requests >= request_limit:\\n      raise UsageLimitExceeded(f\\'The next request would exceed the request_limit of {request_limit}\\')\\n  defcheck_tokens(self, usage: Usage) -> None:\\n\"\"\"Raises a `UsageLimitExceeded` exception if the usage exceeds any of the token limits.\"\"\"\\n    request_tokens = usage.request_tokens or 0\\n    if self.request_tokens_limit is not None and request_tokens > self.request_tokens_limit:\\n      raise UsageLimitExceeded(\\n        f\\'Exceeded the request_tokens_limit of {self.request_tokens_limit} ({request_tokens=})\\'\\n      )\\n    response_tokens = usage.response_tokens or 0\\n    if self.response_tokens_limit is not None and response_tokens > self.response_tokens_limit:\\n      raise UsageLimitExceeded(\\n        f\\'Exceeded the response_tokens_limit of {self.response_tokens_limit} ({response_tokens=})\\'\\n      )\\n    total_tokens = usage.total_tokens or 0\\n    if self.total_tokens_limit is not None and total_tokens > self.total_tokens_limit:\\n      raise UsageLimitExceeded(f\\'Exceeded the total_tokens_limit of {self.total_tokens_limit} ({total_tokens=})\\')'),\n",
       " Document(metadata={'chunk_id': 1, 'type': 'code', 'parent_text_id': 1, 'url': 'https://pydantic.com'}, page_content='request_limit: int | None = 50'),\n",
       " Document(metadata={'chunk_id': 1, 'type': 'code', 'parent_text_id': 1, 'url': 'https://pydantic.com'}, page_content='request_tokens_limit: int | None = None'),\n",
       " Document(metadata={'chunk_id': 1, 'type': 'code', 'parent_text_id': 1, 'url': 'https://pydantic.com'}, page_content='response_tokens_limit: int | None = None'),\n",
       " Document(metadata={'chunk_id': 1, 'type': 'code', 'parent_text_id': 1, 'url': 'https://pydantic.com'}, page_content='total_tokens_limit: int | None = None'),\n",
       " Document(metadata={'chunk_id': 1, 'type': 'code', 'parent_text_id': 1, 'url': 'https://pydantic.com'}, page_content='has_token_limits() -> bool'),\n",
       " Document(metadata={'chunk_id': 1, 'type': 'code', 'parent_text_id': 1, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'chunk_id': 1, 'type': 'code', 'parent_text_id': 1, 'url': 'https://pydantic.com'}, page_content='defhas_token_limits(self) -> bool:\\n\"\"\"Returns `True` if this instance places any limits on token counts.\\n  If this returns `False`, the `check_tokens` method will never raise an error.\\n  This is useful because if we have token limits, we need to check them after receiving each streamed message.\\n  If there are no limits, we can skip that processing in the streaming response iterator.\\n  \"\"\"\\n  return any(\\n    limit is not None\\n    for limit in (self.request_tokens_limit, self.response_tokens_limit, self.total_tokens_limit)\\n  )'),\n",
       " Document(metadata={'chunk_id': 1, 'type': 'code', 'parent_text_id': 1, 'url': 'https://pydantic.com'}, page_content='check_before_request(usage: Usage) -> None'),\n",
       " Document(metadata={'chunk_id': 1, 'type': 'code', 'parent_text_id': 1, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'chunk_id': 1, 'type': 'code', 'parent_text_id': 1, 'url': 'https://pydantic.com'}, page_content='defcheck_before_request(self, usage: Usage) -> None:\\n\"\"\"Raises a `UsageLimitExceeded` exception if the next request would exceed the request_limit.\"\"\"\\n  request_limit = self.request_limit\\n  if request_limit is not None and usage.requests >= request_limit:\\n    raise UsageLimitExceeded(f\\'The next request would exceed the request_limit of {request_limit}\\')'),\n",
       " Document(metadata={'chunk_id': 1, 'type': 'code', 'parent_text_id': 1, 'url': 'https://pydantic.com'}, page_content='check_tokens(usage: Usage) -> None'),\n",
       " Document(metadata={'chunk_id': 2, 'type': 'code', 'parent_text_id': 2, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'chunk_id': 2, 'type': 'code', 'parent_text_id': 2, 'url': 'https://pydantic.com'}, page_content='defcheck_tokens(self, usage: Usage) -> None:\\n\"\"\"Raises a `UsageLimitExceeded` exception if the usage exceeds any of the token limits.\"\"\"\\n  request_tokens = usage.request_tokens or 0\\n  if self.request_tokens_limit is not None and request_tokens > self.request_tokens_limit:\\n    raise UsageLimitExceeded(\\n      f\\'Exceeded the request_tokens_limit of {self.request_tokens_limit} ({request_tokens=})\\'\\n    )\\n  response_tokens = usage.response_tokens or 0\\n  if self.response_tokens_limit is not None and response_tokens > self.response_tokens_limit:\\n    raise UsageLimitExceeded(\\n      f\\'Exceeded the response_tokens_limit of {self.response_tokens_limit} ({response_tokens=})\\'\\n    )\\n  total_tokens = usage.total_tokens or 0\\n  if self.total_tokens_limit is not None and total_tokens > self.total_tokens_limit:\\n    raise UsageLimitExceeded(f\\'Exceeded the total_tokens_limit of {self.total_tokens_limit} ({total_tokens=})\\')')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = chunk_text(con[22])\n",
    "chunks[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in con:\n",
    "    chunks = chunk_text(page)\n",
    "    for i in chunks[0]:\n",
    "        total_text_chunks.append(i)\n",
    "    for i in chunks[1]:\n",
    "        total_code_chunks.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "931"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_code_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'chunk_id': 0, 'type': 'text', 'num_code_blocks': 0, 'url': 'https://pydantic.com'}, page_content=\"Why use PydanticAI  \\n  *  Hello World Example  \\n  *  Tools & Dependency Injection Example  \\n  *  Instrumentation with Pydantic Logfire  \\n  *  Next Steps  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# Introduction\\n\\n_Agent Framework / shim to use Pydantic with LLMs_\\n\\nPydanticAI is a Python agent framework designed to make it less painful to build production grade applications with Generative AI. \\nPydanticAI is a Python Agent Framework designed to make it less painful to build production grade applications with Generative AI.\\nFastAPI revolutionized web development by offering an innovative and ergonomic design, built on the foundation of Pydantic.\\nSimilarly, virtually every agent framework and LLM library in Python uses Pydantic, yet when we began to use LLMs in Pydantic Logfire, we couldn't find anything that gave us the same feeling.\\nWe built PydanticAI with one simple aim: to bring that FastAPI feeling to GenAI app development.\\n## Why use PydanticAI\\n  * **Built by the Pydantic Team** : Built by the team behind Pydantic (the validation layer of the OpenAI SDK, the Anthropic SDK, LangChain, LlamaIndex, AutoGPT, Transformers, CrewAI, Instructor and many more).\\n  * **Model-agnostic** : Supports OpenAI, Anthropic, Gemini, Deepseek, Ollama, Groq, Cohere, and Mistral, and there is a simple interface to implement support for other models.\\n  * **Pydantic Logfire Integration** : Seamlessly integrates with Pydantic Logfire for real-time debugging, performance monitoring, and behavior tracking of your LLM-powered applications.\\n  * **Type-safe** : Designed to make type checking as powerful and informative as possible for you.\\n  * **Python-centric Design** : Leverages Python's familiar control flow and agent composition to build your AI-driven projects, making it easy to apply standard Python best practices you'd use in any other (non-AI) projec\"),\n",
       " Document(metadata={'chunk_id': 1, 'type': 'text', 'num_code_blocks': 3, 'url': 'https://pydantic.com'}, page_content='t.\\n  * **Structured Responses** : Harnesses the power of Pydantic to validate and structure model outputs, ensuring responses are consistent across runs.\\n  * **Dependency Injection System** : Offers an optional dependency injection system to provide data and services to your agent\\'s system prompts, tools and result validators. This is useful for testing and eval-driven iterative development.\\n  * **Streamed Responses** : Provides the ability to stream LLM outputs continuously, with immediate validation, ensuring rapid and accurate results.\\n  * **Graph Support** : Pydantic Graph provides a powerful way to define graphs using typing hints, this is useful in complex applications where standard control flow can degrade to spaghetti code.\\n\\nIn Beta\\nPydanticAI is in early beta, the API is still subject to change and there\\'s a lot more to do. Feedback is very welcome!\\n## Hello World Example\\nHere\\'s a minimal example of PydanticAI:\\nhello_world.py\\n\\n_(This example is complete, it can be run \"as is\")_\\nThe exchange should be very short: PydanticAI will send the system prompt and the user query to the LLM, the model will return a text response.\\nNot very interesting yet, but we can easily add \"tools\", dynamic system prompts, and structured responses to build more powerful agents.\\n## Tools & Dependency Injection Example\\nHere is a concise example using PydanticAI to build a support agent for a bank:\\nbank_support.py\\n\\nComplete `bank_support.py` example\\nThe code included here is incomplete for the sake of brevity (the definition of `DatabaseConn` is missing); you can find the complete `bank_support.py` example here.\\n## Instrumentation with Pydantic Logfire\\nTo understand the flow of the above runs, we can watch the agent in action using Pydantic Logfire.\\nTo do this, we need to set up logfire, and add the following to our code:\\nbank_support_with_logfire.py'),\n",
       " Document(metadata={'chunk_id': 2, 'type': 'text', 'num_code_blocks': 0, 'url': 'https://pydantic.com'}, page_content=\"That's enough to get the following view of your agent in action:\\nSee Monitoring and Performance to learn more.\\n## Next Steps\\nTo try PydanticAI yourself, follow the instructions in the examples.\\nRead the docs to learn more about building applications with PydanticAI.\\nRead the API Reference to understand PydanticAI's interface.\\n© Pydantic Services Inc. 2024 to present\\n\\n---\"),\n",
       " Document(metadata={'chunk_id': 3, 'type': 'text', 'num_code_blocks': 0, 'url': 'https://pydantic.com'}, page_content=\"Introduction  \\n  *  Running Agents  \\n    *  Iterating Over an Agent's Graph  \\n      *  async for iteration  \\n      *  Using .next(...) manually  \\n      *  Accessing usage and the final result  \\n    *  Additional Configuration  \\n      *  Usage Limits  \\n      *  Model (Run) Settings  \\n    *  Model specific settings  \\n  *  Runs vs. Conversations  \\n  *  Type safe by design  \\n  *  System Prompts  \\n  *  Reflection and self-correction  \\n  *  Model errors  \\n\\n  1.  Introduction  \\n  2.  Documentation  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# Agents\\n## Introduction\\nAgents are PydanticAI's primary interface for interacting with LLMs.\\nIn some use cases a single Agent will control an entire application or component, but multiple agents can also interact to embody more complex workflows.\\nThe `Agent` class has full API documentation, but conceptually you can think of an agent as a container for:\\n**Component** | **Description**  \\n---|---  \\nSystem prompt(s) | A set of instructions for the LLM written by the developer.  \\nFunction tool(s) | Functions that the LLM may call to get information while generating a response.  \\nStructured result type | The structured datatype the LLM must return at the end of a run, if specified.  \\nDependency type constraint | System prompt functions, tools, and result validators may all use dependencies when they're run.  \\nLLM model | Optional default LLM model associated with the agent. Can also be specified when running the agent.  \\nModel Settings | Optional default model settings to help fine tune requests. Can also be specified when running the agent.  \\nIn typing terms, agents are generic in their dependency and result types, e.g., an agent which required dependencies of type `Foobar` and returned results of type `liststr]` would have type `Agent[Foobar, list[str]]`.\"),\n",
       " Document(metadata={'chunk_id': 4, 'type': 'text', 'num_code_blocks': 2, 'url': 'https://pydantic.com'}, page_content='In practice, you shouldn\\'t need to care about this, it should just mean your IDE can tell you when you have the right type, and if you choose to use [static type checking it should work well with PydanticAI.\\nHere\\'s a toy example of an agent that simulates a roulette wheel:\\nroulette_wheel.py\\n\\nAgents are designed for reuse, like FastAPI Apps\\nAgents are intended to be instantiated once (frequently as module globals) and reused throughout your application, similar to a small FastAPI app or an APIRouter.\\n## Running Agents\\nThere are four ways to run an agent:\\n  1. `agent.run()` — a coroutine which returns a `RunResult` containing a completed response.\\n  2. `agent.run_sync()` — a plain, synchronous function which returns a `RunResult` containing a completed response (internally, this just calls `loop.run_until_complete(self.run())`).\\n  3. `agent.run_stream()` — a coroutine which returns a `StreamedRunResult`, which contains methods to stream a response as an async iterable.\\n  4. `agent.iter()` — a context manager which returns an `AgentRun`, an async-iterable over the nodes of the agent\\'s underlying `Graph`.\\n\\nHere\\'s a simple example demonstrating the first three:\\nrun_agent.py\\n\\n_(This example is complete, it can be run \"as is\" — you\\'ll need to add`asyncio.run(main())` to run `main`)_\\nYou can also pass messages from previous runs to continue a conversation or provide context, as described in Messages and Chat History.\\n### Iterating Over an Agent\\'s Graph\\nUnder the hood, each `Agent` in PydanticAI uses **pydantic-graph** to manage its execution flow. **pydantic-graph** is a generic, type-centric library for building and running finite state machines in Python.'),\n",
       " Document(metadata={'chunk_id': 5, 'type': 'text', 'num_code_blocks': 2, 'url': 'https://pydantic.com'}, page_content=\"It doesn't actually depend on PydanticAI — you can use it standalone for workflows that have nothing to do with GenAI — but PydanticAI makes use of it to orchestrate the handling of model requests and model responses in an agent's run.\\nIn many scenarios, you don't need to worry about pydantic-graph at all; calling `agent.run(...)` simply traverses the underlying graph from start to finish. However, if you need deeper insight or control — for example to capture each tool invocation, or to inject your own logic at specific stages — PydanticAI exposes the lower-level iteration process via `Agent.iter`. This method returns an `AgentRun`, which you can async-iterate over, or manually drive node-by-node via the `next` method. Once the agent's graph returns an `End`, you have the final result along with a detailed history of all steps.\\n#### `async for` iteration\\nHere's an example of using `async for` with `iter` to record each node the agent executes:\\nagent_iter_async_for.py\\n\\n  * The `AgentRun` is an async iterator that yields each node (`BaseNode` or `End`) in the flow.\\n  * The run ends when an `End` node is returned.\\n\\n#### Using `.next(...)` manually\\nYou can also drive the iteration manually by passing the node you want to run next to the `AgentRun.next(...)` method. This allows you to inspect or modify the node before it executes or skip nodes based on your own logic, and to catch errors in `next()` more easily:\\nagent_iter_next.py\\n\\n#### Accessing usage and the final result\\nYou can retrieve usage statistics (tokens, requests, etc.) at any time from the `AgentRun` object via `agent_run.usage()`.\"),\n",
       " Document(metadata={'chunk_id': 6, 'type': 'text', 'num_code_blocks': 4, 'url': 'https://pydantic.com'}, page_content=\"This method returns a `Usage` object containing the usage data.\\nOnce the run finishes, `agent_run.final_result` becomes a `AgentRunResult` object containing the final output (and related metadata).\\n### Additional Configuration\\n#### Usage Limits\\nPydanticAI offers a `UsageLimits` structure to help you limit your usage (tokens and/or requests) on model runs.\\nYou can apply these settings by passing the `usage_limits` argument to the `run{_sync,_stream}` functions.\\nConsider the following example, where we limit the number of response tokens:\\n\\n\\nRestricting the number of requests can be useful in preventing infinite loops or excessive tool calling:\\n\\n\\nNote\\nThis is especially relevant if you've registered many tools. The `request_limit` can be used to prevent the model from calling them in a loop too many times.\\n#### Model (Run) Settings\\nPydanticAI offers a `settings.ModelSettings` structure to help you fine tune your requests. This structure allows you to configure common parameters that influence the model's behavior, such as `temperature`, `max_tokens`, `timeout`, and more.\\nThere are two ways to apply these settings: 1. Passing to `run{_sync,_stream}` functions via the `model_settings` argument. This allows for fine-tuning on a per-request basis. 2. Setting during `Agent` initialization via the `model_settings` argument. These settings will be applied by default to all subsequent run calls using said agent. However, `model_settings` provided during a specific run call will override the agent's default settings.\\nFor example, if you'd like to set the `temperature` setting to `0.0` to ensure less random behavior, you can do the following:\\n\\n\\n### Model specific settings\\nIf you wish to further customize model behavior, you can use a subclass of `ModelSettings`, like `GeminiModelSettings`, associated with your model of choice.\\nFor example:\\n\\n\\n## Runs vs.\"),\n",
       " Document(metadata={'chunk_id': 7, 'type': 'text', 'num_code_blocks': 3, 'url': 'https://pydantic.com'}, page_content='Conversations\\nAn agent **run** might represent an entire conversation — there\\'s no limit to how many messages can be exchanged in a single run. However, a **conversation** might also be composed of multiple runs, especially if you need to maintain state between separate interactions or API calls.\\nHere\\'s an example of a conversation comprised of multiple runs:\\nconversation_example.py\\n\\n_(This example is complete, it can be run \"as is\")_\\n## Type safe by design\\nPydanticAI is designed to work well with static type checkers, like mypy and pyright.\\nTyping is (somewhat) optional\\nPydanticAI is designed to make type checking as useful as possible for you if you choose to use it, but you don\\'t have to use types everywhere all the time.\\nThat said, because PydanticAI uses Pydantic, and Pydantic uses type hints as the definition for schema and validation, some types (specifically type hints on parameters to tools, and the `result_type` arguments to `Agent`) are used at runtime.\\nWe (the library developers) have messed up if type hints are confusing you more than helping you, if you find this, please create an issue explaining what\\'s annoying you!\\nIn particular, agents are generic in both the type of their dependencies and the type of results they return, so you can use the type hints to ensure you\\'re using the right types.\\nConsider the following script with type mistakes:\\ntype_mistakes.py\\n\\nRunning `mypy` on this will give the following output:\\n\\n\\nRunning `pyright` would identify the same issues.\\n## System Prompts\\nSystem prompts might seem simple at first glance since they\\'re just strings (or sequences of strings that are concatenated), but crafting the right system prompt is key to getting the model to behave as you want.\\nGenerally, system prompts fall into two categories:\\n  1. **Static system prompts** : These are known when writing the code and can be defined via the `system_prompt` parameter of the `Agent` constructor.\\n  2.'),\n",
       " Document(metadata={'chunk_id': 8, 'type': 'text', 'num_code_blocks': 3, 'url': 'https://pydantic.com'}, page_content='**Dynamic system prompts** : These depend in some way on context that isn\\'t known until runtime, and should be defined via functions decorated with `@agent.system_prompt`.\\n\\nYou can add both to a single agent; they\\'re appended in the order they\\'re defined at runtime.\\nHere\\'s an example using both types of system prompts:\\nsystem_prompts.py\\n\\n_(This example is complete, it can be run \"as is\")_\\n## Reflection and self-correction\\nValidation errors from both function tool parameter validation and structured result validation can be passed back to the model with a request to retry.\\nYou can also raise `ModelRetry` from within a tool or result validator function to tell the model it should retry generating a response.\\n  * The default retry count is **1** but can be altered for the entire agent, a specific tool, or a result validator.\\n  * You can access the current retry count from within a tool or result validator via `ctx.retry`.\\n\\nHere\\'s an example:\\ntool_retry.py\\n\\n## Model errors\\nIf models behave unexpectedly (e.g., the retry limit is exceeded, or their API returns `503`), agent runs will raise `UnexpectedModelBehavior`.\\nIn these cases, `capture_run_messages` can be used to access the messages exchanged during the run to help diagnose the issue.\\nagent_model_errors.py\\n\\n_(This example is complete, it can be run \"as is\")_\\nNote\\nIf you call `run`, `run_sync`, or `run_stream` more than once within a single `capture_run_messages` context, `messages` will represent the messages exchanged during the first call only.\\n© Pydantic Services Inc. 2024 to present\\n\\n---'),\n",
       " Document(metadata={'chunk_id': 9, 'type': 'text', 'num_code_blocks': 6, 'url': 'https://pydantic.com'}, page_content=\"Installation and Setup  \\n  *  Running Tests etc.  \\n  *  Documentation Changes  \\n  *  Rules for adding new models to PydanticAI  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# Contributing\\nWe'd love you to contribute to PydanticAI!\\n## Installation and Setup\\nClone your fork and cd into the repo directory\\n\\n\\nInstall `uv` (version 0.4.30 or later) and `pre-commit`\\nWe use pipx here, for other options see:\\n  * `uv` install docs\\n  * `pre-commit` install docs\\n\\nTo get `pipx` itself, see these docs\\n\\n\\nInstall `pydantic-ai`, all dependencies and pre-commit hooks\\n\\n\\n## Running Tests etc.\\nWe use `make` to manage most commands you'll need to run.\\nFor details on available commands, run:\\n\\n\\nTo run code formatting, linting, static type checks, and tests with coverage report generation, run:\\n\\n\\n## Documentation Changes\\nTo run the documentation page locally, run:\\n\\n\\n## Rules for adding new models to PydanticAI\\nTo avoid an excessive workload for the maintainers of PydanticAI, we can't accept all model contributions, so we're setting the following rules for when we'll accept new models and when we won't.\"),\n",
       " Document(metadata={'chunk_id': 10, 'type': 'text', 'num_code_blocks': 0, 'url': 'https://pydantic.com'}, page_content=\"This should hopefully reduce the chances of disappointment and wasted work.\\n  * To add a new model with an extra dependency, that dependency needs > 500k monthly downloads from PyPI consistently over 3 months or more\\n  * To add a new model which uses another models logic internally and has no extra dependencies, that model's GitHub org needs > 20k stars in total\\n  * For any other model that's just a custom URL and API key, we're happy to add a one-paragraph description with a link and instructions on the URL to use\\n  * For any other model that requires more logic, we recommend you release your own Python package `pydantic-ai-xxx`, which depends on `pydantic-ai-slim` and implements a model that inherits from our `Model` ABC\\n\\nIf you're unsure about adding a model, please create an issue.\\n© Pydantic Services Inc. 2024 to present\\n\\n---\"),\n",
       " Document(metadata={'chunk_id': 11, 'type': 'text', 'num_code_blocks': 2, 'url': 'https://pydantic.com'}, page_content='Defining Dependencies  \\n  *  Accessing Dependencies  \\n    *  Asynchronous vs. Synchronous dependencies  \\n  *  Full Example  \\n  *  Overriding Dependencies  \\n  *  Examples  \\n\\n  1.  Introduction  \\n  2.  Documentation  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# Dependencies\\nPydanticAI uses a dependency injection system to provide data and services to your agent\\'s system prompts, tools and result validators.\\nMatching PydanticAI\\'s design philosophy, our dependency system tries to use existing best practice in Python development rather than inventing esoteric \"magic\", this should make dependencies type-safe, understandable easier to test and ultimately easier to deploy in production.\\n## Defining Dependencies\\nDependencies can be any python type. While in simple cases you might be able to pass a single object as a dependency (e.g. an HTTP connection), dataclasses are generally a convenient container when your dependencies included multiple objects.\\nHere\\'s an example of defining an agent that requires dependencies.\\n(**Note:** dependencies aren\\'t actually used in this example, see Accessing Dependencies below)\\nunused_dependencies.py\\n\\n_(This example is complete, it can be run \"as is\" — you\\'ll need to add`asyncio.run(main())` to run `main`)_\\n## Accessing Dependencies\\nDependencies are accessed through the `RunContext` type, this should be the first parameter of system prompt functions etc.\\nsystem_prompt_dependencies.py\\n\\n_(This example is complete, it can be run \"as is\" — you\\'ll need to add`asyncio.run(main())` to run `main`)_\\n### Asynchronous vs. Synchronous dependencies\\nSystem prompt functions, function tools and result validators are all run in the async context of an agent run.\\nIf these functions are not coroutines (e.g.'),\n",
       " Document(metadata={'chunk_id': 12, 'type': 'text', 'num_code_blocks': 4, 'url': 'https://pydantic.com'}, page_content='`async def`) they are called with `run_in_executor` in a thread pool, it\\'s therefore marginally preferable to use `async` methods where dependencies perform IO, although synchronous dependencies should work fine too.\\n`run` vs. `run_sync` and Asynchronous vs. Synchronous dependencies\\nWhether you use synchronous or asynchronous dependencies, is completely independent of whether you use `run` or `run_sync` — `run_sync` is just a wrapper around `run` and agents are always run in an async context.\\nHere\\'s the same example as above, but with a synchronous dependency:\\nsync_dependencies.py\\n\\n_(This example is complete, it can be run \"as is\" — you\\'ll need to add`asyncio.run(main())` to run `main`)_\\n## Full Example\\nAs well as system prompts, dependencies can be used in tools and result validators.\\nfull_example.py\\n\\n_(This example is complete, it can be run \"as is\" — you\\'ll need to add`asyncio.run(main())` to run `main`)_\\n## Overriding Dependencies\\nWhen testing agents, it\\'s useful to be able to customise dependencies.\\nWhile this can sometimes be done by calling the agent directly within unit tests, we can also override dependencies while calling application code which in turn calls the agent.\\nThis is done via the `override` method on the agent.\\njoke_app.py\\n\\n_(This example is complete, it can be run \"as is\")_\\ntest_joke_app.py\\n\\n## Examples\\nThe following examples demonstrate how to use dependencies in PydanticAI:\\n  * Weather Agent\\n  * SQL Generation\\n  * RAG\\n\\n© Pydantic Services Inc. 2024 to present\\n\\n---'),\n",
       " Document(metadata={'chunk_id': 13, 'type': 'text', 'num_code_blocks': 0, 'url': 'https://pydantic.com'}, page_content=\"Installation  \\n  *  Graph Types  \\n    *  GraphRunContext  \\n    *  End  \\n    *  Nodes  \\n    *  Graph  \\n  *  Stateful Graphs  \\n  *  GenAI Example  \\n  *  Custom Control Flow  \\n  *  Iterating Over a Graph  \\n    *  Using Graph.iter for async for iteration  \\n    *  Using GraphRun.next(node) manually  \\n  *  Dependency Injection  \\n  *  Mermaid Diagrams  \\n    *  Setting Direction of the State Diagram  \\n\\n  1.  Introduction  \\n  2.  Documentation  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# Graphs\\nDon't use a nail gun unless you need a nail gun\\nIf PydanticAI agents are a hammer, and multi-agent workflows are a sledgehammer, then graphs are a nail gun:\\n  * sure, nail guns look cooler than hammers\\n  * but nail guns take a lot more setup than hammers\\n  * and nail guns don't make you a better builder, they make you a builder with a nail gun\\n  * Lastly, (and at the risk of torturing this metaphor), if you're a fan of medieval tools like mallets and untyped Python, you probably won't like nail guns or our approach to graphs. (But then again, if you're not a fan of type hints in Python, you've probably already bounced off PydanticAI to use one of the toy agent frameworks — good luck, and feel free to borrow my sledgehammer when you realize you need it)\\n\\nIn short, graphs are a powerful tool, but they're not the right tool for every job.\"),\n",
       " Document(metadata={'chunk_id': 14, 'type': 'text', 'num_code_blocks': 2, 'url': 'https://pydantic.com'}, page_content=\"Please consider other multi-agent approaches before proceeding.\\nIf you're not confident a graph-based approach is a good idea, it might be unnecessary.\\nGraphs and finite state machines (FSMs) are a powerful abstraction to model, execute, control and visualize complex workflows.\\nAlongside PydanticAI, we've developed `pydantic-graph` — an async graph and state machine library for Python where nodes and edges are defined using type hints.\\nWhile this library is developed as part of PydanticAI; it has no dependency on `pydantic-ai` and can be considered as a pure graph-based state machine library. You may find it useful whether or not you're using PydanticAI or even building with GenAI.\\n`pydantic-graph` is designed for advanced users and makes heavy use of Python generics and type hints. It is not designed to be as beginner-friendly as PydanticAI.\\nVery Early beta\\nGraph support was introduced in v0.0.19 and is in a very early beta. The API is subject to change. The documentation is incomplete. The implementation is incomplete.\\n## Installation\\n`pydantic-graph` is a required dependency of `pydantic-ai`, and an optional dependency of `pydantic-ai-slim`, see installation instructions for more information. You can also install it directly:\\npipuv\\n\\n\\n\\n\\n## Graph Types\\n`pydantic-graph` is made up of a few key components:\\n### GraphRunContext\\n`GraphRunContext` — The context for the graph run, similar to PydanticAI's `RunContext`.\"),\n",
       " Document(metadata={'chunk_id': 15, 'type': 'text', 'num_code_blocks': 2, 'url': 'https://pydantic.com'}, page_content=\"This holds the state of the graph and dependencies and is passed to nodes when they're run.\\n`GraphRunContext` is generic in the state type of the graph it's used in, `StateT`.\\n### End\\n`End` — return value to indicate the graph run should end.\\n`End` is generic in the graph return type of the graph it's used in, `RunEndT`.\\n### Nodes\\nSubclasses of `BaseNode` define nodes for execution in the graph.\\nNodes, which are generally `dataclass`es, generally consist of:\\n  * fields containing any parameters required/optional when calling the node\\n  * the business logic to execute the node, in the `run` method\\n  * return annotations of the `run` method, which are read by `pydantic-graph` to determine the outgoing edges of the node\\n\\nNodes are generic in:\\n  * **state** , which must have the same type as the state of graphs they're included in, `StateT` has a default of `None`, so if you're not using state you can omit this generic parameter, see stateful graphs for more information\\n  * **deps** , which must have the same type as the deps of the graph they're included in, `DepsT` has a default of `None`, so if you're not using deps you can omit this generic parameter, see dependency injection for more information\\n  * **graph return type** — this only applies if the node returns `End`. `RunEndT` has a default of Never so this generic parameter can be omitted if the node doesn't return `End`, but must be included if it does.\\n\\nHere's an example of a start or intermediate node in a graph — it can't end the run as it doesn't return `End`:\\nintermediate_node.py\\n\\nWe could extend `MyNode` to optionally end the run if `foo` is divisible by 5:\\nintermediate_or_end_node.py\"),\n",
       " Document(metadata={'chunk_id': 16, 'type': 'text', 'num_code_blocks': 8, 'url': 'https://pydantic.com'}, page_content='### Graph\\n`Graph` — this is the execution graph itself, made up of a set of node classes (i.e., `BaseNode` subclasses).\\n`Graph` is generic in:\\n  * **state** the state type of the graph, `StateT`\\n  * **deps** the deps type of the graph, `DepsT`\\n  * **graph return type** the return type of the graph run, `RunEndT`\\n\\nHere\\'s an example of a simple graph:\\ngraph_example.py\\n\\n_(This example is complete, it can be run \"as is\" with Python 3.10+)_\\nA mermaid diagram for this graph can be generated with the following code:\\ngraph_example_diagram.py\\n\\n\\n\\nIn order to visualize a graph within a `jupyter-notebook`, `IPython.display` needs to be used:\\njupyter_display_mermaid.py\\n\\n## Stateful Graphs\\nThe \"state\" concept in `pydantic-graph` provides an optional way to access and mutate an object (often a `dataclass` or Pydantic model) as nodes run in a graph. If you think of Graphs as a production line, then your state is the engine being passed along the line and built up by each node as the graph is run.\\nIn the future, we intend to extend `pydantic-graph` to provide state persistence with the state recorded after each node is run, see #695.\\nHere\\'s an example of a graph which represents a vending machine where the user may insert coins and select a product to purchase.\\nvending_machine.py\\n\\n_(This example is complete, it can be run \"as is\" with Python 3.10+ — you\\'ll need to add`asyncio.run(main())` to run `main`)_\\nA mermaid diagram for this graph can be generated with the following code:\\nvending_machine_diagram.py\\n\\nThe diagram generated by the above code is:\\n\\n\\nSee below for more information on generating diagrams.\\n## GenAI Example\\nSo far we haven\\'t shown an example of a Graph that actually uses PydanticAI or GenAI at all.\\nIn this example, one agent generates a welcome email to a user and the other agent provides feedback on the email.\\nThis graph has a very simple structure:'),\n",
       " Document(metadata={'chunk_id': 17, 'type': 'text', 'num_code_blocks': 6, 'url': 'https://pydantic.com'}, page_content='genai_email_feedback.py\\n\\n_(This example is complete, it can be run \"as is\" with Python 3.10+ — you\\'ll need to add`asyncio.run(main())` to run `main`)_\\n## Custom Control Flow\\nIn many real-world applications, Graphs cannot run uninterrupted from start to finish — they might require external input, or run over an extended period of time such that a single process cannot execute the entire graph run from start to finish without interruption.\\nIn these scenarios the `next` method can be used to run the graph one node at a time.\\nIn this example, an AI asks the user a question, the user provides an answer, the AI evaluates the answer and ends if the user got it right or asks another question if they got it wrong.\\n`ai_q_and_a_graph.py` — `question_graph` definition\\nai_q_and_a_graph.py\\n\\n_(This example is complete, it can be run \"as is\" with Python 3.10+)_\\nai_q_and_a_run.py\\n\\n_(This example is complete, it can be run \"as is\" with Python 3.10+ — you\\'ll need to add`asyncio.run(main())` to run `main`)_\\nA mermaid diagram for this graph can be generated with the following code:\\nai_q_and_a_diagram.py\\n\\n\\n\\nYou maybe have noticed that although this example transfers control flow out of the graph run, we\\'re still using rich\\'s `Prompt.ask` to get user input, with the process hanging while we wait for the user to enter a response. For an example of genuine out-of-process control flow, see the question graph example.\\n## Iterating Over a Graph\\n### Using `Graph.iter` for `async for` iteration\\nSometimes you want direct control or insight into each node as the graph executes. The easiest way to do that is with the `Graph.iter` method, which returns a **context manager** that yields a `GraphRun` object. The `GraphRun` is an async-iterable over the nodes of your graph, allowing you to record or modify them as they execute.\\nHere\\'s an example:\\ncount_down.py'),\n",
       " Document(metadata={'chunk_id': 18, 'type': 'text', 'num_code_blocks': 3, 'url': 'https://pydantic.com'}, page_content='### Using `GraphRun.next(node)` manually\\nAlternatively, you can drive iteration manually with the `GraphRun.next` method, which allows you to pass in whichever node you want to run next. You can modify or selectively skip nodes this way.\\nBelow is a contrived example that stops whenever the counter is at 2, ignoring any node runs beyond that:\\ncount_down_next.py\\n\\n## Dependency Injection\\nAs with PydanticAI, `pydantic-graph` supports dependency injection via a generic parameter on `Graph` and `BaseNode`, and the `GraphRunContext.deps` field.\\nAs an example of dependency injection, let\\'s modify the `DivisibleBy5` example above to use a `ProcessPoolExecutor` to run the compute load in a separate process (this is a contrived example, `ProcessPoolExecutor` wouldn\\'t actually improve performance in this example):\\ndeps_example.py\\n\\n_(This example is complete, it can be run \"as is\" with Python 3.10+ — you\\'ll need to add`asyncio.run(main())` to run `main`)_\\n## Mermaid Diagrams\\nPydantic Graph can generate mermaid `stateDiagram-v2` diagrams for graphs, as shown above.\\nThese diagrams can be generated with:\\n  * `Graph.mermaid_code` to generate the mermaid code for a graph\\n  * `Graph.mermaid_image` to generate an image of the graph using mermaid.ink\\n  * `Graph.mermaid_save` to generate an image of the graph using mermaid.ink and save it to a file\\n\\nBeyond the diagrams shown above, you can also customize mermaid diagrams with the following options:\\n  * `Edge` allows you to apply a label to an edge\\n  * `BaseNode.docstring_notes` and `BaseNode.get_note` allows you to add notes to nodes\\n  * The `highlighted_nodes` parameter allows you to highlight specific node(s) in the diagram\\n\\nPutting that together, we can edit the last `ai_q_and_a_graph.py` example to:\\n  * add labels to some edges\\n  * add a note to the `Ask` node\\n  * highlight the `Answer` node\\n  * save the diagram as a `PNG` image to file\\n\\nai_q_and_a_graph_extra.py'),\n",
       " Document(metadata={'chunk_id': 19, 'type': 'text', 'num_code_blocks': 3, 'url': 'https://pydantic.com'}, page_content=\"_(This example is not complete and cannot be run directly)_\\nThis would generate an image that looks like this:\\n\\n\\n### Setting Direction of the State Diagram\\nYou can specify the direction of the state diagram using one of the following values:\\n  * `'TB'`: Top to bottom, the diagram flows vertically from top to bottom.\\n  * `'LR'`: Left to right, the diagram flows horizontally from left to right.\\n  * `'RL'`: Right to left, the diagram flows horizontally from right to left.\\n  * `'BT'`: Bottom to top, the diagram flows vertically from bottom to top.\\n\\nHere is an example of how to do this using 'Left to Right' (LR) instead of the default 'Top to Bottom' (TB): \\nvending_machine_diagram.py\\n\\n\\n\\n© Pydantic Services Inc. 2024 to present\\n\\n---\"),\n",
       " Document(metadata={'chunk_id': 20, 'type': 'text', 'num_code_blocks': 0, 'url': 'https://pydantic.com'}, page_content=\"Slack  \\n  *  GitHub Issues  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# Getting Help\\nIf you need help getting started with PydanticAI or with advanced usage, the following sources may be useful.\\n##  Slack\\nJoin the `#pydantic-ai` channel in the Pydantic Slack to ask questions, get help, and chat about PydanticAI. There's also channels for Pydantic, Logfire, and FastUI.\\nIf you're on a Logfire Pro plan, you can also get a dedicated private slack collab channel with us.\\n##  GitHub Issues\\nThe PydanticAI GitHub Issues are a great place to ask questions and give us feedback.\\n© Pydantic Services Inc. 2024 to present\\n\\n---\"),\n",
       " Document(metadata={'chunk_id': 21, 'type': 'text', 'num_code_blocks': 8, 'url': 'https://pydantic.com'}, page_content='Use with Pydantic Logfire  \\n  *  Running Examples  \\n  *  Slim Install  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# Installation\\nPydanticAI is available on PyPI as `pydantic-ai` so installation is as simple as:\\npipuv\\n\\n\\n\\n\\n(Requires Python 3.9+)\\nThis installs the `pydantic_ai` package, core dependencies, and libraries required to use all the models included in PydanticAI. If you want to use a specific model, you can install the \"slim\" version of PydanticAI.\\n## Use with Pydantic Logfire\\nPydanticAI has an excellent (but completely optional) integration with Pydantic Logfire to help you view and understand agent runs.\\nTo use Logfire with PydanticAI, install `pydantic-ai` or `pydantic-ai-slim` with the `logfire` optional group:\\npipuv\\n\\n\\n\\n\\nFrom there, follow the Logfire setup docs to configure Logfire.\\n## Running Examples\\nWe distribute the `pydantic_ai_examples` directory as a separate PyPI package (`pydantic-ai-examples`) to make examples extremely easy to customize and run.\\nTo install examples, use the `examples` optional group:\\npipuv\\n\\n\\n\\n\\nTo run the examples, follow instructions in the examples docs.\\n## Slim Install\\nIf you know which model you\\'re going to use and want to avoid installing superfluous packages, you can use the `pydantic-ai-slim` package. For example, if you\\'re using just `OpenAIModel`, you would run:\\npipuv\\n\\n\\n\\n\\n`pydantic-ai-slim` has the following optional groups:\\n  * `logfire` — installs `logfire` PyPI ↗\\n  * `openai` — installs `openai` PyPI ↗\\n  * `vertexai` — installs `google-auth` PyPI ↗ and `requests` PyPI ↗\\n  * `anthropic` — installs `anthropic` PyPI ↗\\n  * `groq` — installs `groq` PyPI ↗\\n  * `mistral` — installs `mistralai` PyPI ↗\\n  * `cohere` - installs `cohere` PyPI ↗'),\n",
       " Document(metadata={'chunk_id': 22, 'type': 'text', 'num_code_blocks': 2, 'url': 'https://pydantic.com'}, page_content='See the models documentation for information on which optional dependencies are required for each model.\\nYou can also install dependencies for multiple models and use cases, for example:\\npipuv\\n\\n\\n\\n\\n© Pydantic Services Inc. 2024 to present\\n\\n---'),\n",
       " Document(metadata={'chunk_id': 23, 'type': 'text', 'num_code_blocks': 0, 'url': 'https://pydantic.com'}, page_content=\"Pydantic Logfire  \\n  *  Using Logfire  \\n    *  Debugging  \\n    *  Monitoring Performance  \\n    *  Monitoring HTTPX Requests  \\n\\n  1.  Introduction  \\n  2.  Documentation  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# Debugging and Monitoring\\nApplications that use LLMs have some challenges that are well known and understood: LLMs are **slow** , **unreliable** and **expensive**.\\nThese applications also have some challenges that most developers have encountered much less often: LLMs are **fickle** and **non-deterministic**. Subtle changes in a prompt can completely change a model's performance, and there's no `EXPLAIN` query you can run to understand why.\\nWarning\\nFrom a software engineers point of view, you can think of LLMs as the worst database you've ever heard of, but worse.\\nIf LLMs weren't so bloody useful, we'd never touch them.\\nTo build successful applications with LLMs, we need new tools to understand both model performance, and the behavior of applications that rely on them.\\nLLM Observability tools that just let you understand how your model is performing are useless: making API calls to an LLM is easy, it's building that into an application that's hard.\\n## Pydantic Logfire\\nPydantic Logfire is an observability platform developed by the team who created and maintain Pydantic and PydanticAI. Logfire aims to let you understand your entire application: Gen AI, classic predictive AI, HTTP traffic, database queries and everything else a modern application needs.\\nPydantic Logfire is a commercial product\\nLogfire is a commercially supported, hosted platform with an extremely generous and perpetual free tier.\"),\n",
       " Document(metadata={'chunk_id': 24, 'type': 'text', 'num_code_blocks': 7, 'url': 'https://pydantic.com'}, page_content=\"You can sign up and start using Logfire in a couple of minutes.\\nPydanticAI has built-in (but optional) support for Logfire via the `logfire-api` no-op package.\\nThat means if the `logfire` package is installed and configured, detailed information about agent runs is sent to Logfire. But if the `logfire` package is not installed, there's virtually no overhead and nothing is sent.\\nHere's an example showing details of running the Weather Agent in Logfire:\\n\\n## Using Logfire\\nTo use logfire, you'll need a logfire account, and logfire installed:\\npipuv\\n\\n\\n\\n\\nThen authenticate your local environment with logfire:\\npipuv\\n\\n\\n\\n\\nAnd configure a project to send data to:\\npipuv\\n\\n\\n\\n\\n(Or use an existing project with `logfire projects use`)\\nThe last step is to add logfire to your code:\\nadding_logfire.py\\n\\nThe logfire documentation has more details on how to use logfire, including how to instrument other libraries like Pydantic, HTTPX and FastAPI.\\nSince Logfire is build on OpenTelemetry, you can use the Logfire Python SDK to send data to any OpenTelemetry collector.\\nOnce you have logfire set up, there are two primary ways it can help you understand your application:\\n  * **Debugging** — Using the live view to see what's happening in your application in real-time.\\n  * **Monitoring** — Using SQL and dashboards to observe the behavior of your application, Logfire is effectively a SQL database that stores information about how your application is running.\\n\\n### Debugging\\nTo demonstrate how Logfire can let you visualise the flow of a PydanticAI run, here's the view you get from Logfire while running the chat app examples:\\n### Monitoring Performance\\nWe can also query data with SQL in Logfire to monitor the performance of an application. Here's a real world example of using Logfire to monitor PydanticAI runs inside Logfire itself:\"),\n",
       " Document(metadata={'chunk_id': 25, 'type': 'text', 'num_code_blocks': 2, 'url': 'https://pydantic.com'}, page_content=\"### Monitoring HTTPX Requests\\nIn order to monitor HTTPX requests made by models, you can use `logfire`'s HTTPX integration.\\nInstrumentation is as easy as adding the following three lines to your application:\\ninstrument_httpx.py\\n\\nIn particular, this can help you to trace specific requests, responses, and headers:\\ninstrument_httpx_example.py\\n\\nWith `httpx` instrumentationWithout `httpx` instrumentation\\n\\nTip\\n`httpx` instrumentation might be of particular utility if you're using a custom `httpx` client in your model in order to get insights into your custom requests.\\n© Pydantic Services Inc. 2024 to present\\n\\n---\"),\n",
       " Document(metadata={'chunk_id': 26, 'type': 'text', 'num_code_blocks': 2, 'url': 'https://pydantic.com'}, page_content='Accessing Messages from Results  \\n  *  Using Messages as Input for Further Agent Runs  \\n  *  Other ways of using messages  \\n  *  Examples  \\n\\n  1.  Introduction  \\n  2.  Documentation  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# Messages and chat history\\nPydanticAI provides access to messages exchanged during an agent run. These messages can be used both to continue a coherent conversation, and to understand how an agent performed.\\n### Accessing Messages from Results\\nAfter running an agent, you can access the messages exchanged during that run from the `result` object.\\nBoth `RunResult` (returned by `Agent.run`, `Agent.run_sync`) and `StreamedRunResult` (returned by `Agent.run_stream`) have the following methods:\\n  * `all_messages()`: returns all messages, including messages from prior runs. There\\'s also a variant that returns JSON bytes, `all_messages_json()`.\\n  * `new_messages()`: returns only the messages from the current run. There\\'s also a variant that returns JSON bytes, `new_messages_json()`.\\n\\nStreamedRunResult and complete messages\\nOn `StreamedRunResult`, the messages returned from these methods will only include the final result message once the stream has finished.\\nE.g. you\\'ve awaited one of the following coroutines:\\n  * `StreamedRunResult.stream()`\\n  * `StreamedRunResult.stream_text()`\\n  * `StreamedRunResult.stream_structured()`\\n  * `StreamedRunResult.get_data()`\\n\\n**Note:** The final result message will NOT be added to result messages if you use `.stream_text(delta=True)` since in this case the result content is never built as one string.\\nExample of accessing methods on a `RunResult` :\\nrun_result_messages.py\\n\\n_(This example is complete, it can be run \"as is\")_\\nExample of accessing methods on a `StreamedRunResult` :\\nstreamed_run_result_messages.py'),\n",
       " Document(metadata={'chunk_id': 27, 'type': 'text', 'num_code_blocks': 2, 'url': 'https://pydantic.com'}, page_content='_(This example is complete, it can be run \"as is\" — you\\'ll need to add`asyncio.run(main())` to run `main`)_\\n### Using Messages as Input for Further Agent Runs\\nThe primary use of message histories in PydanticAI is to maintain context across multiple agent runs.\\nTo use existing messages in a run, pass them to the `message_history` parameter of `Agent.run`, `Agent.run_sync` or `Agent.run_stream`.\\nIf `message_history` is set and not empty, a new system prompt is not generated — we assume the existing message history includes a system prompt.\\nReusing messages in a conversation\\n\\n_(This example is complete, it can be run \"as is\")_\\n## Other ways of using messages\\nSince messages are defined by simple dataclasses, you can manually create and manipulate, e.g. for testing.\\nThe message format is independent of the model used, so you can use messages in different agents, or the same agent with different models.\\nIn the example below, we reuse the message from the first agent run, which uses the `openai:gpt-4o` model, in a second agent run using the `google-gla:gemini-1.5-pro` model.\\nReusing messages with a different model\\n\\n## Examples\\nFor a more complete example of using messages in conversations, see the chat app example.\\n© Pydantic Services Inc. 2024 to present\\n\\n---'),\n",
       " Document(metadata={'chunk_id': 28, 'type': 'text', 'num_code_blocks': 0, 'url': 'https://pydantic.com'}, page_content='OpenAI  \\n    *  Install  \\n    *  Configuration  \\n    *  Environment variable  \\n    *  api_key argument  \\n    *  Custom OpenAI Client  \\n  *  Anthropic  \\n    *  Install  \\n    *  Configuration  \\n    *  Environment variable  \\n    *  api_key argument  \\n  *  Gemini  \\n    *  Install  \\n    *  Configuration  \\n    *  Environment variable  \\n    *  api_key argument  \\n  *  Gemini via VertexAI  \\n    *  Install  \\n    *  Configuration  \\n    *  Application default credentials  \\n    *  Service account  \\n    *  Customising region  \\n  *  Groq  \\n    *  Install  \\n    *  Configuration  \\n    *  Environment variable  \\n    *  api_key argument  \\n  *  Mistral  \\n    *  Install  \\n    *  Configuration  \\n    *  Environment variable  \\n    *  api_key argument  \\n  *  Cohere  \\n    *  Install  \\n    *  Configuration  \\n    *  Environment variable  \\n    *  api_key argument  \\n  *  OpenAI-compatible Models  \\n    *  Ollama  \\n      *  Example local usage  \\n      *  Example using a remote server  \\n    *  OpenRouter  \\n    *  Grok (xAI)  \\n    *  DeepSeek  \\n    *  Perplexity  \\n  *  Implementing Custom Models  \\n\\n  1.  Introduction  \\n  2.  Documentation  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# Models\\nPydanticAI is Model-agnostic and has built in support for the following model providers:\\n  * OpenAI\\n  * Anthropic\\n  * Gemini via two different APIs: Generative Language API and VertexAI API\\n  * Ollama\\n  * Deepseek\\n  * Groq\\n  * Mistral\\n  * Cohere'),\n",
       " Document(metadata={'chunk_id': 29, 'type': 'text', 'num_code_blocks': 9, 'url': 'https://pydantic.com'}, page_content=\"See OpenAI-compatible models for more examples on how to use models such as OpenRouter, and Grok (xAI) that support the OpenAI SDK.\\nYou can also add support for other models.\\nPydanticAI also comes with `TestModel` and `FunctionModel` for testing and development.\\nTo use each model provider, you need to configure your local environment and make sure you have the right packages installed.\\n## OpenAI\\n### Install\\nTo use OpenAI models, you need to either install `pydantic-ai`, or install `pydantic-ai-slim` with the `openai` optional group:\\npipuv\\n\\n\\n\\n\\n### Configuration\\nTo use `OpenAIModel` through their main API, go to platform.openai.com and follow your nose until you find the place to generate an API key.\\n### Environment variable\\nOnce you have the API key, you can set it as an environment variable:\\n\\n\\nYou can then use `OpenAIModel` by name:\\nopenai_model_by_name.py\\n\\nOr initialise the model directly with just the model name:\\nopenai_model_init.py\\n\\n### `api_key` argument\\nIf you don't want to or can't set the environment variable, you can pass it at runtime via the `api_key` argument:\\nopenai_model_api_key.py\\n\\n### Custom OpenAI Client\\n`OpenAIModel` also accepts a custom `AsyncOpenAI` client via the `openai_client` parameter, so you can customise the `organization`, `project`, `base_url` etc. as defined in the OpenAI API docs.\\nYou could also use the `AsyncAzureOpenAI` client to use the Azure OpenAI API.\\nopenai_azure.py\\n\\n## Anthropic\\n### Install\\nTo use `AnthropicModel` models, you need to either install `pydantic-ai`, or install `pydantic-ai-slim` with the `anthropic` optional group:\\npipuv\"),\n",
       " Document(metadata={'chunk_id': 30, 'type': 'text', 'num_code_blocks': 7, 'url': 'https://pydantic.com'}, page_content='### Configuration\\nTo use Anthropic through their API, go to console.anthropic.com/settings/keys to generate an API key.\\n`AnthropicModelName` contains a list of available Anthropic models.\\n### Environment variable\\nOnce you have the API key, you can set it as an environment variable:\\n\\n\\nYou can then use `AnthropicModel` by name:\\nanthropic_model_by_name.py\\n\\nOr initialise the model directly with just the model name:\\nanthropic_model_init.py\\n\\n### `api_key` argument\\nIf you don\\'t want to or can\\'t set the environment variable, you can pass it at runtime via the `api_key` argument:\\nanthropic_model_api_key.py\\n\\n## Gemini\\nFor prototyping only\\nGoogle themselves refer to this API as the \"hobby\" API, I\\'ve received 503 responses from it a number of times. The API is easy to use and useful for prototyping and simple demos, but I would not rely on it in production.\\nIf you want to run Gemini models in production, you should use the VertexAI API described below.\\n### Install\\nTo use `GeminiModel` models, you just need to install `pydantic-ai` or `pydantic-ai-slim`, no extra dependencies are required.\\n### Configuration\\n`GeminiModel` let\\'s you use the Google\\'s Gemini models through their Generative Language API, `generativelanguage.googleapis.com`.\\n`GeminiModelName` contains a list of available Gemini models that can be used through this interface.\\nTo use `GeminiModel`, go to aistudio.google.com and follow your nose until you find the place to generate an API key.\\n### Environment variable\\nOnce you have the API key, you can set it as an environment variable:\\n\\n\\nYou can then use `GeminiModel` by name:\\ngemini_model_by_name.py\\n\\nNote\\nThe `google-gla` provider prefix represents the Google **G** enerative **L** anguage **A** PI for `GeminiModel`s. `google-vertex` is used with Vertex AI for `VertexAIModel`s.\\nOr initialise the model directly with just the model name:\\ngemini_model_init.py'),\n",
       " Document(metadata={'chunk_id': 31, 'type': 'text', 'num_code_blocks': 4, 'url': 'https://pydantic.com'}, page_content='### `api_key` argument\\nIf you don\\'t want to or can\\'t set the environment variable, you can pass it at runtime via the `api_key` argument:\\ngemini_model_api_key.py\\n\\n## Gemini via VertexAI\\nTo run Google\\'s Gemini models in production, you should use `VertexAIModel` which uses the `*-aiplatform.googleapis.com` API.\\n`GeminiModelName` contains a list of available Gemini models that can be used through this interface.\\n### Install\\nTo use `VertexAIModel`, you need to either install `pydantic-ai`, or install `pydantic-ai-slim` with the `vertexai` optional group:\\npipuv\\n\\n\\n\\n\\n### Configuration\\nThis interface has a number of advantages over `generativelanguage.googleapis.com` documented above:\\n  1. The VertexAI API is more reliably and marginally lower latency in our experience.\\n  2. You can purchase provisioned throughput with VertexAI to guarantee capacity.\\n  3. If you\\'re running PydanticAI inside GCP, you don\\'t need to set up authentication, it should \"just work\".\\n  4. You can decide which region to use, which might be important from a regulatory perspective, and might improve latency.\\n\\nThe big disadvantage is that for local development you may need to create and configure a \"service account\", which I\\'ve found extremely painful to get right in the past.\\nWhichever way you authenticate, you\\'ll need to have VertexAI enabled in your GCP account.\\n### Application default credentials\\nLuckily if you\\'re running PydanticAI inside GCP, or you have the `gcloud` CLI installed and configured, you should be able to use `VertexAIModel` without any additional setup.\\nTo use `VertexAIModel`, with application default credentials configured (e.g. with `gcloud`), you can simply use:\\nvertexai_application_default_credentials.py'),\n",
       " Document(metadata={'chunk_id': 32, 'type': 'text', 'num_code_blocks': 5, 'url': 'https://pydantic.com'}, page_content='Internally this uses `google.auth.default()` from the `google-auth` package to obtain credentials.\\nWon\\'t fail until `agent.run()`\\nBecause `google.auth.default()` requires network requests and can be slow, it\\'s not run until you call `agent.run()`. Meaning any configuration or permissions error will only be raised when you try to use the model. To initialize the model for this check to be run, call `await model.ainit()`.\\nYou may also need to pass the `project_id` argument to `VertexAIModel` if application default credentials don\\'t set a project, if you pass `project_id` and it conflicts with the project set by application default credentials, an error is raised.\\n### Service account\\nIf instead of application default credentials, you want to authenticate with a service account, you\\'ll need to create a service account, add it to your GCP project (note: AFAIK this step is necessary even if you created the service account within the project), give that service account the \"Vertex AI Service Agent\" role, and download the service account JSON file.\\nOnce you have the JSON file, you can use it thus:\\nvertexai_service_account.py\\n\\n### Customising region\\nWhichever way you authenticate, you can specify which region requests will be sent to via the `region` argument.\\nUsing a region close to your application can improve latency and might be important from a regulatory perspective.\\nvertexai_region.py\\n\\n`VertexAiRegion` contains a list of available regions.\\n## Groq\\n### Install\\nTo use `GroqModel`, you need to either install `pydantic-ai`, or install `pydantic-ai-slim` with the `groq` optional group:\\npipuv\\n\\n\\n\\n\\n### Configuration\\nTo use Groq through their API, go to console.groq.com/keys and follow your nose until you find the place to generate an API key.\\n`GroqModelName` contains a list of available Groq models.\\n### Environment variable\\nOnce you have the API key, you can set it as an environment variable:'),\n",
       " Document(metadata={'chunk_id': 33, 'type': 'text', 'num_code_blocks': 14, 'url': 'https://pydantic.com'}, page_content=\"You can then use `GroqModel` by name:\\ngroq_model_by_name.py\\n\\nOr initialise the model directly with just the model name:\\ngroq_model_init.py\\n\\n### `api_key` argument\\nIf you don't want to or can't set the environment variable, you can pass it at runtime via the `api_key` argument:\\ngroq_model_api_key.py\\n\\n## Mistral\\n### Install\\nTo use `MistralModel`, you need to either install `pydantic-ai`, or install `pydantic-ai-slim` with the `mistral` optional group:\\npipuv\\n\\n\\n\\n\\n### Configuration\\nTo use Mistral through their API, go to console.mistral.ai/api-keys/ and follow your nose until you find the place to generate an API key.\\n`MistralModelName` contains a list of the most popular Mistral models.\\n### Environment variable\\nOnce you have the API key, you can set it as an environment variable:\\n\\n\\nYou can then use `MistralModel` by name:\\nmistral_model_by_name.py\\n\\nOr initialise the model directly with just the model name:\\nmistral_model_init.py\\n\\n### `api_key` argument\\nIf you don't want to or can't set the environment variable, you can pass it at runtime via the `api_key` argument:\\nmistral_model_api_key.py\\n\\n## Cohere\\n### Install\\nTo use `CohereModel`, you need to either install `pydantic-ai`, or install `pydantic-ai-slim` with the `cohere` optional group:\\npipuv\\n\\n\\n\\n\\n### Configuration\\nTo use Cohere through their API, go to dashboard.cohere.com/api-keys and follow your nose until you find the place to generate an API key.\\n`CohereModelName` contains a list of the most popular Cohere models.\\n### Environment variable\\nOnce you have the API key, you can set it as an environment variable:\\n\\n\\nYou can then use `CohereModel` by name:\\ncohere_model_by_name.py\\n\\nOr initialise the model directly with just the model name:\\ncohere_model_init.py\"),\n",
       " Document(metadata={'chunk_id': 34, 'type': 'text', 'num_code_blocks': 8, 'url': 'https://pydantic.com'}, page_content=\"### `api_key` argument\\nIf you don't want to or can't set the environment variable, you can pass it at runtime via the `api_key` argument:\\ncohere_model_api_key.py\\n\\n## OpenAI-compatible Models\\nMany of the models are compatible with OpenAI API, and thus can be used with `OpenAIModel` in PydanticAI. Before getting started, check the OpenAI section for installation and configuration instructions.\\nTo use another OpenAI-compatible API, you can make use of the `base_url` and `api_key` arguments:\\nopenai_model_base_url.py\\n\\n### Ollama\\nTo use Ollama, you must first download the Ollama client, and then download a model using the Ollama model library.\\nYou must also ensure the Ollama server is running when trying to make requests to it. For more information, please see the Ollama documentation.\\n#### Example local usage\\nWith `ollama` installed, you can run the server with the model you want to use:\\nterminal-run-ollama\\n\\n(this will pull the `llama3.2` model if you don't already have it downloaded) \\nThen run your code, here's a minimal example:\\nollama_example.py\\n\\n#### Example using a remote server\\nollama_example_with_remote_server.py\\n\\n### OpenRouter\\nTo use OpenRouter, first create an API key at openrouter.ai/keys.\\nOnce you have the API key, you can pass it to `OpenAIModel` as the `api_key` argument:\\nopenrouter_model_init.py\\n\\n### Grok (xAI)\\nGo to xAI API Console and create an API key. Once you have the API key, follow the xAI API Documentation, and set the `base_url` and `api_key` arguments appropriately:\\ngrok_model_init.py\\n\\n### DeepSeek\\nGo to DeepSeek API Platform and create an API key. Once you have the API key, follow the DeepSeek API Documentation, and set the `base_url` and `api_key` arguments appropriately:\\ndeepseek_model_init.py\\n\\n### Perplexity\\nFollow the Perplexity getting started guide to create an API key.\"),\n",
       " Document(metadata={'chunk_id': 35, 'type': 'text', 'num_code_blocks': 1, 'url': 'https://pydantic.com'}, page_content=\"Then, you can query the Perplexity API with the following:\\nperplexity_model_init.py\\n\\n## Implementing Custom Models\\nTo implement support for models not already supported, you will need to subclass the `Model` abstract base class.\\nFor streaming, you'll also need to implement the following abstract base class:\\n  * `StreamedResponse`\\n\\nThe best place to start is to review the source code for existing implementations, e.g. `OpenAIModel`.\\nFor details on when we'll accept contributions adding new models to PydanticAI, see the contributing guidelines.\\n© Pydantic Services Inc. 2024 to present\\n\\n---\"),\n",
       " Document(metadata={'chunk_id': 36, 'type': 'text', 'num_code_blocks': 2, 'url': 'https://pydantic.com'}, page_content='Agent delegation  \\n    *  Agent delegation and dependencies  \\n  *  Programmatic agent hand-off  \\n  *  Pydantic Graphs  \\n  *  Examples  \\n\\n  1.  Introduction  \\n  2.  Documentation  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# Multi-agent Applications\\nThere are roughly four levels of complexity when building applications with PydanticAI:\\n  1. Single agent workflows — what most of the `pydantic_ai` documentation covers\\n  2. Agent delegation — agents using another agent via tools\\n  3. Programmatic agent hand-off — one agent runs, then application code calls another agent\\n  4. Graph based control flow — for the most complex cases, a graph-based state machine can be used to control the execution of multiple agents\\n\\nOf course, you can combine multiple strategies in a single application.\\n## Agent delegation\\n\"Agent delegation\" refers to the scenario where an agent delegates work to another agent, then takes back control when the delegate agent (the agent called from within a tool) finishes.\\nSince agents are stateless and designed to be global, you do not need to include the agent itself in agent dependencies.\\nYou\\'ll generally want to pass `ctx.usage` to the `usage` keyword argument of the delegate agent run so usage within that run counts towards the total usage of the parent agent run.\\nMultiple models\\nAgent delegation doesn\\'t need to use the same model for each agent. If you choose to use different models within a run, calculating the monetary cost from the final `result.usage()` of the run will not be possible, but you can still use `UsageLimits` to avoid unexpected costs.\\nagent_delegation_simple.py\\n\\n_(This example is complete, it can be run \"as is\")_\\nThe control flow for this example is pretty simple and can be summarised as follows:'),\n",
       " Document(metadata={'chunk_id': 37, 'type': 'text', 'num_code_blocks': 4, 'url': 'https://pydantic.com'}, page_content='### Agent delegation and dependencies\\nGenerally the delegate agent needs to either have the same dependencies as the calling agent, or dependencies which are a subset of the calling agent\\'s dependencies.\\nInitializing dependencies\\nWe say \"generally\" above since there\\'s nothing to stop you initializing dependencies within a tool call and therefore using interdependencies in a delegate agent that are not available on the parent, this should often be avoided since it can be significantly slower than reusing connections etc. from the parent agent.\\nagent_delegation_deps.py\\n\\n_(This example is complete, it can be run \"as is\" — you\\'ll need to add`asyncio.run(main())` to run `main`)_\\nThis example shows how even a fairly simple agent delegation can lead to a complex control flow:\\n\\n\\n## Programmatic agent hand-off\\n\"Programmatic agent hand-off\" refers to the scenario where multiple agents are called in succession, with application code and/or a human in the loop responsible for deciding which agent to call next.\\nHere agents don\\'t need to use the same deps.\\nHere we show two agents used in succession, the first to find a flight and the second to extract the user\\'s seat preference.\\nprogrammatic_handoff.py\\n\\n_(This example is complete, it can be run \"as is\" — you\\'ll need to add`asyncio.run(main())` to run `main`)_\\nThe control flow for this example can be summarised as follows:\\n\\n\\n## Pydantic Graphs\\nSee the graph documentation on when and how to use graphs.\\n## Examples\\nThe following examples demonstrate how to use dependencies in PydanticAI:\\n  * Flight booking\\n\\n© Pydantic Services Inc. 2024 to present\\n\\n---'),\n",
       " Document(metadata={'chunk_id': 38, 'type': 'text', 'num_code_blocks': 1, 'url': 'https://pydantic.com'}, page_content='Result data  \\n    *  Result validators functions  \\n  *  Streamed Results  \\n    *  Streaming Text  \\n    *  Streaming Structured Responses  \\n  *  Examples  \\n\\n  1.  Introduction  \\n  2.  Documentation  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# Results\\nResults are the final values returned from running an agent. The result values are wrapped in `AgentRunResult` and `StreamedRunResult` so you can access other data like usage of the run and message history\\nBoth `RunResult` and `StreamedRunResult` are generic in the data they wrap, so typing information about the data returned by the agent is preserved.\\nolympics.py\\n\\n_(This example is complete, it can be run \"as is\")_\\nRuns end when either a plain text response is received or the model calls a tool associated with one of the structured result types. We will add limits to make sure a run doesn\\'t go on indefinitely, see #70.\\n## Result data\\nWhen the result type is `str`, or a union including `str`, plain text responses are enabled on the model, and the raw text response from the model is used as the response data.\\nIf the result type is a union with multiple members (after remove `str` from the members), each member is registered as a separate tool with the model in order to reduce the complexity of the tool schemas and maximise the chances a model will respond correctly.\\nIf the result type schema is not of type `\"object\"`, the result type is wrapped in a single element object, so the schema of all tools registered with the model are object schemas.\\nStructured results (like tools) use Pydantic to build the JSON schema used for the tool, and to validate the data returned by the model.\\nBring on PEP-747\\nUntil PEP-747 \"Annotating Type Forms\" lands, unions are not valid as `type`s in Python.\\nWhen creating the agent we need to `# type: ignore` the `result_type` argument, and add a typ'),\n",
       " Document(metadata={'chunk_id': 39, 'type': 'text', 'num_code_blocks': 5, 'url': 'https://pydantic.com'}, page_content='e hint to tell type checkers about the type of the agent.\\nHere\\'s an example of returning either text or a structured value\\nbox_or_error.py\\n\\n_(This example is complete, it can be run \"as is\")_\\nHere\\'s an example of using a union return type which registered multiple tools, and wraps non-object schemas in an object:\\ncolors_or_sizes.py\\n\\n_(This example is complete, it can be run \"as is\")_\\n### Result validators functions\\nSome validation is inconvenient or impossible to do in Pydantic validators, in particular when the validation requires IO and is asynchronous. PydanticAI provides a way to add validation functions via the `agent.result_validator` decorator.\\nHere\\'s a simplified variant of the SQL Generation example:\\nsql_gen.py\\n\\n_(This example is complete, it can be run \"as is\")_\\n## Streamed Results\\nThere two main challenges with streamed results:\\n  1. Validating structured responses before they\\'re complete, this is achieved by \"partial validation\" which was recently added to Pydantic in pydantic/pydantic#10748.\\n  2. When receiving a response, we don\\'t know if it\\'s the final response without starting to stream it and peeking at the content. PydanticAI streams just enough of the response to sniff out if it\\'s a tool call or a result, then streams the whole thing and calls tools, or returns the stream as a `StreamedRunResult`.\\n\\n### Streaming Text\\nExample of streamed text result:\\nstreamed_hello_world.py\\n\\n_(This example is complete, it can be run \"as is\" — you\\'ll need to add`asyncio.run(main())` to run `main`)_\\nWe can also stream text as deltas rather than the entire text in each item:\\nstreamed_delta_hello_world.py'),\n",
       " Document(metadata={'chunk_id': 40, 'type': 'text', 'num_code_blocks': 2, 'url': 'https://pydantic.com'}, page_content='_(This example is complete, it can be run \"as is\" — you\\'ll need to add`asyncio.run(main())` to run `main`)_\\nResult message not included in `messages`\\nThe final result message will **NOT** be added to result messages if you use `.stream_text(delta=True)`, see Messages and chat history for more information.\\n### Streaming Structured Responses\\nNot all types are supported with partial validation in Pydantic, see pydantic/pydantic#10748, generally for model-like structures it\\'s currently best to use `TypeDict`.\\nHere\\'s an example of streaming a use profile as it\\'s built:\\nstreamed_user_profile.py\\n\\n_(This example is complete, it can be run \"as is\" — you\\'ll need to add`asyncio.run(main())` to run `main`)_\\nIf you want fine-grained control of validation, particularly catching validation errors, you can use the following pattern:\\nstreamed_user_profile.py\\n\\n_(This example is complete, it can be run \"as is\" — you\\'ll need to add`asyncio.run(main())` to run `main`)_\\n## Examples\\nThe following examples demonstrate how to use streamed responses in PydanticAI:\\n  * Stream markdown\\n  * Stream Whales\\n\\n© Pydantic Services Inc. 2024 to present\\n\\n---'),\n",
       " Document(metadata={'chunk_id': 41, 'type': 'text', 'num_code_blocks': 0, 'url': 'https://pydantic.com'}, page_content=\"Unit tests  \\n    *  Unit testing with TestModel  \\n    *  Unit testing with FunctionModel  \\n    *  Overriding model via pytest fixtures  \\n  *  Evals  \\n    *  Measuring performance  \\n    *  System prompt customization  \\n\\n  1.  Introduction  \\n  2.  Documentation  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# Testing and Evals\\nWith PydanticAI and LLM integrations in general, there are two distinct kinds of test:\\n  1. **Unit tests** — tests of your application code, and whether it's behaving correctly\\n  2. **Evals** — tests of the LLM, and how good or bad its responses are\\n\\nFor the most part, these two kinds of tests have pretty separate goals and considerations.\\n## Unit tests\\nUnit tests for PydanticAI code are just like unit tests for any other Python code.\\nBecause for the most part they're nothing new, we have pretty well established tools and patterns for writing and running these kinds of tests.\\nUnless you're really sure you know better, you'll probably want to follow roughly this strategy:\\n  * Use `pytest` as your test harness\\n  * If you find yourself typing out long assertions, use inline-snapshot\\n  * Similarly, dirty-equals can be useful for comparing large data structures\\n  * Use `TestModel` or `FunctionModel` in place of your actual model to avoid the usage, latency and variability of real LLM calls\\n  * Use `Agent.override` to replace your model inside your application logic\\n  * Set `ALLOW_MODEL_REQUESTS=False` globally to block any requests from being made to non-test models accidentally\"),\n",
       " Document(metadata={'chunk_id': 42, 'type': 'text', 'num_code_blocks': 3, 'url': 'https://pydantic.com'}, page_content='### Unit testing with `TestModel`\\nThe simplest and fastest way to exercise most of your application code is using `TestModel`, this will (by default) call all tools in the agent, then return either plain text or a structured response depending on the return type of the agent.\\n`TestModel` is not magic\\nThe \"clever\" (but not too clever) part of `TestModel` is that it will attempt to generate valid structured data for function tools and result types based on the schema of the registered tools.\\nThere\\'s no ML or AI in `TestModel`, it\\'s just plain old procedural Python code that tries to generate data that satisfies the JSON schema of a tool.\\nThe resulting data won\\'t look pretty or relevant, but it should pass Pydantic\\'s validation in most cases. If you want something more sophisticated, use `FunctionModel` and write your own data generation logic.\\nLet\\'s write unit tests for the following application code:\\nweather_app.py\\n\\nHere we have a function that takes a list of `(user_prompt, user_id)` tuples, gets a weather forecast for each prompt, and stores the result in the database.\\n**We want to test this code without having to mock certain objects or modify our code so we can pass test objects in.**\\nHere\\'s how we would write tests using `TestModel`:\\ntest_weather_app.py\\n\\n### Unit testing with `FunctionModel`\\nThe above tests are a great start, but careful readers will notice that the `WeatherService.get_forecast` is never called since `TestModel` calls `weather_forecast` with a date in the past.\\nTo fully exercise `weather_forecast`, we need to use `FunctionModel` to customise how the tools is called.\\nHere\\'s an example of using `FunctionModel` to test the `weather_forecast` tool with custom inputs\\ntest_weather_app2.py'),\n",
       " Document(metadata={'chunk_id': 43, 'type': 'text', 'num_code_blocks': 1, 'url': 'https://pydantic.com'}, page_content='### Overriding model via pytest fixtures\\nIf you\\'re writing lots of tests that all require model to be overridden, you can use pytest fixtures to override the model with `TestModel` or `FunctionModel` in a reusable way.\\nHere\\'s an example of a fixture that overrides the model with `TestModel`:\\ntests.py\\n\\n## Evals\\n\"Evals\" refers to evaluating a models performance for a specific application.\\nWarning\\nUnlike unit tests, evals are an emerging art/science; anyone who claims to know for sure exactly how your evals should be defined can safely be ignored.\\nEvals are generally more like benchmarks than unit tests, they never \"pass\" although they do \"fail\"; you care mostly about how they change over time.\\nSince evals need to be run against the real model, then can be slow and expensive to run, you generally won\\'t want to run them in CI for every commit.\\n### Measuring performance\\nThe hardest part of evals is measuring how well the model has performed.\\nIn some cases (e.g. an agent to generate SQL) there are simple, easy to run tests that can be used to measure performance (e.g. is the SQL valid? Does it return the right results? Does it return just the right results?).\\nIn other cases (e.g.'),\n",
       " Document(metadata={'chunk_id': 44, 'type': 'text', 'num_code_blocks': 0, 'url': 'https://pydantic.com'}, page_content=\"an agent that gives advice on quitting smoking) it can be very hard or impossible to make quantitative measures of performance — in the smoking case you'd really need to run a double-blind trial over months, then wait 40 years and observe health outcomes to know if changes to your prompt were an improvement.\\nThere are a few different strategies you can use to measure performance:\\n  * **End to end, self-contained tests** — like the SQL example, we can test the final result of the agent near-instantly\\n  * **Synthetic self-contained tests** — writing unit test style checks that the output is as expected, checks like `'chewing gum' in response`, while these checks might seem simplistic they can be helpful, one nice characteristic is that it's easy to tell what's wrong when they fail\\n  * **LLMs evaluating LLMs** — using another models, or even the same model with a different prompt to evaluate the performance of the agent (like when the class marks each other's homework because the teacher has a hangover), while the downsides and complexities of this approach are obvious, some think it can be a useful tool in the right circumstances\\n  * **Evals in prod** — measuring the end results of the agent in production, then creating a quantitative measure of performance, so you can easily measure changes over time as you change the prompt or model used, logfire can be extremely useful in this case since you can write a custom query to measure the performance of your agent\\n\\n### System prompt customization\\nThe system prompt is the developer's primary tool in controlling an agent's behavior, so it's often useful to be able to customise the system prompt and see how performance changes.\"),\n",
       " Document(metadata={'chunk_id': 45, 'type': 'text', 'num_code_blocks': 4, 'url': 'https://pydantic.com'}, page_content='This is particularly relevant when the system prompt contains a list of examples and you want to understand how changing that list affects the model\\'s performance.\\nLet\\'s assume we have the following app for running SQL generated from a user prompt (this examples omits a lot of details for brevity, see the SQL gen example for a more complete code):\\nsql_app.py\\n\\n`examples.json` looks something like this:\\n\\n\\nexamples.json\\n\\nNow we want a way to quantify the success of the SQL generation so we can judge how changes to the agent affect its performance.\\nWe can use `Agent.override` to replace the system prompt with a custom one that uses a subset of examples, and then run the application code (in this case `user_search`). We also run the actual SQL from the examples and compare the \"correct\" result from the example SQL to the SQL generated by the agent. (We compare the results of running the SQL rather than the SQL itself since the SQL might be semantically equivalent but written in a different way).\\nTo get a quantitative measure of performance, we assign points to each run as follows:\\n  * **-100** points if the generated SQL is invalid\\n  * **-1** point for each row returned by the agent (so returning lots of results is discouraged)\\n  * **+5** points for each row returned by the agent that matches the expected result\\n\\nWe use 5-fold cross-validation to judge the performance of the agent using our existing set of examples.\\nsql_app_evals.py\\n\\nWe can then change the prompt, the model, or the examples and see how the score changes over time.\\n© Pydantic Services Inc. 2024 to present\\n\\n---'),\n",
       " Document(metadata={'chunk_id': 46, 'type': 'text', 'num_code_blocks': 3, 'url': 'https://pydantic.com'}, page_content='Registering Function Tools via kwarg  \\n  *  Function Tools vs. Structured Results  \\n  *  Function tools and schema  \\n  *  Dynamic Function tools  \\n\\n  1.  Introduction  \\n  2.  Documentation  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# Function Tools\\nFunction tools provide a mechanism for models to retrieve extra information to help them generate a response.\\nThey\\'re useful when it is impractical or impossible to put all the context an agent might need into the system prompt, or when you want to make agents\\' behavior more deterministic or reliable by deferring some of the logic required to generate a response to another (not necessarily AI-powered) tool.\\nFunction tools vs. RAG\\nFunction tools are basically the \"R\" of RAG (Retrieval-Augmented Generation) — they augment what the model can do by letting it request extra information.\\nThe main semantic difference between PydanticAI Tools and RAG is RAG is synonymous with vector search, while PydanticAI tools are more general-purpose. (Note: we may add support for vector search functionality in the future, particularly an API for generating embeddings. See #58)\\nThere are a number of ways to register tools with an agent:\\n  * via the `@agent.tool` decorator — for tools that need access to the agent context\\n  * via the `@agent.tool_plain` decorator — for tools that do not need access to the agent context\\n  * via the `tools` keyword argument to `Agent` which can take either plain functions, or instances of `Tool`\\n\\n`@agent.tool` is considered the default decorator since in the majority of cases tools will need access to the agent context.\\nHere\\'s an example using both:\\ndice_game.py\\n\\n_(This example is complete, it can be run \"as is\")_\\nLet\\'s print the messages from that game to see what happened:\\ndice_game_messages.py\\n\\nWe can represent this with a diagram:'),\n",
       " Document(metadata={'chunk_id': 47, 'type': 'text', 'num_code_blocks': 2, 'url': 'https://pydantic.com'}, page_content='## Registering Function Tools via kwarg\\nAs well as using the decorators, we can register tools via the `tools` argument to the `Agent` constructor. This is useful when you want to reuse tools, and can also give more fine-grained control over the tools.\\ndice_game_tool_kwarg.py\\n\\n_(This example is complete, it can be run \"as is\")_\\n## Function Tools vs. Structured Results\\nAs the name suggests, function tools use the model\\'s \"tools\" or \"functions\" API to let the model know what is available to call. Tools or functions are also used to define the schema(s) for structured responses, thus a model might have access to many tools, some of which call function tools while others end the run and return a result.\\n## Function tools and schema\\nFunction parameters are extracted from the function signature, and all parameters except `RunContext` are used to build the schema for that tool call.\\nEven better, PydanticAI extracts the docstring from functions and (thanks to griffe) extracts parameter descriptions from the docstring and adds them to the schema.\\nGriffe supports extracting parameter descriptions from `google`, `numpy`, and `sphinx` style docstrings. PydanticAI will infer the format to use based on the docstring, but you can explicitly set it using `docstring_format`. You can also enforce parameter requirements by setting `require_parameter_descriptions=True`. This will raise a `UserError` if a parameter description is missing.\\nTo demonstrate a tool\\'s schema, here we use `FunctionModel` to print the schema a model would receive:\\ntool_schema.py\\n\\n_(This example is complete, it can be run \"as is\")_\\nThe return type of tool can be anything which Pydantic can serialize to JSON as some models (e.g. Gemini) support semi-structured return values, some expect text (OpenAI) but seem to be just as good at extracting meaning from the data.'),\n",
       " Document(metadata={'chunk_id': 48, 'type': 'text', 'num_code_blocks': 3, 'url': 'https://pydantic.com'}, page_content='If a Python object is returned and the model expects a string, the value will be serialized to JSON.\\nIf a tool has a single parameter that can be represented as an object in JSON schema (e.g. dataclass, TypedDict, pydantic model), the schema for the tool is simplified to be just that object.\\nHere\\'s an example where we use `TestModel.last_model_request_parameters` to inspect the tool schema that would be passed to the model.\\nsingle_parameter_tool.py\\n\\n_(This example is complete, it can be run \"as is\")_\\n## Dynamic Function tools\\nTools can optionally be defined with another function: `prepare`, which is called at each step of a run to customize the definition of the tool passed to the model, or omit the tool completely from that step.\\nA `prepare` method can be registered via the `prepare` kwarg to any of the tool registration mechanisms:\\n  * `@agent.tool` decorator\\n  * `@agent.tool_plain` decorator\\n  * `Tool` dataclass\\n\\nThe `prepare` method, should be of type `ToolPrepareFunc`, a function which takes `RunContext` and a pre-built `ToolDefinition`, and should either return that `ToolDefinition` with or without modifying it, return a new `ToolDefinition`, or return `None` to indicate this tools should not be registered for that step.\\nHere\\'s a simple `prepare` method that only includes the tool if the value of the dependency is `42`.\\nAs with the previous example, we use `TestModel` to demonstrate the behavior without calling a real model.\\ntool_only_if_42.py\\n\\n_(This example is complete, it can be run \"as is\")_\\nHere\\'s a more complex example where we change the description of the `name` parameter to based on the value of `deps`\\nFor the sake of variation, we create this tool using the `Tool` dataclass.\\ncustomize_name.py\\n\\n_(This example is complete, it can be run \"as is\")_\\n© Pydantic Services Inc. 2024 to present\\n\\n---'),\n",
       " Document(metadata={'chunk_id': 49, 'type': 'text', 'num_code_blocks': 1, 'url': 'https://pydantic.com'}, page_content=\"Jupyter Notebook Errors  \\n    *  RuntimeError: This event loop is already running  \\n  *  API Key Configuration  \\n    *  UserError: API key must be provided or set in the [MODEL]_API_KEY environment variable  \\n  *  Monitoring HTTPX Requests  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# Troubleshooting\\nBelow are suggestions on how to fix some common errors you might encounter while using PydanticAI. If the issue you're experiencing is not listed below or addressed in the documentation, please feel free to ask in the Pydantic Slack or create an issue on GitHub.\\n## Jupyter Notebook Errors\\n### `RuntimeError: This event loop is already running`\\nThis error is caused by conflicts between the event loops in Jupyter notebook and PydanticAI's. One way to manage these conflicts is by using `nest-asyncio`. Namely, before you execute any agent runs, do the following: \\n\\n\\nNote: This fix also applies to Google Colab. \\n## API Key Configuration\\n### `UserError: API key must be provided or set in the [MODEL]_API_KEY environment variable`\\nIf you're running into issues with setting the API key for your model, visit the Models page to learn more about how to set an environment variable and/or pass in an `api_key` argument.\\n## Monitoring HTTPX Requests\\nYou can use custom `httpx` clients in your models in order to access specific requests, responses, and headers at runtime.\\nIt's particularly helpful to use `logfire`'s HTTPX integration to monitor the above.\\n© Pydantic Services Inc. 2024 to present\\n\\n---\"),\n",
       " Document(metadata={'chunk_id': 50, 'type': 'text', 'num_code_blocks': 1, 'url': 'https://pydantic.com'}, page_content='agent  \\n  *  Agent  \\n    *  model  \\n    *  __init__  \\n    *  end_strategy  \\n    *  name  \\n    *  model_settings  \\n    *  result_type  \\n    *  run  \\n    *  iter  \\n    *  run_sync  \\n    *  run_stream  \\n    *  override  \\n    *  system_prompt  \\n    *  result_validator  \\n    *  tool  \\n    *  tool_plain  \\n  *  AgentRun  \\n    *  ctx  \\n    *  next_node  \\n    *  result  \\n    *  __aiter__  \\n    *  __anext__  \\n    *  next  \\n    *  usage  \\n  *  AgentRunResult  \\n    *  all_messages  \\n    *  all_messages_json  \\n    *  new_messages  \\n    *  new_messages_json  \\n    *  usage  \\n  *  EndStrategy  \\n  *  RunResultDataT  \\n  *  capture_run_messages  \\n\\n  1.  Introduction  \\n  2.  API Reference  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# `pydantic_ai.agent`\\n###  Agent `dataclass`\\nBases: `GenericAgentDepsT[, ResultDataT]`\\nClass for defining \"agents\" - a way to have a specific type of \"conversation\" with an LLM.\\nAgents are generic in the dependency type they take `AgentDepsT` and the result data type they return, `ResultDataT`.\\nBy default, if neither generic parameter is customised, agents have type `Agent[None, str]`.\\nMinimal usage example:'),\n",
       " Document(metadata={'chunk_id': 51, 'type': 'text', 'num_code_blocks': 7, 'url': 'https://pydantic.com'}, page_content=\"Source code in `pydantic_ai_slim/pydantic_ai/agent.py`\\n\\n| python\\n  from pydantic_ai import Agent\\n  agent = Agent('openai:gpt-4o')\\n  result = agent.run_sync('What is the capital of France?')\\n  print(result.data)\\n  #> Paris\\n  python\\n    from pydantic_ai import Agent\\n    agent = Agent('openai:gpt-4o')\\n    async def main():\\n      agent_run = await agent.run('What is the capital of France?')\\n      print(agent_run.data)\\n      #> Paris\\npython\\n    from pydantic_ai import Agent\\n    agent = Agent('openai:gpt-4o')\\n    async def main():\\n      nodes = []\\n      with agent.iter('What is the capital of France?') as agent_run:\\n        async for node in agent_run:\\n          nodes.append(node)\\n      print(nodes)\\n      '''\\n      [\\n        ModelRequestNode(\\n          request=ModelRequest(\\n            parts=[\\n              UserPromptPart(\\n                content='What is the capital of France?',\\n                timestamp=datetime.datetime(...),\\n                part_kind='user-prompt',\\n              )\\n            ],\\n            kind='request',\\n          )\\n        ),\\n        HandleResponseNode(\\n          model_response=ModelResponse(\\n            parts=[TextPart(content='Paris', part_kind='text')],\\n            model_name='function:model_logic',\\n            timestamp=datetime.datetime(...),\\n            kind='response',\\n          )\\n        ),\\n        End(data=FinalResult(data='Paris', tool_name=None)),\\n      ]\\n      '''\\n      print(agent_run.result.data)\\n      #> Paris\\npython\\n    from pydantic_ai import Agent\\n    agent = Agent('openai:gpt-4o')\\n    result_sync = agent.run_sync('What is the capital of Italy?')\\n    print(result_sync.data)\\n    #> Rome\\npython\\n    from pydantic_ai import Agent\\n    agent = Agent('openai:gpt-4o')\\n    async def main():\\n      async with agent.run_stream('What is the capital of the UK?') as response:\\n        print(await response.get_data())\\n        #> London\\npython\\n    from pydantic_ai\"),\n",
       " Document(metadata={'chunk_id': 52, 'type': 'text', 'num_code_blocks': 6, 'url': 'https://pydantic.com'}, page_content='import Agent, RunContext\\n    agent = Agent(\\'test\\', deps_type=str)\\n    @agent.system_prompt\\n    def simple_system_prompt() -> str:\\n      return \\'foobar\\'\\n    @agent.system_prompt(dynamic=True)\\n    async def async_system_prompt(ctx: RunContext[str]) -> str:\\n      return f\\'{ctx.deps} is the best\\'\\npython\\n    from pydantic_ai import Agent, ModelRetry, RunContext\\n    agent = Agent(\\'test\\', deps_type=str)\\n    @agent.result_validator\\n    def result_validator_simple(data: str) -> str:\\n      if \\'wrong\\' in data:\\n        raise ModelRetry(\\'wrong response\\')\\n      return data\\n    @agent.result_validator\\n    async def result_validator_deps(ctx: RunContext[str], data: str) -> str:\\n      if ctx.deps in data:\\n        raise ModelRetry(\\'wrong response\\')\\n      return data\\n    result = agent.run_sync(\\'foobar\\', deps=\\'spam\\')\\n    print(result.data)\\n    #> success (no tool calls)\\npython\\n    from pydantic_ai import Agent, RunContext\\n    agent = Agent(\\'test\\', deps_type=int)\\n    @agent.tool\\n    def foobar(ctx: RunContext[int], x: int) -> int:\\n      return ctx.deps + x\\n    @agent.tool(retries=2)\\n    async def spam(ctx: RunContext[str], y: float) -> float:\\n      return ctx.deps + y\\n    result = agent.run_sync(\\'foobar\\', deps=1)\\n    print(result.data)\\n    #> {\"foobar\":1,\"spam\":1.0}\\npython\\n    from pydantic_ai import Agent, RunContext\\n    agent = Agent(\\'test\\')\\n    @agent.tool\\n    def foobar(ctx: RunContext[int]) -> int:\\n      return 123\\n    @agent.tool(retries=2)\\n    async def spam(ctx: RunContext[str]) -> float:\\n      return 3.14\\n    result = agent.run_sync(\\'foobar\\', deps=1)\\n    print(result.data)\\n    #> {\"foobar\":123,\"spam\":3.14}\\n\\n\\n---|---  \\n####  model `instance-attribute`\\n\\n\\nThe default model configured for this agent.\\n####  __init__'),\n",
       " Document(metadata={'chunk_id': 53, 'type': 'text', 'num_code_blocks': 0, 'url': 'https://pydantic.com'}, page_content=\"Create an agent.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`model` |  `Model | KnownModelName | None` |  The default model to use for this agent, if not provide, you must provide the model when calling it. |  `None`  \\n`result_type` |  `typeResultDataT[]` |  The type of the result data, used to validate the result data, defaults to `str`. |  `str`  \\n`system_prompt` |  `str | Sequencestr[]` |  Static system prompts to use for this agent, you can also register system prompts via a function with `system_prompt`. |  `()`  \\n`deps_type` |  `typeAgentDepsT[]` |  The type used for dependency injection, this parameter exists solely to allow you to fully parameterize the agent, and therefore get the best out of static type checking. If you're not using deps, but want type checking to pass, you can set `deps=None` to satisfy Pyright or add a type hint `: Agent[None, <return type>]`. |  `NoneType`  \\n`name` |  `str | None` |  The name of the agent, used for logging. If `None`, we try to infer the agent name from the call frame when the agent is first run. |  `None`  \\n`model_settings` |  `ModelSettings | None` |  Optional model request settings to use for this agent's runs, by default. |  `None`  \\n`retries` |  `int` |  The default number of retries to allow before raising an error. |  `1`  \\n`result_tool_name` |  `str` |  The name of the tool to use for the final result. |  `'final_result'`  \\n`result_tool_description` |  `str | None` |  The description of the final result tool. |  `None`  \\n`result_retries` |  `int | None` |  The maximum number of retries to allow for result validation, defaults to `retries`. |  `None`  \\n`tools` |  `SequenceTool[AgentDepsT[] | ToolFuncEitherAgentDepsT[, ...]]` |  Tools to register with the agent, you can also register tools via the decorators `@agent.tool` and `@agent.tool_plain`.\"),\n",
       " Document(metadata={'chunk_id': 54, 'type': 'text', 'num_code_blocks': 10, 'url': 'https://pydantic.com'}, page_content=\"|  `()`  \\n`defer_model_check` |  `bool` |  by default, if you provide a named model, it's evaluated to create a `Model` instance immediately, which checks for the necessary environment variables. Set this to `false` to defer the evaluation until the first run. Useful if you want to override the model for testing. |  `False`  \\n`end_strategy` |  `EndStrategy` |  Strategy for handling tool calls that are requested alongside a final result. See `EndStrategy` for more information. |  `'early'`  \\nSource code in `pydantic_ai_slim/pydantic_ai/agent.py`\\n\\n| \\n\\n---|---  \\n####  end_strategy `instance-attribute`\\n\\n\\nStrategy for handling tool calls when a final result is found.\\n####  name `instance-attribute`\\n\\n\\nThe name of the agent, used for logging.\\nIf `None`, we try to infer the agent name from the call frame when the agent is first run.\\n####  model_settings `instance-attribute`\\n\\n\\nOptional model request settings to use for this agents's runs, by default.\\nNote, if `model_settings` is provided by `run`, `run_sync`, or `run_stream`, those settings will be merged with this value, with the runtime argument taking priority.\\n####  result_type `class-attribute` `instance-attribute`\\n\\n\\nThe type of the result data, used to validate the result data, defaults to `str`.\\n####  run `async`\\n\\n\\n\\n\\n\\n\\nRun the agent with a user prompt in async mode.\\nThis method builds an internal agent graph (using system prompts, tools and result schemas) and then runs the graph to completion. The result of the run is returned.\\nExample: \\n\\n\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`user_prompt` |  `str` |  User input to start/continue the conversation.\"),\n",
       " Document(metadata={'chunk_id': 55, 'type': 'text', 'num_code_blocks': 4, 'url': 'https://pydantic.com'}, page_content=\"|  _required_  \\n`result_type` |  `typeRunResultDataT[] | None` |  Custom result type to use for this run, `result_type` may only be used if the agent has no result validators since result validators would expect an argument that matches the agent's result type. |  `None`  \\n`message_history` |  `listModelMessage[] | None` |  History of the conversation so far. |  `None`  \\n`model` |  `Model | KnownModelName | None` |  Optional model to use for this run, required if `model` was not set when creating the agent. |  `None`  \\n`deps` |  `AgentDepsT` |  Optional dependencies to use for this run. |  `None`  \\n`model_settings` |  `ModelSettings | None` |  Optional settings to use for this model's request. |  `None`  \\n`usage_limits` |  `UsageLimits | None` |  Optional limits on model request count or token usage. |  `None`  \\n`usage` |  `Usage | None` |  Optional usage to start with, useful for resuming a conversation or agents used in tools. |  `None`  \\n`infer_name` |  `bool` |  Whether to try to infer the agent name from the call frame if it's not set. |  `True`  \\nReturns:\\nType | Description  \\n---|---  \\n`AgentRunResultAny[]` |  The result of the run.  \\nSource code in `pydantic_ai_slim/pydantic_ai/agent.py`\\n\\n| python\\n  from pydantic_ai import Agent\\n  agent = Agent('openai:gpt-4o')\\n  async def main():\\n    agent_run = await agent.run('What is the capital of France?')\\n    print(agent_run.data)\\n    #> Paris\\n  \\n\\n---|---  \\n####  iter\\n\\n\\nA contextmanager which can be used to iterate over the agent graph's nodes as they are executed.\\nThis method builds an internal agent graph (using system prompts, tools and result schemas) and then returns an `AgentRun` object. The `AgentRun` can be used to async-iterate over the nodes of the graph as they are executed.\"),\n",
       " Document(metadata={'chunk_id': 56, 'type': 'text', 'num_code_blocks': 1, 'url': 'https://pydantic.com'}, page_content=\"This is the API to use if you want to consume the outputs coming from each LLM model response, or the stream of events coming from the execution of tools.\\nThe `AgentRun` also provides methods to access the full message history, new messages, and usage statistics, and the final result of the run once it has completed.\\nFor more details, see the documentation of `AgentRun`.\\nExample: \\n\\n\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`user_prompt` |  `str` |  User input to start/continue the conversation. |  _required_  \\n`result_type` |  `typeRunResultDataT[] | None` |  Custom result type to use for this run, `result_type` may only be used if the agent has no result validators since result validators would expect an argument that matches the agent's result type. |  `None`  \\n`message_history` |  `listModelMessage[] | None` |  History of the conversation so far. |  `None`  \\n`model` |  `Model | KnownModelName | None` |  Optional model to use for this run, required if `model` was not set when creating the agent. |  `None`  \\n`deps` |  `AgentDepsT` |  Optional dependencies to use for this run. |  `None`  \\n`model_settings` |  `ModelSettings | None` |  Optional settings to use for this model's request. |  `None`  \\n`usage_limits` |  `UsageLimits | None` |  Optional limits on model request count or token usage. |  `None`  \\n`usage` |  `Usage | None` |  Optional usage to start with, useful for resuming a conversation or agents used in tools. |  `None`  \\n`infer_name` |  `bool` |  Whether to try to infer the agent name from the call frame if it's not set. |  `True`  \\nReturns:\\nType | Description  \\n---|---  \\n`IteratorAgentRun[AgentDepsT[, Any]]` |  The result of the run.\"),\n",
       " Document(metadata={'chunk_id': 57, 'type': 'text', 'num_code_blocks': 7, 'url': 'https://pydantic.com'}, page_content=\"Source code in `pydantic_ai_slim/pydantic_ai/agent.py`\\n\\n| python\\n  from pydantic_ai import Agent\\n  agent = Agent('openai:gpt-4o')\\n  async def main():\\n    nodes = []\\n    with agent.iter('What is the capital of France?') as agent_run:\\n      async for node in agent_run:\\n        nodes.append(node)\\n    print(nodes)\\n    '''\\n    [\\n      ModelRequestNode(\\n        request=ModelRequest(\\n          parts=[\\n            UserPromptPart(\\n              content='What is the capital of France?',\\n              timestamp=datetime.datetime(...),\\n              part_kind='user-prompt',\\n            )\\n          ],\\n          kind='request',\\n        )\\n      ),\\n      HandleResponseNode(\\n        model_response=ModelResponse(\\n          parts=[TextPart(content='Paris', part_kind='text')],\\n          model_name='function:model_logic',\\n          timestamp=datetime.datetime(...),\\n          kind='response',\\n        )\\n      ),\\n      End(data=FinalResult(data='Paris', tool_name=None)),\\n    ]\\n    '''\\n    print(agent_run.result.data)\\n    #> Paris\\n  \\n\\n---|---  \\n####  run_sync\\n\\n\\n\\n\\n\\n\\nSynchronously run the agent with a user prompt.\\nThis is a convenience method that wraps `self.run` with `loop.run_until_complete(...)`. You therefore can't use this method inside async code or if there's an active event loop.\\nExample: \\n\\n\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`user_prompt` |  `str` |  User input to start/continue the conversation. |  _required_  \\n`result_type` |  `typeRunResultDataT[] | None` |  Custom result type to use for this run, `result_type` may only be used if the agent has no result validators since result validators would expect an argument that matches the agent's result type. |  `None`  \\n`message_history` |  `listModelMessage[] | None` |  History of the conversation so far.\"),\n",
       " Document(metadata={'chunk_id': 58, 'type': 'text', 'num_code_blocks': 7, 'url': 'https://pydantic.com'}, page_content=\"|  `None`  \\n`model` |  `Model | KnownModelName | None` |  Optional model to use for this run, required if `model` was not set when creating the agent. |  `None`  \\n`deps` |  `AgentDepsT` |  Optional dependencies to use for this run. |  `None`  \\n`model_settings` |  `ModelSettings | None` |  Optional settings to use for this model's request. |  `None`  \\n`usage_limits` |  `UsageLimits | None` |  Optional limits on model request count or token usage. |  `None`  \\n`usage` |  `Usage | None` |  Optional usage to start with, useful for resuming a conversation or agents used in tools. |  `None`  \\n`infer_name` |  `bool` |  Whether to try to infer the agent name from the call frame if it's not set. |  `True`  \\nReturns:\\nType | Description  \\n---|---  \\n`AgentRunResultAny[]` |  The result of the run.  \\nSource code in `pydantic_ai_slim/pydantic_ai/agent.py`\\n\\n| python\\n  from pydantic_ai import Agent\\n  agent = Agent('openai:gpt-4o')\\n  result_sync = agent.run_sync('What is the capital of Italy?')\\n  print(result_sync.data)\\n  #> Rome\\n  \\n\\n---|---  \\n####  run_stream `async`\\n\\n\\n\\n\\n\\n\\nRun the agent with a user prompt in async mode, returning a streamed response.\\nExample: \\n\\n\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`user_prompt` |  `str` |  User input to start/continue the conversation. |  _required_  \\n`result_type` |  `typeRunResultDataT[] | None` |  Custom result type to use for this run, `result_type` may only be used if the agent has no result validators since result validators would expect an argument that matches the agent's result type. |  `None`  \\n`message_history` |  `listModelMessage[] | None` |  History of the conversation so far. |  `None`  \\n`model` |  `Model | KnownModelName | None` |  Optional model to use for this run, required if `model` was not set when creating the agent. |  `None`  \\n`deps` |  `AgentDepsT` |  Optional dependencies to use for this run.\"),\n",
       " Document(metadata={'chunk_id': 59, 'type': 'text', 'num_code_blocks': 12, 'url': 'https://pydantic.com'}, page_content=\"|  `None`  \\n`model_settings` |  `ModelSettings | None` |  Optional settings to use for this model's request. |  `None`  \\n`usage_limits` |  `UsageLimits | None` |  Optional limits on model request count or token usage. |  `None`  \\n`usage` |  `Usage | None` |  Optional usage to start with, useful for resuming a conversation or agents used in tools. |  `None`  \\n`infer_name` |  `bool` |  Whether to try to infer the agent name from the call frame if it's not set. |  `True`  \\nReturns:\\nType | Description  \\n---|---  \\n`AsyncIteratorStreamedRunResult[AgentDepsT[, Any]]` |  The result of the run.  \\nSource code in `pydantic_ai_slim/pydantic_ai/agent.py`\\n\\n| python\\n  from pydantic_ai import Agent\\n  agent = Agent('openai:gpt-4o')\\n  async def main():\\n    async with agent.run_stream('What is the capital of the UK?') as response:\\n      print(await response.get_data())\\n      #> London\\n  \\n\\n---|---  \\n####  override\\n\\n\\nContext manager to temporarily override agent dependencies and model.\\nThis is particularly useful when testing. You can find an example of this here.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`deps` |  `AgentDepsT | Unset` |  The dependencies to use instead of the dependencies passed to the agent run. |  `UNSET`  \\n`model` |  `Model | KnownModelName | Unset` |  The model to use instead of the model passed to the agent run. |  `UNSET`  \\nSource code in `pydantic_ai_slim/pydantic_ai/agent.py`\\n\\n| \\n\\n---|---  \\n####  system_prompt\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDecorator to register a system prompt function.\\nOptionally takes `RunContext` as its only argument.\"),\n",
       " Document(metadata={'chunk_id': 60, 'type': 'text', 'num_code_blocks': 10, 'url': 'https://pydantic.com'}, page_content=\"Can decorate a sync or async functions.\\nThe decorator can be used either bare (`agent.system_prompt`) or as a function call (`agent.system_prompt(...)`), see the examples below.\\nOverloads for every possible signature of `system_prompt` are included so the decorator doesn't obscure the type of the function, see `tests/typed_agent.py` for tests.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`func` |  `SystemPromptFuncAgentDepsT[] | None` |  The function to decorate |  `None`  \\n`dynamic` |  `bool` |  If True, the system prompt will be reevaluated even when `messages_history` is provided, see `SystemPromptPart.dynamic_ref` |  `False`  \\nExample: \\n\\n\\nSource code in `pydantic_ai_slim/pydantic_ai/agent.py`\\n\\n| python\\n  from pydantic_ai import Agent, RunContext\\n  agent = Agent('test', deps_type=str)\\n  @agent.system_prompt\\n  def simple_system_prompt() -> str:\\n    return 'foobar'\\n  @agent.system_prompt(dynamic=True)\\n  async def async_system_prompt(ctx: RunContext[str]) -> str:\\n    return f'{ctx.deps} is the best'\\n  \\n\\n---|---  \\n####  result_validator\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDecorator to register a result validator function.\\nOptionally takes `RunContext` as its first argument. Can decorate a sync or async functions.\\nOverloads for every possible signature of `result_validator` are included so the decorator doesn't obscure the type of the function, see `tests/typed_agent.py` for tests.\\nExample:\"),\n",
       " Document(metadata={'chunk_id': 61, 'type': 'text', 'num_code_blocks': 7, 'url': 'https://pydantic.com'}, page_content=\"Source code in `pydantic_ai_slim/pydantic_ai/agent.py`\\n\\n| python\\n  from pydantic_ai import Agent, ModelRetry, RunContext\\n  agent = Agent('test', deps_type=str)\\n  @agent.result_validator\\n  def result_validator_simple(data: str) -> str:\\n    if 'wrong' in data:\\n      raise ModelRetry('wrong response')\\n    return data\\n  @agent.result_validator\\n  async def result_validator_deps(ctx: RunContext[str], data: str) -> str:\\n    if ctx.deps in data:\\n      raise ModelRetry('wrong response')\\n    return data\\n  result = agent.run_sync('foobar', deps='spam')\\n  print(result.data)\\n  #> success (no tool calls)\\n  \\n\\n---|---  \\n####  tool\\n\\n\\n\\n\\n\\n\\nDecorator to register a tool function which takes `RunContext` as its first argument.\\nCan decorate a sync or async functions.\\nThe docstring is inspected to extract both the tool description and description of each parameter, learn more.\\nWe can't add overloads for every possible signature of tool, since the return type is a recursive union so the signature of functions decorated with `@agent.tool` is obscured.\\nExample: \\n\\n\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`func` |  `ToolFuncContextAgentDepsT[, ToolParams] | None` |  The tool function to register. |  `None`  \\n`retries` |  `int | None` |  The number of retries to allow for this tool, defaults to the agent's default retries, which defaults to 1. |  `None`  \\n`prepare` |  `ToolPrepareFuncAgentDepsT[] | None` |  custom method to prepare the tool definition for each step, return `None` to omit this tool from a given step. This is useful if you want to customise a tool at call time, or omit it completely from a step. See `ToolPrepareFunc`. |  `None`  \\n`docstring_format` |  `DocstringFormat` |  The format of the docstring, see `DocstringFormat`. Defaults to `'auto'`, such that the format is inferred from the structure of the docstring.\"),\n",
       " Document(metadata={'chunk_id': 62, 'type': 'text', 'num_code_blocks': 7, 'url': 'https://pydantic.com'}, page_content='|  `\\'auto\\'`  \\n`require_parameter_descriptions` |  `bool` |  If True, raise an error if a parameter description is missing. Defaults to False. |  `False`  \\nSource code in `pydantic_ai_slim/pydantic_ai/agent.py`\\n\\n| python\\n  from pydantic_ai import Agent, RunContext\\n  agent = Agent(\\'test\\', deps_type=int)\\n  @agent.tool\\n  def foobar(ctx: RunContext[int], x: int) -> int:\\n    return ctx.deps + x\\n  @agent.tool(retries=2)\\n  async def spam(ctx: RunContext[str], y: float) -> float:\\n    return ctx.deps + y\\n  result = agent.run_sync(\\'foobar\\', deps=1)\\n  print(result.data)\\n  #> {\"foobar\":1,\"spam\":1.0}\\n  \\n\\n---|---  \\n####  tool_plain\\n\\n\\n\\n\\n\\n\\nDecorator to register a tool function which DOES NOT take `RunContext` as an argument.\\nCan decorate a sync or async functions.\\nThe docstring is inspected to extract both the tool description and description of each parameter, learn more.\\nWe can\\'t add overloads for every possible signature of tool, since the return type is a recursive union so the signature of functions decorated with `@agent.tool` is obscured.\\nExample: \\n\\n\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`func` |  `ToolFuncPlainToolParams[] | None` |  The tool function to register. |  `None`  \\n`retries` |  `int | None` |  The number of retries to allow for this tool, defaults to the agent\\'s default retries, which defaults to 1. |  `None`  \\n`prepare` |  `ToolPrepareFuncAgentDepsT[] | None` |  custom method to prepare the tool definition for each step, return `None` to omit this tool from a given step. This is useful if you want to customise a tool at call time, or omit it completely from a step. See `ToolPrepareFunc`. |  `None`  \\n`docstring_format` |  `DocstringFormat` |  The format of the docstring, see `DocstringFormat`. Defaults to `\\'auto\\'`, such that the format is inferred from the structure of the docstring.'),\n",
       " Document(metadata={'chunk_id': 63, 'type': 'text', 'num_code_blocks': 6, 'url': 'https://pydantic.com'}, page_content='|  `\\'auto\\'`  \\n`require_parameter_descriptions` |  `bool` |  If True, raise an error if a parameter description is missing. Defaults to False. |  `False`  \\nSource code in `pydantic_ai_slim/pydantic_ai/agent.py`\\n\\n| python\\n  from pydantic_ai import Agent, RunContext\\n  agent = Agent(\\'test\\')\\n  @agent.tool\\n  def foobar(ctx: RunContext[int]) -> int:\\n    return 123\\n  @agent.tool(retries=2)\\n  async def spam(ctx: RunContext[str]) -> float:\\n    return 3.14\\n  result = agent.run_sync(\\'foobar\\', deps=1)\\n  print(result.data)\\n  #> {\"foobar\":123,\"spam\":3.14}\\n  \\n\\n---|---  \\n###  AgentRun `dataclass`\\nBases: `GenericAgentDepsT[, ResultDataT]`\\nA stateful, async-iterable run of an `Agent`.\\nYou generally obtain an `AgentRun` instance by calling `with my_agent.iter(...) as agent_run:`.\\nOnce you have an instance, you can use it to iterate through the run\\'s nodes as they execute. When an `End` is reached, the run finishes and `result` becomes available.\\nExample: \\n\\n\\nYou can also manually drive the iteration using the `next` method for more granular control.\\nSource code in `pydantic_ai_slim/pydantic_ai/agent.py`\\n\\n| python\\n  from pydantic_ai import Agent\\n  agent = Agent(\\'openai:gpt-4o\\')\\n  async def main():\\n    nodes = []\\n    # Iterate through the run, recording each node along the way:\\n    with agent.iter(\\'What is the capital of France?\\') as agent_run:\\n      async for node in agent_run:\\n        nodes.append(node)\\n    print(nodes)\\n    \\'\\'\\'\\n    [\\n      ModelRequestNode(\\n        request=ModelRequest(\\n          parts=[\\n            UserPromptPart(\\n              content=\\'What is the capital of France?\\',\\n              timestamp=datetime.datetime(...),\\n              part_kind=\\'user-prompt\\',\\n            )\\n          ],\\n          kind=\\'request\\',\\n        )\\n      ),\\n      HandleResponseNode(\\n        model_response=ModelResponse(\\n          parts=[TextPart(content=\\'Paris\\', part_kind=\\'text\\')],\\n          model_name=\\'function:model_logic\\','),\n",
       " Document(metadata={'chunk_id': 64, 'type': 'text', 'num_code_blocks': 4, 'url': 'https://pydantic.com'}, page_content=\"timestamp=datetime.datetime(...),\\n          kind='response',\\n        )\\n      ),\\n      End(data=FinalResult(data='Paris', tool_name=None)),\\n    ]\\n    '''\\n    print(agent_run.result.data)\\n    #> Paris\\n  python\\n    from pydantic_ai import Agent\\n    from pydantic_graph import End\\n    agent = Agent('openai:gpt-4o')\\n    async def main():\\n      with agent.iter('What is the capital of France?') as agent_run:\\n        next_node = agent_run.next_node # start with the first node\\n        nodes = [next_node]\\n        while not isinstance(next_node, End):\\n          next_node = await agent_run.next(next_node)\\n          nodes.append(next_node)\\n        # Once `next_node` is an End, we've finished:\\n        print(nodes)\\n        '''\\n        [\\n          UserPromptNode(\\n            user_prompt='What is the capital of France?',\\n            system_prompts=(),\\n            system_prompt_functions=[],\\n            system_prompt_dynamic_functions={},\\n          ),\\n          ModelRequestNode(\\n            request=ModelRequest(\\n              parts=[\\n                UserPromptPart(\\n                  content='What is the capital of France?',\\n                  timestamp=datetime.datetime(...),\\n                  part_kind='user-prompt',\\n                )\\n              ],\\n              kind='request',\\n            )\\n          ),\\n          HandleResponseNode(\\n            model_response=ModelResponse(\\n              parts=[TextPart(content='Paris', part_kind='text')],\\n              model_name='function:model_logic',\\n              timestamp=datetime.datetime(...),\\n              kind='response',\\n            )\\n          ),\\n          End(data=FinalResult(data='Paris', tool_name=None)),\\n        ]\\n        '''\\n        print('Final result:', agent_run.result.data)\\n        #> Final result: Paris\\n\\n\\n---|---  \\n####  ctx `property`\\n\\n\\nThe current context of the agent run.\\n####  next_node `property`\"),\n",
       " Document(metadata={'chunk_id': 65, 'type': 'text', 'num_code_blocks': 9, 'url': 'https://pydantic.com'}, page_content='The next node that will be run in the agent graph.\\nThis is the next node that will be used during async iteration, or if a node is not passed to `self.next(...)`.\\n####  result `property`\\n\\n\\nThe final result of the run if it has ended, otherwise `None`.\\nOnce the run returns an `End` node, `result` is populated with an `AgentRunResult`.\\n####  __aiter__\\n\\n\\nProvide async-iteration over the nodes in the agent run.\\nSource code in `pydantic_ai_slim/pydantic_ai/agent.py`\\n\\n| \\n\\n---|---  \\n####  __anext__ `async`\\n\\n\\nAdvance to the next node automatically based on the last returned node.\\nSource code in `pydantic_ai_slim/pydantic_ai/agent.py`\\n\\n| \\n\\n---|---  \\n####  next `async`\\n\\n\\nManually drive the agent run by passing in the node you want to run next.\\nThis lets you inspect or mutate the node before continuing execution, or skip certain nodes under dynamic conditions. The agent run should be stopped when you return an `End` node.\\nExample: \\n\\n\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`node` |  `BaseNodeGraphAgentState, GraphAgentDeps[AgentDepsT[, Any], FinalResultResultDataT[]]` |  The node to run next in the graph. |  _required_  \\nReturns:\\nType | Description  \\n---|---  \\n`BaseNodeGraphAgentState, GraphAgentDeps[AgentDepsT[, Any], FinalResultResultDataT[]] | EndFinalResult[ResultDataT[]]` |  The next node returned by the graph logic, or an `End` node if  \\n`BaseNodeGraphAgentState, GraphAgentDeps[AgentDepsT[, Any], FinalResultResultDataT[]] | EndFinalResult[ResultDataT[]]` |  the run has completed.'),\n",
       " Document(metadata={'chunk_id': 66, 'type': 'text', 'num_code_blocks': 8, 'url': 'https://pydantic.com'}, page_content=\"Source code in `pydantic_ai_slim/pydantic_ai/agent.py`\\n\\n| python\\n  from pydantic_ai import Agent\\n  from pydantic_graph import End\\n  agent = Agent('openai:gpt-4o')\\n  async def main():\\n    with agent.iter('What is the capital of France?') as agent_run:\\n      next_node = agent_run.next_node # start with the first node\\n      nodes = [next_node]\\n      while not isinstance(next_node, End):\\n        next_node = await agent_run.next(next_node)\\n        nodes.append(next_node)\\n      # Once `next_node` is an End, we've finished:\\n      print(nodes)\\n      '''\\n      [\\n        UserPromptNode(\\n          user_prompt='What is the capital of France?',\\n          system_prompts=(),\\n          system_prompt_functions=[],\\n          system_prompt_dynamic_functions={},\\n        ),\\n        ModelRequestNode(\\n          request=ModelRequest(\\n            parts=[\\n              UserPromptPart(\\n                content='What is the capital of France?',\\n                timestamp=datetime.datetime(...),\\n                part_kind='user-prompt',\\n              )\\n            ],\\n            kind='request',\\n          )\\n        ),\\n        HandleResponseNode(\\n          model_response=ModelResponse(\\n            parts=[TextPart(content='Paris', part_kind='text')],\\n            model_name='function:model_logic',\\n            timestamp=datetime.datetime(...),\\n            kind='response',\\n          )\\n        ),\\n        End(data=FinalResult(data='Paris', tool_name=None)),\\n      ]\\n      '''\\n      print('Final result:', agent_run.result.data)\\n      #> Final result: Paris\\n  \\n\\n---|---  \\n####  usage\\n\\n\\nGet usage statistics for the run so far, including token usage, model requests, and so on.\\nSource code in `pydantic_ai_slim/pydantic_ai/agent.py`\\n\\n| \\n\\n---|---  \\n###  AgentRunResult `dataclass`\\nBases: `GenericResultDataT[]`\\nThe final result of an agent run.\\nSource code in `pydantic_ai_slim/pydantic_ai/agent.py`\\n\\n|\"),\n",
       " Document(metadata={'chunk_id': 67, 'type': 'text', 'num_code_blocks': 7, 'url': 'https://pydantic.com'}, page_content='---|---  \\n####  all_messages\\n\\n\\nReturn the history of _messages.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`result_tool_return_content` |  `str | None` |  The return content of the tool call to set in the last message. This provides a convenient way to modify the content of the result tool call if you want to continue the conversation and want to set the response to the result tool call. If `None`, the last message will not be modified. |  `None`  \\nReturns:\\nType | Description  \\n---|---  \\n`listModelMessage[]` |  List of messages.  \\nSource code in `pydantic_ai_slim/pydantic_ai/agent.py`\\n\\n| \\n\\n---|---  \\n####  all_messages_json\\n\\n\\nReturn all messages from `all_messages` as JSON bytes.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`result_tool_return_content` |  `str | None` |  The return content of the tool call to set in the last message. This provides a convenient way to modify the content of the result tool call if you want to continue the conversation and want to set the response to the result tool call. If `None`, the last message will not be modified. |  `None`  \\nReturns:\\nType | Description  \\n---|---  \\n`bytes` |  JSON bytes representing the messages.  \\nSource code in `pydantic_ai_slim/pydantic_ai/agent.py`\\n\\n| \\n\\n---|---  \\n####  new_messages\\n\\n\\nReturn new messages associated with this run.\\nMessages from older runs are excluded.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`result_tool_return_content` |  `str | None` |  The return content of the tool call to set in the last message. This provides a convenient way to modify the content of the result tool call if you want to continue the conversation and want to set the response to the result tool call. If `None`, the last message will not be modified. |  `None`  \\nReturns:\\nType | Description  \\n---|---  \\n`listModelMessage[]` |  List of new messages.'),\n",
       " Document(metadata={'chunk_id': 68, 'type': 'text', 'num_code_blocks': 11, 'url': 'https://pydantic.com'}, page_content='Source code in `pydantic_ai_slim/pydantic_ai/agent.py`\\n\\n| \\n\\n---|---  \\n####  new_messages_json\\n\\n\\nReturn new messages from `new_messages` as JSON bytes.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`result_tool_return_content` |  `str | None` |  The return content of the tool call to set in the last message. This provides a convenient way to modify the content of the result tool call if you want to continue the conversation and want to set the response to the result tool call. If `None`, the last message will not be modified. |  `None`  \\nReturns:\\nType | Description  \\n---|---  \\n`bytes` |  JSON bytes representing the new messages.  \\nSource code in `pydantic_ai_slim/pydantic_ai/agent.py`\\n\\n| \\n\\n---|---  \\n####  usage\\n\\n\\nReturn the usage of the whole run.\\nSource code in `pydantic_ai_slim/pydantic_ai/agent.py`\\n\\n| \\n\\n---|---  \\n###  EndStrategy `module-attribute`\\n\\n\\n###  RunResultDataT `module-attribute`\\n\\n\\nType variable for the result data of a run where `result_type` was customized on the run call.\\n###  capture_run_messages `module-attribute`\\n\\n\\n© Pydantic Services Inc. 2024 to present\\n\\n---'),\n",
       " Document(metadata={'chunk_id': 69, 'type': 'text', 'num_code_blocks': 15, 'url': 'https://pydantic.com'}, page_content=\"exceptions  \\n  *  ModelRetry  \\n    *  message  \\n  *  UserError  \\n    *  message  \\n  *  AgentRunError  \\n    *  message  \\n  *  UsageLimitExceeded  \\n  *  UnexpectedModelBehavior  \\n    *  message  \\n    *  body  \\n\\n  1.  Introduction  \\n  2.  API Reference  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# `pydantic_ai.exceptions`\\n###  ModelRetry\\nBases: `Exception`\\nException raised when a tool function should be retried.\\nThe agent will return the message to the model and ask it to try calling the function/tool again.\\nSource code in `pydantic_ai_slim/pydantic_ai/exceptions.py`\\n\\n| \\n\\n---|---  \\n####  message `instance-attribute`\\n\\n\\nThe message to return to the model.\\n###  UserError\\nBases: `RuntimeError`\\nError caused by a usage mistake by the application developer — You!\\nSource code in `pydantic_ai_slim/pydantic_ai/exceptions.py`\\n\\n| \\n\\n---|---  \\n####  message `instance-attribute`\\n\\n\\nDescription of the mistake.\\n###  AgentRunError\\nBases: `RuntimeError`\\nBase class for errors occurring during an agent run.\\nSource code in `pydantic_ai_slim/pydantic_ai/exceptions.py`\\n\\n| \\n\\n---|---  \\n####  message `instance-attribute`\\n\\n\\nThe error message.\\n###  UsageLimitExceeded\\nBases: `AgentRunError`\\nError raised when a Model's usage exceeds the specified limits.\\nSource code in `pydantic_ai_slim/pydantic_ai/exceptions.py`\\n\\n| \\n\\n---|---  \\n###  UnexpectedModelBehavior\\nBases: `AgentRunError`\\nError caused by unexpected Model behavior, e.g. an unexpected response code.\\nSource code in `pydantic_ai_slim/pydantic_ai/exceptions.py`\\n\\n| \\n\\n---|---  \\n####  message `instance-attribute`\\n\\n\\nDescription of the unexpected behavior.\\n####  body `instance-attribute`\"),\n",
       " Document(metadata={'chunk_id': 70, 'type': 'text', 'num_code_blocks': 0, 'url': 'https://pydantic.com'}, page_content='The body of the response, if available.\\n© Pydantic Services Inc. 2024 to present\\n\\n---'),\n",
       " Document(metadata={'chunk_id': 71, 'type': 'text', 'num_code_blocks': 5, 'url': 'https://pydantic.com'}, page_content='format_as_xml  \\n  *  format_as_xml  \\n\\n  1.  Introduction  \\n  2.  API Reference  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# `pydantic_ai.format_as_xml`\\n###  format_as_xml\\n\\n\\nFormat a Python object as XML.\\nThis is useful since LLMs often find it easier to read semi-structured data (e.g. examples) as XML, rather than JSON etc.\\nSupports: `str`, `bytes`, `bytearray`, `bool`, `int`, `float`, `date`, `datetime`, `Mapping`, `Iterable`, `dataclass`, and `BaseModel`.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`obj` |  `Any` |  Python Object to serialize to XML. |  _required_  \\n`root_tag` |  `str` |  Outer tag to wrap the XML in, use `None` to omit the outer tag. |  `\\'examples\\'`  \\n`item_tag` |  `str` |  Tag to use for each item in an iterable (e.g. list), this is overridden by the class name for dataclasses and Pydantic models. |  `\\'example\\'`  \\n`include_root_tag` |  `bool` |  Whether to include the root tag in the output (The root tag is always included if it includes a body - e.g. when the input is a simple value). |  `True`  \\n`none_str` |  `str` |  String to use for `None` values. |  `\\'null\\'`  \\n`indent` |  `str | None` |  Indentation string to use for pretty printing. |  `\\' \\'`  \\nReturns:\\nType | Description  \\n---|---  \\n`str` |  XML representation of the object.  \\nExample: \\nformat_as_xml_example.py\\n\\nSource code in `pydantic_ai_slim/pydantic_ai/format_as_xml.py`\\n\\n| python {title=\"format_as_xml_example.py\" lint=\"skip\"}\\n  from pydantic_ai.format_as_xml import format_as_xml\\n  print(format_as_xml({\\'name\\': \\'John\\', \\'height\\': 6, \\'weight\\': 200}, root_tag=\\'user\\'))\\n  \\'\\'\\'\\n  <user>\\n   <name>John</name>\\n   <height>6</height>\\n   <weight>200</weight>\\n  </user>\\n  \\'\\'\\'\\n  \\n\\n---|---  \\n© Pydantic Services Inc. 2024 to present\\n\\n---'),\n",
       " Document(metadata={'chunk_id': 72, 'type': 'text', 'num_code_blocks': 1, 'url': 'https://pydantic.com'}, page_content='messages  \\n  *  SystemPromptPart  \\n    *  content  \\n    *  dynamic_ref  \\n    *  part_kind  \\n  *  UserPromptPart  \\n    *  content  \\n    *  timestamp  \\n    *  part_kind  \\n  *  ToolReturnPart  \\n    *  tool_name  \\n    *  content  \\n    *  tool_call_id  \\n    *  timestamp  \\n    *  part_kind  \\n    *  model_response_str  \\n    *  model_response_object  \\n  *  RetryPromptPart  \\n    *  content  \\n    *  tool_name  \\n    *  tool_call_id  \\n    *  timestamp  \\n    *  part_kind  \\n    *  model_response  \\n  *  ModelRequestPart  \\n  *  ModelRequest  \\n    *  parts  \\n    *  kind  \\n  *  TextPart  \\n    *  content  \\n    *  part_kind  \\n    *  has_content  \\n  *  ToolCallPart  \\n    *  tool_name  \\n    *  args  \\n    *  tool_call_id  \\n    *  part_kind  \\n    *  args_as_dict  \\n    *  args_as_json_str  \\n    *  has_content  \\n  *  ModelResponsePart  \\n  *  ModelResponse  \\n    *  parts  \\n    *  model_name  \\n    *  timestamp  \\n    *  kind  \\n  *  ModelMessage  \\n  *  ModelMessagesTypeAdapter  \\n  *  TextPartDelta  \\n    *  content_delta  \\n    *  part_delta_kind  \\n    *  apply  \\n  *  ToolCallPartDelta  \\n    *  tool_name_delta  \\n    *  args_delta  \\n    *  tool_call_id  \\n    *  part_delta_kind  \\n    *  as_part  \\n    *  apply  \\n  *  ModelResponsePartDelta  \\n  *  PartStartEvent  \\n    *  index  \\n    *  part  \\n    *  event_kind  \\n  *  PartDeltaEvent  \\n    *  index  \\n    *  delta  \\n    *  event_kind  \\n  *  ModelResponseStreamEvent  \\n  *  FunctionToolCallEvent  \\n    *  part  \\n    *  call_id  \\n    *  event_kind  \\n  *  FunctionToolResultEvent  \\n    *  result  \\n    *  call_id  \\n    *  event_kind  \\n\\n  1.  Introduction  \\n  2.  API Reference  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# `pydantic_ai.messages`\\nThe structure of `ModelMessage` can be shown as a graph:'),\n",
       " Document(metadata={'chunk_id': 73, 'type': 'text', 'num_code_blocks': 17, 'url': 'https://pydantic.com'}, page_content='###  SystemPromptPart `dataclass`\\nA system prompt, generally written by the application developer.\\nThis gives the model context and guidance on how to respond.\\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\\n\\n| \\n\\n---|---  \\n####  content `instance-attribute`\\n\\n\\nThe content of the prompt.\\n####  dynamic_ref `class-attribute` `instance-attribute`\\n\\n\\nThe ref of the dynamic system prompt function that generated this part.\\nOnly set if system prompt is dynamic, see `system_prompt` for more information.\\n####  part_kind `class-attribute` `instance-attribute`\\n\\n\\nPart type identifier, this is available on all parts as a discriminator.\\n###  UserPromptPart `dataclass`\\nA user prompt, generally written by the end user.\\nContent comes from the `user_prompt` parameter of `Agent.run`, `Agent.run_sync`, and `Agent.run_stream`.\\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\\n\\n| \\n\\n---|---  \\n####  content `instance-attribute`\\n\\n\\nThe content of the prompt.\\n####  timestamp `class-attribute` `instance-attribute`\\n\\n\\nThe timestamp of the prompt.\\n####  part_kind `class-attribute` `instance-attribute`\\n\\n\\nPart type identifier, this is available on all parts as a discriminator.\\n###  ToolReturnPart `dataclass`\\nA tool return message, this encodes the result of running a tool.\\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\\n\\n| \\n\\n---|---  \\n####  tool_name `instance-attribute`\\n\\n\\nThe name of the \"tool\" was called.\\n####  content `instance-attribute`\\n\\n\\nThe return value.\\n####  tool_call_id `class-attribute` `instance-attribute`\\n\\n\\nOptional tool call identifier, this is used by some models including OpenAI.\\n####  timestamp `class-attribute` `instance-attribute`\\n\\n\\nThe timestamp, when the tool returned.\\n####  part_kind `class-attribute` `instance-attribute`'),\n",
       " Document(metadata={'chunk_id': 74, 'type': 'text', 'num_code_blocks': 14, 'url': 'https://pydantic.com'}, page_content='Part type identifier, this is available on all parts as a discriminator.\\n####  model_response_str\\n\\n\\nReturn a string representation of the content for the model.\\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\\n\\n| \\n\\n---|---  \\n####  model_response_object\\n\\n\\nReturn a dictionary representation of the content, wrapping non-dict types appropriately.\\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\\n\\n| \\n\\n---|---  \\n###  RetryPromptPart `dataclass`\\nA message back to a model asking it to try again.\\nThis can be sent for a number of reasons:\\n  * Pydantic validation of tool arguments failed, here content is derived from a Pydantic `ValidationError`\\n  * a tool raised a `ModelRetry` exception\\n  * no tool was found for the tool name\\n  * the model returned plain text when a structured response was expected\\n  * Pydantic validation of a structured response failed, here content is derived from a Pydantic `ValidationError`\\n  * a result validator raised a `ModelRetry` exception\\n\\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\\n\\n| \\n\\n---|---  \\n####  content `instance-attribute`\\n\\n\\nDetails of why and how the model should retry.\\nIf the retry was triggered by a `ValidationError`, this will be a list of error details.\\n####  tool_name `class-attribute` `instance-attribute`\\n\\n\\nThe name of the tool that was called, if any.\\n####  tool_call_id `class-attribute` `instance-attribute`\\n\\n\\nOptional tool call identifier, this is used by some models including OpenAI.\\n####  timestamp `class-attribute` `instance-attribute`\\n\\n\\nThe timestamp, when the retry was triggered.\\n####  part_kind `class-attribute` `instance-attribute`\\n\\n\\nPart type identifier, this is available on all parts as a discriminator.\\n####  model_response'),\n",
       " Document(metadata={'chunk_id': 75, 'type': 'text', 'num_code_blocks': 20, 'url': 'https://pydantic.com'}, page_content='Return a string message describing why the retry is requested.\\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\\n\\n| \\n\\n---|---  \\n###  ModelRequestPart `module-attribute`\\n\\n\\nA message part sent by PydanticAI to a model.\\n###  ModelRequest `dataclass`\\nA request generated by PydanticAI and sent to a model, e.g. a message from the PydanticAI app to the model.\\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\\n\\n| \\n\\n---|---  \\n####  parts `instance-attribute`\\n\\n\\nThe parts of the user message.\\n####  kind `class-attribute` `instance-attribute`\\n\\n\\nMessage type identifier, this is available on all parts as a discriminator.\\n###  TextPart `dataclass`\\nA plain text response from a model.\\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\\n\\n| \\n\\n---|---  \\n####  content `instance-attribute`\\n\\n\\nThe text content of the response.\\n####  part_kind `class-attribute` `instance-attribute`\\n\\n\\nPart type identifier, this is available on all parts as a discriminator.\\n####  has_content\\n\\n\\nReturn `True` if the text content is non-empty.\\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\\n\\n| \\n\\n---|---  \\n###  ToolCallPart `dataclass`\\nA tool call from a model.\\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\\n\\n| \\n\\n---|---  \\n####  tool_name `instance-attribute`\\n\\n\\nThe name of the tool to call.\\n####  args `instance-attribute`\\n\\n\\nThe arguments to pass to the tool.\\nThis is stored either as a JSON string or a Python dictionary depending on how data was received.\\n####  tool_call_id `class-attribute` `instance-attribute`\\n\\n\\nOptional tool call identifier, this is used by some models including OpenAI.\\n####  part_kind `class-attribute` `instance-attribute`'),\n",
       " Document(metadata={'chunk_id': 76, 'type': 'text', 'num_code_blocks': 20, 'url': 'https://pydantic.com'}, page_content='Part type identifier, this is available on all parts as a discriminator.\\n####  args_as_dict\\n\\n\\nReturn the arguments as a Python dictionary.\\nThis is just for convenience with models that require dicts as input.\\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\\n\\n| \\n\\n---|---  \\n####  args_as_json_str\\n\\n\\nReturn the arguments as a JSON string.\\nThis is just for convenience with models that require JSON strings as input.\\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\\n\\n| \\n\\n---|---  \\n####  has_content\\n\\n\\nReturn `True` if the arguments contain any data.\\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\\n\\n| \\n\\n---|---  \\n###  ModelResponsePart `module-attribute`\\n\\n\\nA message part returned by a model.\\n###  ModelResponse `dataclass`\\nA response from a model, e.g. a message from the model to the PydanticAI app.\\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\\n\\n| \\n\\n---|---  \\n####  parts `instance-attribute`\\n\\n\\nThe parts of the model message.\\n####  model_name `class-attribute` `instance-attribute`\\n\\n\\nThe name of the model that generated the response.\\n####  timestamp `class-attribute` `instance-attribute`\\n\\n\\nThe timestamp of the response.\\nIf the model provides a timestamp in the response (as OpenAI does) that will be used.\\n####  kind `class-attribute` `instance-attribute`\\n\\n\\nMessage type identifier, this is available on all parts as a discriminator.\\n###  ModelMessage `module-attribute`\\n\\n\\nAny message sent to or returned by a model.\\n###  ModelMessagesTypeAdapter `module-attribute`\\n\\n\\nPydantic `TypeAdapter` for (de)serializing messages.\\n###  TextPartDelta `dataclass`\\nA partial update (delta) for a `TextPart` to append new text content.\\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\\n\\n|'),\n",
       " Document(metadata={'chunk_id': 77, 'type': 'text', 'num_code_blocks': 12, 'url': 'https://pydantic.com'}, page_content='---|---  \\n####  content_delta `instance-attribute`\\n\\n\\nThe incremental text content to add to the existing `TextPart` content.\\n####  part_delta_kind `class-attribute` `instance-attribute`\\n\\n\\nPart delta type identifier, used as a discriminator.\\n####  apply\\n\\n\\nApply this text delta to an existing `TextPart`.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`part` |  `ModelResponsePart` |  The existing model response part, which must be a `TextPart`. |  _required_  \\nReturns:\\nType | Description  \\n---|---  \\n`TextPart` |  A new `TextPart` with updated text content.  \\nRaises:\\nType | Description  \\n---|---  \\n`ValueError` |  If `part` is not a `TextPart`.  \\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\\n\\n| \\n\\n---|---  \\n###  ToolCallPartDelta `dataclass`\\nA partial update (delta) for a `ToolCallPart` to modify tool name, arguments, or tool call ID.\\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\\n\\n| \\n\\n---|---  \\n####  tool_name_delta `class-attribute` `instance-attribute`\\n\\n\\nIncremental text to add to the existing tool name, if any.\\n####  args_delta `class-attribute` `instance-attribute`\\n\\n\\nIncremental data to add to the tool arguments.\\nIf this is a string, it will be appended to existing JSON arguments. If this is a dict, it will be merged with existing dict arguments.\\n####  tool_call_id `class-attribute` `instance-attribute`\\n\\n\\nOptional tool call identifier, this is used by some models including OpenAI.\\nNote this is never treated as a delta — it can replace None, but otherwise if a non-matching value is provided an error will be raised.\\n####  part_delta_kind `class-attribute` `instance-attribute`\\n\\n\\nPart delta type identifier, used as a discriminator.\\n####  as_part'),\n",
       " Document(metadata={'chunk_id': 78, 'type': 'text', 'num_code_blocks': 13, 'url': 'https://pydantic.com'}, page_content='Convert this delta to a fully formed `ToolCallPart` if possible, otherwise return `None`.\\nReturns:\\nType | Description  \\n---|---  \\n`ToolCallPart | None` |  A `ToolCallPart` if both `tool_name_delta` and `args_delta` are set, otherwise `None`.  \\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\\n\\n| \\n\\n---|---  \\n####  apply\\n\\n\\n\\n\\n\\n\\nApply this delta to a part or delta, returning a new part or delta with the changes applied.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`part` |  `ModelResponsePart | ToolCallPartDelta` |  The existing model response part or delta to update. |  _required_  \\nReturns:\\nType | Description  \\n---|---  \\n`ToolCallPart | ToolCallPartDelta` |  Either a new `ToolCallPart` or an updated `ToolCallPartDelta`.  \\nRaises:\\nType | Description  \\n---|---  \\n`ValueError` |  If `part` is neither a `ToolCallPart` nor a `ToolCallPartDelta`.  \\n`UnexpectedModelBehavior` |  If applying JSON deltas to dict arguments or vice versa.  \\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\\n\\n| \\n\\n---|---  \\n###  ModelResponsePartDelta `module-attribute`\\n\\n\\nA partial update (delta) for any model response part.\\n###  PartStartEvent `dataclass`\\nAn event indicating that a new part has started.\\nIf multiple `PartStartEvent`s are received with the same index, the new one should fully replace the old one.\\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\\n\\n| \\n\\n---|---  \\n####  index `instance-attribute`\\n\\n\\nThe index of the part within the overall response parts list.\\n####  part `instance-attribute`\\n\\n\\nThe newly started `ModelResponsePart`.\\n####  event_kind `class-attribute` `instance-attribute`'),\n",
       " Document(metadata={'chunk_id': 79, 'type': 'text', 'num_code_blocks': 16, 'url': 'https://pydantic.com'}, page_content=\"Event type identifier, used as a discriminator.\\n###  PartDeltaEvent `dataclass`\\nAn event indicating a delta update for an existing part.\\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\\n\\n| \\n\\n---|---  \\n####  index `instance-attribute`\\n\\n\\nThe index of the part within the overall response parts list.\\n####  delta `instance-attribute`\\n\\n\\nThe delta to apply to the specified part.\\n####  event_kind `class-attribute` `instance-attribute`\\n\\n\\nEvent type identifier, used as a discriminator.\\n###  ModelResponseStreamEvent `module-attribute`\\n\\n\\nAn event in the model response stream, either starting a new part or applying a delta to an existing one.\\n###  FunctionToolCallEvent `dataclass`\\nAn event indicating the start to a call to a function tool.\\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\\n\\n| \\n\\n---|---  \\n####  part `instance-attribute`\\n\\n\\nThe (function) tool call to make.\\n####  call_id `class-attribute` `instance-attribute`\\n\\n\\nAn ID used for matching details about the call to its result. If present, defaults to the part's tool_call_id.\\n####  event_kind `class-attribute` `instance-attribute`\\n\\n\\nEvent type identifier, used as a discriminator.\\n###  FunctionToolResultEvent `dataclass`\\nAn event indicating the result of a function tool call.\\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\\n\\n| \\n\\n---|---  \\n####  result `instance-attribute`\\n\\n\\nThe result of the call to the function tool.\\n####  call_id `instance-attribute`\\n\\n\\nAn ID used to match the result to its original call.\\n####  event_kind `class-attribute` `instance-attribute`\\n\\n\\nEvent type identifier, used as a discriminator.\\n© Pydantic Services Inc. 2024 to present\\n\\n---\"),\n",
       " Document(metadata={'chunk_id': 80, 'type': 'text', 'num_code_blocks': 8, 'url': 'https://pydantic.com'}, page_content='result  \\n  *  ResultDataT  \\n  *  StreamedRunResult  \\n    *  is_complete  \\n    *  all_messages  \\n    *  all_messages_json  \\n    *  new_messages  \\n    *  new_messages_json  \\n    *  stream  \\n    *  stream_text  \\n    *  stream_structured  \\n    *  get_data  \\n    *  usage  \\n    *  timestamp  \\n    *  validate_structured_result  \\n\\n  1.  Introduction  \\n  2.  API Reference  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# `pydantic_ai.result`\\n###  ResultDataT `module-attribute`\\n\\n\\nCovariant type variable for the result data type of a run.\\n###  StreamedRunResult `dataclass`\\nBases: `GenericAgentDepsT[, ResultDataT]`\\nResult of a streamed run that returns structured data via a tool call.\\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\\n\\n| \\n\\n---|---  \\n####  is_complete `class-attribute` `instance-attribute`\\n\\n\\nWhether the stream has all been received.\\nThis is set to `True` when one of `stream`, `stream_text`, `stream_structured` or `get_data` completes.\\n####  all_messages\\n\\n\\nReturn the history of _messages.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`result_tool_return_content` |  `str | None` |  The return content of the tool call to set in the last message. This provides a convenient way to modify the content of the result tool call if you want to continue the conversation and want to set the response to the result tool call. If `None`, the last message will not be modified. |  `None`  \\nReturns:\\nType | Description  \\n---|---  \\n`listModelMessage[]` |  List of messages.  \\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\\n\\n| \\n\\n---|---  \\n####  all_messages_json'),\n",
       " Document(metadata={'chunk_id': 81, 'type': 'text', 'num_code_blocks': 6, 'url': 'https://pydantic.com'}, page_content='Return all messages from `all_messages` as JSON bytes.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`result_tool_return_content` |  `str | None` |  The return content of the tool call to set in the last message. This provides a convenient way to modify the content of the result tool call if you want to continue the conversation and want to set the response to the result tool call. If `None`, the last message will not be modified. |  `None`  \\nReturns:\\nType | Description  \\n---|---  \\n`bytes` |  JSON bytes representing the messages.  \\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\\n\\n| \\n\\n---|---  \\n####  new_messages\\n\\n\\nReturn new messages associated with this run.\\nMessages from older runs are excluded.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`result_tool_return_content` |  `str | None` |  The return content of the tool call to set in the last message. This provides a convenient way to modify the content of the result tool call if you want to continue the conversation and want to set the response to the result tool call. If `None`, the last message will not be modified. |  `None`  \\nReturns:\\nType | Description  \\n---|---  \\n`listModelMessage[]` |  List of new messages.  \\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\\n\\n| \\n\\n---|---  \\n####  new_messages_json\\n\\n\\nReturn new messages from `new_messages` as JSON bytes.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`result_tool_return_content` |  `str | None` |  The return content of the tool call to set in the last message. This provides a convenient way to modify the content of the result tool call if you want to continue the conversation and want to set the response to the result tool call. If `None`, the last message will not be modified. |  `None`  \\nReturns:\\nType | Description  \\n---|---  \\n`bytes` |  JSON bytes representing the new messages.'),\n",
       " Document(metadata={'chunk_id': 82, 'type': 'text', 'num_code_blocks': 9, 'url': 'https://pydantic.com'}, page_content='Source code in `pydantic_ai_slim/pydantic_ai/result.py`\\n\\n| \\n\\n---|---  \\n####  stream `async`\\n\\n\\nStream the response as an async iterable.\\nThe pydantic validator for structured data will be called in partial mode on each iteration.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`debounce_by` |  `float | None` |  by how much (if at all) to debounce/group the response chunks by. `None` means no debouncing. Debouncing is particularly important for long structured responses to reduce the overhead of performing validation as each token is received. |  `0.1`  \\nReturns:\\nType | Description  \\n---|---  \\n`AsyncIteratorResultDataT[]` |  An async iterable of the response data.  \\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\\n\\n| \\n\\n---|---  \\n####  stream_text `async`\\n\\n\\nStream the text result as an async iterable.\\nNote\\nResult validators will NOT be called on the text result if `delta=True`.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`delta` |  `bool` |  if `True`, yield each chunk of text as it is received, if `False` (default), yield the full text up to the current point. |  `False`  \\n`debounce_by` |  `float | None` |  by how much (if at all) to debounce/group the response chunks by. `None` means no debouncing. Debouncing is particularly important for long structured responses to reduce the overhead of performing validation as each token is received. |  `0.1`  \\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\\n\\n| \\n\\n---|---  \\n####  stream_structured `async`\\n\\n\\nStream the response as an async iterable of Structured LLM Messages.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`debounce_by` |  `float | None` |  by how much (if at all) to debounce/group the response chunks by. `None` means no debouncing.'),\n",
       " Document(metadata={'chunk_id': 83, 'type': 'text', 'num_code_blocks': 14, 'url': 'https://pydantic.com'}, page_content=\"Debouncing is particularly important for long structured responses to reduce the overhead of performing validation as each token is received. |  `0.1`  \\nReturns:\\nType | Description  \\n---|---  \\n`AsyncIteratortuple[ModelResponse[, bool]]` |  An async iterable of the structured response message and whether that is the last message.  \\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\\n\\n| \\n\\n---|---  \\n####  get_data `async`\\n\\n\\nStream the whole response, validate and return it.\\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\\n\\n| \\n\\n---|---  \\n####  usage\\n\\n\\nReturn the usage of the whole run.\\nNote\\nThis won't return the full usage until the stream is finished.\\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\\n\\n| \\n\\n---|---  \\n####  timestamp\\n\\n\\nGet the timestamp of the response.\\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\\n\\n| \\n\\n---|---  \\n####  validate_structured_result `async`\\n\\n\\nValidate a structured result message.\\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\\n\\n| \\n\\n---|---  \\n© Pydantic Services Inc. 2024 to present\\n\\n---\"),\n",
       " Document(metadata={'chunk_id': 84, 'type': 'text', 'num_code_blocks': 6, 'url': 'https://pydantic.com'}, page_content=\"settings  \\n  *  ModelSettings  \\n    *  max_tokens  \\n    *  temperature  \\n    *  top_p  \\n    *  timeout  \\n    *  parallel_tool_calls  \\n    *  seed  \\n    *  presence_penalty  \\n    *  frequency_penalty  \\n    *  logit_bias  \\n\\n  1.  Introduction  \\n  2.  API Reference  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# `pydantic_ai.settings`\\n###  ModelSettings\\nBases: `TypedDict`\\nSettings to configure an LLM.\\nHere we include only settings which apply to multiple models / model providers, though not all of these settings are supported by all models.\\nSource code in `pydantic_ai_slim/pydantic_ai/settings.py`\\n\\n| \\n\\n---|---  \\n####  max_tokens `instance-attribute`\\n\\n\\nThe maximum number of tokens to generate before stopping.\\nSupported by:\\n  * Gemini\\n  * Anthropic\\n  * OpenAI\\n  * Groq\\n  * Cohere\\n  * Mistral\\n\\n####  temperature `instance-attribute`\\n\\n\\nAmount of randomness injected into the response.\\nUse `temperature` closer to `0.0` for analytical / multiple choice, and closer to a model's maximum `temperature` for creative and generative tasks.\\nNote that even with `temperature` of `0.0`, the results will not be fully deterministic.\\nSupported by:\\n  * Gemini\\n  * Anthropic\\n  * OpenAI\\n  * Groq\\n  * Cohere\\n  * Mistral\\n\\n####  top_p `instance-attribute`\\n\\n\\nAn alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.\\nSo 0.1 means only the tokens comprising the top 10% probability mass are considered.\\nYou should either alter `temperature` or `top_p`, but not both.\\nSupported by:\\n  * Gemini\\n  * Anthropic\\n  * OpenAI\\n  * Groq\\n  * Cohere\\n  * Mistral\\n\\n####  timeout `instance-attribute`\"),\n",
       " Document(metadata={'chunk_id': 85, 'type': 'text', 'num_code_blocks': 5, 'url': 'https://pydantic.com'}, page_content='Override the client-level default timeout for a request, in seconds.\\nSupported by:\\n  * Gemini\\n  * Anthropic\\n  * OpenAI\\n  * Groq\\n  * Mistral\\n\\n####  parallel_tool_calls `instance-attribute`\\n\\n\\nWhether to allow parallel tool calls.\\nSupported by:\\n  * OpenAI (some models, not o1)\\n  * Groq\\n  * Anthropic\\n\\n####  seed `instance-attribute`\\n\\n\\nThe random seed to use for the model, theoretically allowing for deterministic results.\\nSupported by:\\n  * OpenAI\\n  * Groq\\n  * Cohere\\n  * Mistral\\n\\n####  presence_penalty `instance-attribute`\\n\\n\\nPenalize new tokens based on whether they have appeared in the text so far.\\nSupported by:\\n  * OpenAI\\n  * Groq\\n  * Cohere\\n  * Gemini\\n  * Mistral\\n\\n####  frequency_penalty `instance-attribute`\\n\\n\\nPenalize new tokens based on their existing frequency in the text so far.\\nSupported by:\\n  * OpenAI\\n  * Groq\\n  * Cohere\\n  * Gemini\\n  * Mistral\\n\\n####  logit_bias `instance-attribute`\\n\\n\\nModify the likelihood of specified tokens appearing in the completion.\\nSupported by:\\n  * OpenAI\\n  * Groq\\n\\n© Pydantic Services Inc. 2024 to present\\n\\n---'),\n",
       " Document(metadata={'chunk_id': 86, 'type': 'text', 'num_code_blocks': 13, 'url': 'https://pydantic.com'}, page_content='tools  \\n  *  AgentDepsT  \\n  *  RunContext  \\n    *  deps  \\n    *  model  \\n    *  usage  \\n    *  prompt  \\n    *  messages  \\n    *  tool_name  \\n    *  retry  \\n    *  run_step  \\n  *  ToolParams  \\n  *  SystemPromptFunc  \\n  *  ToolFuncContext  \\n  *  ToolFuncPlain  \\n  *  ToolFuncEither  \\n  *  ToolPrepareFunc  \\n  *  DocstringFormat  \\n  *  Tool  \\n    *  __init__  \\n    *  prepare_tool_def  \\n    *  run  \\n  *  ObjectJsonSchema  \\n  *  ToolDefinition  \\n    *  name  \\n    *  description  \\n    *  parameters_json_schema  \\n    *  outer_typed_dict_key  \\n\\n  1.  Introduction  \\n  2.  API Reference  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# `pydantic_ai.tools`\\n###  AgentDepsT `module-attribute`\\n\\n\\nType variable for agent dependencies.\\n###  RunContext `dataclass`\\nBases: `GenericAgentDepsT[]`\\nInformation about the current call.\\nSource code in `pydantic_ai_slim/pydantic_ai/tools.py`\\n\\n| \\n\\n---|---  \\n####  deps `instance-attribute`\\n\\n\\nDependencies for the agent.\\n####  model `instance-attribute`\\n\\n\\nThe model used in this run.\\n####  usage `instance-attribute`\\n\\n\\nLLM usage associated with the run.\\n####  prompt `instance-attribute`\\n\\n\\nThe original user prompt passed to the run.\\n####  messages `class-attribute` `instance-attribute`\\n\\n\\nMessages exchanged in the conversation so far.\\n####  tool_name `class-attribute` `instance-attribute`\\n\\n\\nName of the tool being called.\\n####  retry `class-attribute` `instance-attribute`\\n\\n\\nNumber of retries so far.\\n####  run_step `class-attribute` `instance-attribute`\\n\\n\\nThe current step in the run.\\n###  ToolParams `module-attribute`\\n\\n\\nRetrieval function param spec.\\n###  SystemPromptFunc `module-attribute`'),\n",
       " Document(metadata={'chunk_id': 87, 'type': 'text', 'num_code_blocks': 6, 'url': 'https://pydantic.com'}, page_content=\"A function that may or maybe not take `RunContext` as an argument, and may or may not be async.\\nUsage `SystemPromptFunc[AgentDepsT]`.\\n###  ToolFuncContext `module-attribute`\\n\\n\\nA tool function that takes `RunContext` as the first argument.\\nUsage `ToolContextFunc[AgentDepsT, ToolParams]`.\\n###  ToolFuncPlain `module-attribute`\\n\\n\\nA tool function that does not take `RunContext` as the first argument.\\nUsage `ToolPlainFunc[ToolParams]`.\\n###  ToolFuncEither `module-attribute`\\n\\n\\nEither kind of tool function.\\nThis is just a union of `ToolFuncContext` and `ToolFuncPlain`.\\nUsage `ToolFuncEither[AgentDepsT, ToolParams]`.\\n###  ToolPrepareFunc `module-attribute`\\n\\n\\nDefinition of a function that can prepare a tool definition at call time.\\nSee tool docs for more information.\\nExample — here `only_if_42` is valid as a `ToolPrepareFunc`:\\n\\n\\nUsage `ToolPrepareFunc[AgentDepsT]`.\\n###  DocstringFormat `module-attribute`\\n\\n\\nSupported docstring formats.\\n  * `'google'` — Google-style docstrings.\\n  * `'numpy'` — Numpy-style docstrings.\\n  * `'sphinx'` — Sphinx-style docstrings.\\n  * `'auto'` — Automatically infer the format based on the structure of the docstring.\"),\n",
       " Document(metadata={'chunk_id': 88, 'type': 'text', 'num_code_blocks': 7, 'url': 'https://pydantic.com'}, page_content='###  Tool `dataclass`\\nBases: `GenericAgentDepsT[]`\\nA tool function for an agent.\\nSource code in `pydantic_ai_slim/pydantic_ai/tools.py`\\n\\n| python {noqa=\"I001\"}\\n    from pydantic_ai import Agent, RunContext, Tool\\n    async def my_tool(ctx: RunContext[int], x: int, y: int) -> str:\\n      return f\\'{ctx.deps} {x} {y}\\'\\n    agent = Agent(\\'test\\', tools=[Tool(my_tool)])\\npython {noqa=\"I001\"}\\n    from typing import Union\\n    from pydantic_ai import Agent, RunContext, Tool\\n    from pydantic_ai.tools import ToolDefinition\\n    async def my_tool(ctx: RunContext[int], x: int, y: int) -> str:\\n      return f\\'{ctx.deps} {x} {y}\\'\\n    async def prep_my_tool(\\n      ctx: RunContext[int], tool_def: ToolDefinition\\n    ) -> Union[ToolDefinition, None]:\\n      # only register the tool if `deps == 42`\\n      if ctx.deps == 42:\\n        return tool_def\\n    agent = Agent(\\'test\\', tools=[Tool(my_tool, prepare=prep_my_tool)])\\n\\n\\n---|---  \\n####  __init__\\n\\n\\nCreate a new tool instance.\\nExample usage:\\n\\n\\nor with a custom prepare method:\\n\\n\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`function` |  `ToolFuncEitherAgentDepsT[]` |  The Python function to call as the tool. |  _required_  \\n`takes_ctx` |  `bool | None` |  Whether the function takes a `RunContext` first argument, this is inferred if unset. |  `None`  \\n`max_retries` |  `int | None` |  Maximum number of retries allowed for this tool, set to the agent default if `None`. |  `None`  \\n`name` |  `str | None` |  Name of the tool, inferred from the function if `None`. |  `None`  \\n`description` |  `str | None` |  Description of the tool, inferred from the function if `None`. |  `None`  \\n`prepare` |  `ToolPrepareFuncAgentDepsT[] | None` |  custom method to prepare the tool definition for each step, return `None` to omit this tool from a given step. This is useful if you want to customise a tool at call time, or omit it completely from a step.'),\n",
       " Document(metadata={'chunk_id': 89, 'type': 'text', 'num_code_blocks': 11, 'url': 'https://pydantic.com'}, page_content='See `ToolPrepareFunc`. |  `None`  \\n`docstring_format` |  `DocstringFormat` |  The format of the docstring, see `DocstringFormat`. Defaults to `\\'auto\\'`, such that the format is inferred from the structure of the docstring. |  `\\'auto\\'`  \\n`require_parameter_descriptions` |  `bool` |  If True, raise an error if a parameter description is missing. Defaults to False. |  `False`  \\nSource code in `pydantic_ai_slim/pydantic_ai/tools.py`\\n\\n| python {noqa=\"I001\"}\\n  from pydantic_ai import Agent, RunContext, Tool\\n  async def my_tool(ctx: RunContext[int], x: int, y: int) -> str:\\n    return f\\'{ctx.deps} {x} {y}\\'\\n  agent = Agent(\\'test\\', tools=[Tool(my_tool)])\\n  python {noqa=\"I001\"}\\n  from typing import Union\\n  from pydantic_ai import Agent, RunContext, Tool\\n  from pydantic_ai.tools import ToolDefinition\\n  async def my_tool(ctx: RunContext[int], x: int, y: int) -> str:\\n    return f\\'{ctx.deps} {x} {y}\\'\\n  async def prep_my_tool(\\n    ctx: RunContext[int], tool_def: ToolDefinition\\n  ) -> Union[ToolDefinition, None]:\\n    # only register the tool if `deps == 42`\\n    if ctx.deps == 42:\\n      return tool_def\\n  agent = Agent(\\'test\\', tools=[Tool(my_tool, prepare=prep_my_tool)])\\n  \\n\\n---|---  \\n####  prepare_tool_def `async`\\n\\n\\nGet the tool definition.\\nBy default, this method creates a tool definition, then either returns it, or calls `self.prepare` if it\\'s set.\\nReturns:\\nType | Description  \\n---|---  \\n`ToolDefinition | None` |  return a `ToolDefinition` or `None` if the tools should not be registered for this run.  \\nSource code in `pydantic_ai_slim/pydantic_ai/tools.py`\\n\\n| \\n\\n---|---  \\n####  run `async`\\n\\n\\nRun the tool function asynchronously.\\nSource code in `pydantic_ai_slim/pydantic_ai/tools.py`\\n\\n| \\n\\n---|---  \\n###  ObjectJsonSchema `module-attribute`\\n\\n\\nType representing JSON schema of an object, e.g.'),\n",
       " Document(metadata={'chunk_id': 90, 'type': 'text', 'num_code_blocks': 6, 'url': 'https://pydantic.com'}, page_content='where `\"type\": \"object\"`.\\nThis type is used to define tools parameters (aka arguments) in ToolDefinition.\\nWith PEP-728 this should be a TypedDict with `type: Literal[\\'object\\']`, and `extra_parts=Any`\\n###  ToolDefinition `dataclass`\\nDefinition of a tool passed to a model.\\nThis is used for both function tools result tools.\\nSource code in `pydantic_ai_slim/pydantic_ai/tools.py`\\n\\n| \\n\\n---|---  \\n####  name `instance-attribute`\\n\\n\\nThe name of the tool.\\n####  description `instance-attribute`\\n\\n\\nThe description of the tool.\\n####  parameters_json_schema `instance-attribute`\\n\\n\\nThe JSON schema for the tool\\'s parameters.\\n####  outer_typed_dict_key `class-attribute` `instance-attribute`\\n\\n\\nThe key in the outer [TypedDict] that wraps a result tool.\\nThis will only be set for result tools which don\\'t have an `object` JSON schema.\\n© Pydantic Services Inc. 2024 to present\\n\\n---'),\n",
       " Document(metadata={'chunk_id': 91, 'type': 'text', 'num_code_blocks': 10, 'url': 'https://pydantic.com'}, page_content=\"usage  \\n  *  Usage  \\n    *  requests  \\n    *  request_tokens  \\n    *  response_tokens  \\n    *  total_tokens  \\n    *  details  \\n    *  incr  \\n    *  __add__  \\n  *  UsageLimits  \\n    *  request_limit  \\n    *  request_tokens_limit  \\n    *  response_tokens_limit  \\n    *  total_tokens_limit  \\n    *  has_token_limits  \\n    *  check_before_request  \\n    *  check_tokens  \\n\\n  1.  Introduction  \\n  2.  API Reference  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# `pydantic_ai.usage`\\n###  Usage `dataclass`\\nLLM usage associated with a request or run.\\nResponsibility for calculating usage is on the model; PydanticAI simply sums the usage information across requests.\\nYou'll need to look up the documentation of the model you're using to convert usage to monetary costs.\\nSource code in `pydantic_ai_slim/pydantic_ai/usage.py`\\n\\n| \\n\\n---|---  \\n####  requests `class-attribute` `instance-attribute`\\n\\n\\nNumber of requests made to the LLM API.\\n####  request_tokens `class-attribute` `instance-attribute`\\n\\n\\nTokens used in processing requests.\\n####  response_tokens `class-attribute` `instance-attribute`\\n\\n\\nTokens used in generating responses.\\n####  total_tokens `class-attribute` `instance-attribute`\\n\\n\\nTotal tokens used in the whole run, should generally be equal to `request_tokens + response_tokens`.\\n####  details `class-attribute` `instance-attribute`\\n\\n\\nAny extra details returned by the model.\\n####  incr\\n\\n\\nIncrement the usage in place.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`incr_usage` |  `Usage` |  The usage to increment by. |  _required_  \\n`requests` |  `int` |  The number of requests to increment by in addition to `incr_usage.requests`. |  `0`  \\nSource code in `pydantic_ai_slim/pydantic_ai/usage.py`\\n\\n|\"),\n",
       " Document(metadata={'chunk_id': 92, 'type': 'text', 'num_code_blocks': 16, 'url': 'https://pydantic.com'}, page_content=\"---|---  \\n####  __add__\\n\\n\\nAdd two Usages together.\\nThis is provided so it's trivial to sum usage information from multiple requests and runs.\\nSource code in `pydantic_ai_slim/pydantic_ai/usage.py`\\n\\n| \\n\\n---|---  \\n###  UsageLimits `dataclass`\\nLimits on model usage.\\nThe request count is tracked by pydantic_ai, and the request limit is checked before each request to the model. Token counts are provided in responses from the model, and the token limits are checked after each response.\\nEach of the limits can be set to `None` to disable that limit.\\nSource code in `pydantic_ai_slim/pydantic_ai/usage.py`\\n\\n| \\n\\n---|---  \\n####  request_limit `class-attribute` `instance-attribute`\\n\\n\\nThe maximum number of requests allowed to the model.\\n####  request_tokens_limit `class-attribute` `instance-attribute`\\n\\n\\nThe maximum number of tokens allowed in requests to the model.\\n####  response_tokens_limit `class-attribute` `instance-attribute`\\n\\n\\nThe maximum number of tokens allowed in responses from the model.\\n####  total_tokens_limit `class-attribute` `instance-attribute`\\n\\n\\nThe maximum number of tokens allowed in requests and responses combined.\\n####  has_token_limits\\n\\n\\nReturns `True` if this instance places any limits on token counts.\\nIf this returns `False`, the `check_tokens` method will never raise an error.\\nThis is useful because if we have token limits, we need to check them after receiving each streamed message. If there are no limits, we can skip that processing in the streaming response iterator.\\nSource code in `pydantic_ai_slim/pydantic_ai/usage.py`\\n\\n| \\n\\n---|---  \\n####  check_before_request\\n\\n\\nRaises a `UsageLimitExceeded` exception if the next request would exceed the request_limit.\\nSource code in `pydantic_ai_slim/pydantic_ai/usage.py`\\n\\n| \\n\\n---|---  \\n####  check_tokens\"),\n",
       " Document(metadata={'chunk_id': 93, 'type': 'text', 'num_code_blocks': 2, 'url': 'https://pydantic.com'}, page_content='Raises a `UsageLimitExceeded` exception if the usage exceeds any of the token limits.\\nSource code in `pydantic_ai_slim/pydantic_ai/usage.py`\\n\\n| \\n\\n---|---  \\n© Pydantic Services Inc. 2024 to present\\n\\n---'),\n",
       " Document(metadata={'chunk_id': 94, 'type': 'text', 'num_code_blocks': 8, 'url': 'https://pydantic.com'}, page_content='Setup  \\n    *  anthropic  \\n    *  LatestAnthropicModelNames  \\n    *  AnthropicModelName  \\n    *  AnthropicModelSettings  \\n      *  anthropic_metadata  \\n    *  AnthropicModel  \\n      *  __init__  \\n      *  model_name  \\n      *  system  \\n    *  AnthropicStreamedResponse  \\n      *  model_name  \\n      *  timestamp  \\n\\n  1.  Introduction  \\n  2.  API Reference  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# `pydantic_ai.models.anthropic`\\n## Setup\\nFor details on how to set up authentication with this model, see model configuration for Anthropic.\\n###  LatestAnthropicModelNames `module-attribute`\\n\\n\\nLatest Anthropic models.\\n###  AnthropicModelName `module-attribute`\\n\\n\\nPossible Anthropic model names.\\nSince Anthropic supports a variety of date-stamped models, we explicitly list the latest models but allow any name in the type hints. See the Anthropic docs for a full list.\\n###  AnthropicModelSettings\\nBases: `ModelSettings`\\nSettings used for an Anthropic model request.\\nSource code in `pydantic_ai_slim/pydantic_ai/models/anthropic.py`\\n\\n| \\n\\n---|---  \\n####  anthropic_metadata `instance-attribute`\\n\\n\\nAn object describing metadata about the request.\\nContains `user_id`, an external identifier for the user who is associated with the request.\\n###  AnthropicModel `dataclass`\\nBases: `Model`\\nA model that uses the Anthropic API.\\nInternally, this uses the Anthropic Python client to interact with the API.\\nApart from `__init__`, all methods are private or match those of the base class.\\nNote\\nThe `AnthropicModel` class does not yet support streaming responses. We anticipate adding support for streaming responses in a near-term future release.\\nSource code in `pydantic_ai_slim/pydantic_ai/models/anthropic.py`\\n\\n| \\n\\n---|---  \\n####  __init__'),\n",
       " Document(metadata={'chunk_id': 95, 'type': 'text', 'num_code_blocks': 8, 'url': 'https://pydantic.com'}, page_content='Initialize an Anthropic model.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`model_name` |  `AnthropicModelName` |  The name of the Anthropic model to use. List of model names available here. |  _required_  \\n`api_key` |  `str | None` |  The API key to use for authentication, if not provided, the `ANTHROPIC_API_KEY` environment variable will be used if available. |  `None`  \\n`anthropic_client` |  `AsyncAnthropic | None` |  An existing `AsyncAnthropic` client to use, if provided, `api_key` and `http_client` must be `None`. |  `None`  \\n`http_client` |  `AsyncClient | None` |  An existing `httpx.AsyncClient` to use for making HTTP requests. |  `None`  \\nSource code in `pydantic_ai_slim/pydantic_ai/models/anthropic.py`\\n\\n| \\n\\n---|---  \\n####  model_name `property`\\n\\n\\nThe model name.\\n####  system `property`\\n\\n\\nThe system / model provider.\\n###  AnthropicStreamedResponse `dataclass`\\nBases: `StreamedResponse`\\nImplementation of `StreamedResponse` for Anthropic models.\\nSource code in `pydantic_ai_slim/pydantic_ai/models/anthropic.py`\\n\\n| \\n\\n---|---  \\n####  model_name `property`\\n\\n\\nGet the model name of the response.\\n####  timestamp `property`\\n\\n\\nGet the timestamp of the response.\\n© Pydantic Services Inc. 2024 to present\\n\\n---'),\n",
       " Document(metadata={'chunk_id': 96, 'type': 'text', 'num_code_blocks': 13, 'url': 'https://pydantic.com'}, page_content=\"models  \\n  *  KnownModelName  \\n  *  ModelRequestParameters  \\n  *  Model  \\n    *  request  \\n    *  request_stream  \\n    *  model_name  \\n    *  system  \\n  *  StreamedResponse  \\n    *  __aiter__  \\n    *  get  \\n    *  usage  \\n    *  model_name  \\n    *  timestamp  \\n  *  ALLOW_MODEL_REQUESTS  \\n  *  check_allow_model_requests  \\n  *  override_allow_model_requests  \\n\\n  1.  Introduction  \\n  2.  API Reference  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# `pydantic_ai.models`\\nLogic related to making requests to an LLM.\\nThe aim here is to make a common interface for different LLMs, so that the rest of the code can be agnostic to the specific LLM being used.\\n###  KnownModelName `module-attribute`\\n\\n\\nKnown model names that can be used with the `model` parameter of `Agent`.\\n`KnownModelName` is provided as a concise way to specify a model.\\n###  ModelRequestParameters `dataclass`\\nConfiguration for an agent's request to a model, specifically related to tools and result handling.\\nSource code in `pydantic_ai_slim/pydantic_ai/models/__init__.py`\\n\\n| \\n\\n---|---  \\n###  Model\\nBases: `ABC`\\nAbstract class for a model.\\nSource code in `pydantic_ai_slim/pydantic_ai/models/__init__.py`\\n\\n| \\n\\n---|---  \\n####  request `abstractmethod` `async`\\n\\n\\nMake a request to the model.\\nSource code in `pydantic_ai_slim/pydantic_ai/models/__init__.py`\\n\\n| \\n\\n---|---  \\n####  request_stream `async`\\n\\n\\nMake a request to the model and return a streaming response.\\nSource code in `pydantic_ai_slim/pydantic_ai/models/__init__.py`\\n\\n| \\n\\n---|---  \\n####  model_name `abstractmethod` `property`\\n\\n\\nThe model name.\\n####  system `abstractmethod` `property`\"),\n",
       " Document(metadata={'chunk_id': 97, 'type': 'text', 'num_code_blocks': 18, 'url': 'https://pydantic.com'}, page_content=\"The system / model provider, ex: openai.\\n###  StreamedResponse `dataclass`\\nBases: `ABC`\\nStreamed response from an LLM when calling a tool.\\nSource code in `pydantic_ai_slim/pydantic_ai/models/__init__.py`\\n\\n| \\n\\n---|---  \\n####  __aiter__\\n\\n\\nStream the response as an async iterable of `ModelResponseStreamEvent`s.\\nSource code in `pydantic_ai_slim/pydantic_ai/models/__init__.py`\\n\\n| \\n\\n---|---  \\n####  get\\n\\n\\nBuild a `ModelResponse` from the data received from the stream so far.\\nSource code in `pydantic_ai_slim/pydantic_ai/models/__init__.py`\\n\\n| \\n\\n---|---  \\n####  usage\\n\\n\\nGet the usage of the response so far. This will not be the final usage until the stream is exhausted.\\nSource code in `pydantic_ai_slim/pydantic_ai/models/__init__.py`\\n\\n| \\n\\n---|---  \\n####  model_name `abstractmethod` `property`\\n\\n\\nGet the model name of the response.\\n####  timestamp `abstractmethod` `property`\\n\\n\\nGet the timestamp of the response.\\n###  ALLOW_MODEL_REQUESTS `module-attribute`\\n\\n\\nWhether to allow requests to models.\\nThis global setting allows you to disable request to most models, e.g. to make sure you don't accidentally make costly requests to a model during tests.\\nThe testing models `TestModel` and `FunctionModel` are no affected by this setting.\\n###  check_allow_model_requests\\n\\n\\nCheck if model requests are allowed.\\nIf you're defining your own models that have costs or latency associated with their use, you should call this in `Model.request` and `Model.request_stream`.\\nRaises:\\nType | Description  \\n---|---  \\n`RuntimeError` |  If model requests are not allowed.  \\nSource code in `pydantic_ai_slim/pydantic_ai/models/__init__.py`\\n\\n| \\n\\n---|---  \\n###  override_allow_model_requests\"),\n",
       " Document(metadata={'chunk_id': 98, 'type': 'text', 'num_code_blocks': 2, 'url': 'https://pydantic.com'}, page_content='Context manager to temporarily override `ALLOW_MODEL_REQUESTS`.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`allow_model_requests` |  `bool` |  Whether to allow model requests within the context. |  _required_  \\nSource code in `pydantic_ai_slim/pydantic_ai/models/__init__.py`\\n\\n| \\n\\n---|---  \\n© Pydantic Services Inc. 2024 to present\\n\\n---'),\n",
       " Document(metadata={'chunk_id': 99, 'type': 'text', 'num_code_blocks': 7, 'url': 'https://pydantic.com'}, page_content=\"Setup  \\n    *  cohere  \\n    *  LatestCohereModelNames  \\n    *  CohereModelName  \\n    *  CohereModelSettings  \\n    *  CohereModel  \\n      *  __init__  \\n      *  model_name  \\n      *  system  \\n\\n  1.  Introduction  \\n  2.  API Reference  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# `pydantic_ai.models.cohere`\\n## Setup\\nFor details on how to set up authentication with this model, see model configuration for Cohere.\\n###  LatestCohereModelNames `module-attribute`\\n\\n\\nLatest Cohere models.\\n###  CohereModelName `module-attribute`\\n\\n\\nPossible Cohere model names.\\nSince Cohere supports a variety of date-stamped models, we explicitly list the latest models but allow any name in the type hints. See Cohere's docs for a list of all available models.\\n###  CohereModelSettings\\nBases: `ModelSettings`\\nSettings used for a Cohere model request.\\nSource code in `pydantic_ai_slim/pydantic_ai/models/cohere.py`\\n\\n| \\n\\n---|---  \\n###  CohereModel `dataclass`\\nBases: `Model`\\nA model that uses the Cohere API.\\nInternally, this uses the Cohere Python client to interact with the API.\\nApart from `__init__`, all methods are private or match those of the base class.\\nSource code in `pydantic_ai_slim/pydantic_ai/models/cohere.py`\\n\\n| \\n\\n---|---  \\n####  __init__\\n\\n\\nInitialize an Cohere model.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`model_name` |  `CohereModelName` |  The name of the Cohere model to use. List of model names available here. |  _required_  \\n`api_key` |  `str | None` |  The API key to use for authentication, if not provided, the `CO_API_KEY` environment variable will be used if available. |  `None`  \\n`cohere_client` |  `AsyncClientV2 | None` |  An existing Cohere async client to use. If provided, `api_key` and `http_client` must be `None`.\"),\n",
       " Document(metadata={'chunk_id': 100, 'type': 'text', 'num_code_blocks': 4, 'url': 'https://pydantic.com'}, page_content='|  `None`  \\n`http_client` |  `AsyncClient | None` |  An existing `httpx.AsyncClient` to use for making HTTP requests. |  `None`  \\nSource code in `pydantic_ai_slim/pydantic_ai/models/cohere.py`\\n\\n| \\n\\n---|---  \\n####  model_name `property`\\n\\n\\nThe model name.\\n####  system `property`\\n\\n\\nThe system / model provider.\\n© Pydantic Services Inc. 2024 to present\\n\\n---'),\n",
       " Document(metadata={'chunk_id': 101, 'type': 'text', 'num_code_blocks': 11, 'url': 'https://pydantic.com'}, page_content=\"function  \\n  *  FunctionModel  \\n    *  __init__  \\n    *  model_name  \\n    *  system  \\n  *  AgentInfo  \\n    *  function_tools  \\n    *  allow_text_result  \\n    *  result_tools  \\n    *  model_settings  \\n  *  DeltaToolCall  \\n    *  name  \\n    *  json_args  \\n  *  DeltaToolCalls  \\n  *  FunctionDef  \\n  *  StreamFunctionDef  \\n  *  FunctionStreamedResponse  \\n    *  model_name  \\n    *  timestamp  \\n\\n  1.  Introduction  \\n  2.  API Reference  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# `pydantic_ai.models.function`\\nA model controlled by a local function.\\n`FunctionModel` is similar to `TestModel`, but allows greater control over the model's behavior.\\nIts primary use case is for more advanced unit testing than is possible with `TestModel`.\\nHere's a minimal example:\\nfunction_model_usage.py\\n\\nSee Unit testing with `FunctionModel` for detailed documentation.\\n###  FunctionModel `dataclass`\\nBases: `Model`\\nA model controlled by a local function.\\nApart from `__init__`, all methods are private or match those of the base class.\\nSource code in `pydantic_ai_slim/pydantic_ai/models/function.py`\\n\\n| \\n\\n---|---  \\n####  __init__\\n\\n\\n\\n\\n\\n\\n\\n\\nInitialize a `FunctionModel`.\\nEither `function` or `stream_function` must be provided, providing both is allowed.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`function` |  `FunctionDef | None` |  The function to call for non-streamed requests. |  `None`  \\n`stream_function` |  `StreamFunctionDef | None` |  The function to call for streamed requests. |  `None`  \\nSource code in `pydantic_ai_slim/pydantic_ai/models/function.py`\\n\\n| \\n\\n---|---  \\n####  model_name `property`\\n\\n\\nThe model name.\\n####  system `property`\"),\n",
       " Document(metadata={'chunk_id': 102, 'type': 'text', 'num_code_blocks': 15, 'url': 'https://pydantic.com'}, page_content='The system / model provider.\\n###  AgentInfo `dataclass`\\nInformation about an agent.\\nThis is passed as the second to functions used within `FunctionModel`.\\nSource code in `pydantic_ai_slim/pydantic_ai/models/function.py`\\n\\n| \\n\\n---|---  \\n####  function_tools `instance-attribute`\\n\\n\\nThe function tools available on this agent.\\nThese are the tools registered via the `tool` and `tool_plain` decorators.\\n####  allow_text_result `instance-attribute`\\n\\n\\nWhether a plain text result is allowed.\\n####  result_tools `instance-attribute`\\n\\n\\nThe tools that can called as the final result of the run.\\n####  model_settings `instance-attribute`\\n\\n\\nThe model settings passed to the run call.\\n###  DeltaToolCall `dataclass`\\nIncremental change to a tool call.\\nUsed to describe a chunk when streaming structured responses.\\nSource code in `pydantic_ai_slim/pydantic_ai/models/function.py`\\n\\n| \\n\\n---|---  \\n####  name `class-attribute` `instance-attribute`\\n\\n\\nIncremental change to the name of the tool.\\n####  json_args `class-attribute` `instance-attribute`\\n\\n\\nIncremental change to the arguments as JSON\\n###  DeltaToolCalls `module-attribute`\\n\\n\\nA mapping of tool call IDs to incremental changes.\\n###  FunctionDef `module-attribute`\\n\\n\\nA function used to generate a non-streamed response.\\n###  StreamFunctionDef `module-attribute`\\n\\n\\nA function used to generate a streamed response.\\nWhile this is defined as having return type of `AsyncIterator[Union[str, DeltaToolCalls]]`, it should really be considered as `Union[AsyncIterator[str], AsyncIterator[DeltaToolCalls]`,\\nE.g. you need to yield all text or all `DeltaToolCalls`, not mix them.\\n###  FunctionStreamedResponse `dataclass`\\nBases: `StreamedResponse`\\nImplementation of `StreamedResponse` for FunctionModel.\\nSource code in `pydantic_ai_slim/pydantic_ai/models/function.py`\\n\\n|'),\n",
       " Document(metadata={'chunk_id': 103, 'type': 'text', 'num_code_blocks': 2, 'url': 'https://pydantic.com'}, page_content='---|---  \\n####  model_name `property`\\n\\n\\nGet the model name of the response.\\n####  timestamp `property`\\n\\n\\nGet the timestamp of the response.\\n© Pydantic Services Inc. 2024 to present\\n\\n---'),\n",
       " Document(metadata={'chunk_id': 104, 'type': 'text', 'num_code_blocks': 4, 'url': 'https://pydantic.com'}, page_content=\"Setup  \\n    *  gemini  \\n    *  LatestGeminiModelNames  \\n    *  GeminiModelName  \\n    *  GeminiModelSettings  \\n    *  GeminiModel  \\n      *  __init__  \\n      *  model_name  \\n      *  system  \\n    *  AuthProtocol  \\n    *  ApiKeyAuth  \\n    *  GeminiStreamedResponse  \\n      *  model_name  \\n      *  timestamp  \\n    *  GeminiSafetySettings  \\n      *  category  \\n      *  threshold  \\n\\n  1.  Introduction  \\n  2.  API Reference  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# `pydantic_ai.models.gemini`\\nCustom interface to the `generativelanguage.googleapis.com` API using HTTPX and Pydantic.\\nThe Google SDK for interacting with the `generativelanguage.googleapis.com` API `google-generativeai` reads like it was written by a Java developer who thought they knew everything about OOP, spent 30 minutes trying to learn Python, gave up and decided to build the library to prove how horrible Python is. It also doesn't use httpx for HTTP requests, and tries to implement tool calling itself, but doesn't use Pydantic or equivalent for validation.\\nWe therefore implement support for the API directly.\\nDespite these shortcomings, the Gemini model is actually quite powerful and very fast.\\n## Setup\\nFor details on how to set up authentication with this model, see model configuration for Gemini.\\n###  LatestGeminiModelNames `module-attribute`\\n\\n\\nLatest Gemini models.\\n###  GeminiModelName `module-attribute`\\n\\n\\nPossible Gemini model names.\\nSince Gemini supports a variety of date-stamped models, we explicitly list the latest models but allow any name in the type hints. See the Gemini API docs for a full list.\\n###  GeminiModelSettings\\nBases: `ModelSettings`\\nSettings used for a Gemini model request.\\nSource code in `pydantic_ai_slim/pydantic_ai/models/gemini.py`\\n\\n|\"),\n",
       " Document(metadata={'chunk_id': 105, 'type': 'text', 'num_code_blocks': 11, 'url': 'https://pydantic.com'}, page_content=\"---|---  \\n###  GeminiModel `dataclass`\\nBases: `Model`\\nA model that uses Gemini via `generativelanguage.googleapis.com` API.\\nThis is implemented from scratch rather than using a dedicated SDK, good API documentation is available here.\\nApart from `__init__`, all methods are private or match those of the base class.\\nSource code in `pydantic_ai_slim/pydantic_ai/models/gemini.py`\\n\\n| \\n\\n---|---  \\n####  __init__\\n\\n\\nInitialize a Gemini model.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`model_name` |  `GeminiModelName` |  The name of the model to use. |  _required_  \\n`api_key` |  `str | None` |  The API key to use for authentication, if not provided, the `GEMINI_API_KEY` environment variable will be used if available. |  `None`  \\n`http_client` |  `AsyncClient | None` |  An existing `httpx.AsyncClient` to use for making HTTP requests. |  `None`  \\n`url_template` |  `str` |  The URL template to use for making requests, you shouldn't need to change this, docs here, `model` is substituted with the model name, and `function` is added to the end of the URL. |  `'https://generativelanguage.googleapis.com/v1beta/models/{model}:'`  \\nSource code in `pydantic_ai_slim/pydantic_ai/models/gemini.py`\\n\\n| \\n\\n---|---  \\n####  model_name `property`\\n\\n\\nThe model name.\\n####  system `property`\\n\\n\\nThe system / model provider.\\n###  AuthProtocol\\nBases: `Protocol`\\nAbstract definition for Gemini authentication.\\nSource code in `pydantic_ai_slim/pydantic_ai/models/gemini.py`\\n\\n| \\n\\n---|---  \\n###  ApiKeyAuth `dataclass`\\nAuthentication using an API key for the `X-Goog-Api-Key` header.\\nSource code in `pydantic_ai_slim/pydantic_ai/models/gemini.py`\\n\\n|\"),\n",
       " Document(metadata={'chunk_id': 106, 'type': 'text', 'num_code_blocks': 8, 'url': 'https://pydantic.com'}, page_content='---|---  \\n###  GeminiStreamedResponse `dataclass`\\nBases: `StreamedResponse`\\nImplementation of `StreamedResponse` for the Gemini model.\\nSource code in `pydantic_ai_slim/pydantic_ai/models/gemini.py`\\n\\n| \\n\\n---|---  \\n####  model_name `property`\\n\\n\\nGet the model name of the response.\\n####  timestamp `property`\\n\\n\\nGet the timestamp of the response.\\n###  GeminiSafetySettings\\nBases: `TypedDict`\\nSafety settings options for Gemini model request.\\nSee Gemini API docs for safety category and threshold descriptions. For an example on how to use `GeminiSafetySettings`, see here.\\nSource code in `pydantic_ai_slim/pydantic_ai/models/gemini.py`\\n\\n| \\n\\n---|---  \\n####  category `instance-attribute`\\n\\n\\nSafety settings category.\\n####  threshold `instance-attribute`\\n\\n\\nSafety settings threshold.\\n© Pydantic Services Inc. 2024 to present\\n\\n---'),\n",
       " Document(metadata={'chunk_id': 107, 'type': 'text', 'num_code_blocks': 7, 'url': 'https://pydantic.com'}, page_content='Setup  \\n    *  groq  \\n    *  LatestGroqModelNames  \\n    *  GroqModelName  \\n    *  GroqModelSettings  \\n    *  GroqModel  \\n      *  __init__  \\n      *  model_name  \\n      *  system  \\n    *  GroqStreamedResponse  \\n      *  model_name  \\n      *  timestamp  \\n\\n  1.  Introduction  \\n  2.  API Reference  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# `pydantic_ai.models.groq`\\n## Setup\\nFor details on how to set up authentication with this model, see model configuration for Groq.\\n###  LatestGroqModelNames `module-attribute`\\n\\n\\nLatest Groq models.\\n###  GroqModelName `module-attribute`\\n\\n\\nPossible Groq model names.\\nSince Groq supports a variety of date-stamped models, we explicitly list the latest models but allow any name in the type hints. See the Groq docs for a full list.\\n###  GroqModelSettings\\nBases: `ModelSettings`\\nSettings used for a Groq model request.\\nSource code in `pydantic_ai_slim/pydantic_ai/models/groq.py`\\n\\n| \\n\\n---|---  \\n###  GroqModel `dataclass`\\nBases: `Model`\\nA model that uses the Groq API.\\nInternally, this uses the Groq Python client to interact with the API.\\nApart from `__init__`, all methods are private or match those of the base class.\\nSource code in `pydantic_ai_slim/pydantic_ai/models/groq.py`\\n\\n| \\n\\n---|---  \\n####  __init__\\n\\n\\nInitialize a Groq model.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`model_name` |  `GroqModelName` |  The name of the Groq model to use. List of model names available here. |  _required_  \\n`api_key` |  `str | None` |  The API key to use for authentication, if not provided, the `GROQ_API_KEY` environment variable will be used if available. |  `None`  \\n`groq_client` |  `AsyncGroq | None` |  An existing `AsyncGroq` client to use, if provided, `api_key` and `http_client` must be `None`.'),\n",
       " Document(metadata={'chunk_id': 108, 'type': 'text', 'num_code_blocks': 8, 'url': 'https://pydantic.com'}, page_content='|  `None`  \\n`http_client` |  `AsyncClient | None` |  An existing `httpx.AsyncClient` to use for making HTTP requests. |  `None`  \\nSource code in `pydantic_ai_slim/pydantic_ai/models/groq.py`\\n\\n| \\n\\n---|---  \\n####  model_name `property`\\n\\n\\nThe model name.\\n####  system `property`\\n\\n\\nThe system / model provider.\\n###  GroqStreamedResponse `dataclass`\\nBases: `StreamedResponse`\\nImplementation of `StreamedResponse` for Groq models.\\nSource code in `pydantic_ai_slim/pydantic_ai/models/groq.py`\\n\\n| \\n\\n---|---  \\n####  model_name `property`\\n\\n\\nGet the model name of the response.\\n####  timestamp `property`\\n\\n\\nGet the timestamp of the response.\\n© Pydantic Services Inc. 2024 to present\\n\\n---'),\n",
       " Document(metadata={'chunk_id': 109, 'type': 'text', 'num_code_blocks': 4, 'url': 'https://pydantic.com'}, page_content='Setup  \\n    *  mistral  \\n    *  LatestMistralModelNames  \\n    *  MistralModelName  \\n    *  MistralModelSettings  \\n    *  MistralModel  \\n      *  __init__  \\n      *  request  \\n      *  request_stream  \\n      *  model_name  \\n      *  system  \\n    *  MistralStreamedResponse  \\n      *  model_name  \\n      *  timestamp  \\n\\n  1.  Introduction  \\n  2.  API Reference  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# `pydantic_ai.models.mistral`\\n## Setup\\nFor details on how to set up authentication with this model, see model configuration for Mistral.\\n###  LatestMistralModelNames `module-attribute`\\n\\n\\nLatest Mistral models.\\n###  MistralModelName `module-attribute`\\n\\n\\nPossible Mistral model names.\\nSince Mistral supports a variety of date-stamped models, we explicitly list the most popular models but allow any name in the type hints. Since the Mistral docs for a full list.\\n###  MistralModelSettings\\nBases: `ModelSettings`\\nSettings used for a Mistral model request.\\nSource code in `pydantic_ai_slim/pydantic_ai/models/mistral.py`\\n\\n|'),\n",
       " Document(metadata={'chunk_id': 110, 'type': 'text', 'num_code_blocks': 2, 'url': 'https://pydantic.com'}, page_content='---|---  \\n###  MistralModel `dataclass`\\nBases: `Model`\\nA model that uses Mistral.\\nInternally, this uses the Mistral Python client to interact with the API.\\nAPI Documentation\\nSource code in `pydantic_ai_slim/pydantic_ai/models/mistral.py`\\n\\n| \\\\n{schema}\\\\n```\\\\n\"\"\"\\n  _model_name: MistralModelName = field(repr=False)\\n  _system: str | None = field(default=\\'mistral\\', repr=False)\\n  def__init__(\\n    self,\\n    model_name: MistralModelName,\\n    *,\\n    api_key: str | Callable[[], str | None] | None = None,\\n    client: Mistral | None = None,\\n    http_client: AsyncHTTPClient | None = None,\\n    json_mode_schema_prompt: str = \"\"\"Answer in JSON Object, respect the format:\\\\n```\\\\n{schema}\\\\n```\\\\n\"\"\",\\n  ):\\n\"\"\"Initialize a Mistral model.\\n    Args:\\n      model_name: The name of the model to use.\\n      api_key: The API key to use for authentication, if unset uses `MISTRAL_API_KEY` environment variable.\\n      client: An existing `Mistral` client to use, if provided, `api_key` and `http_client` must be `None`.\\n      http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.\\n      json_mode_schema_prompt: The prompt to show when the model expects a JSON object as input.\\n    \"\"\"\\n    self._model_name = model_name\\n    self.json_mode_schema_prompt = json_mode_schema_prompt\\n    if client is not None:\\n      assert http_client is None, \\'Cannot provide both `mistral_client` and `http_client`\\'\\n      assert api_key is None, \\'Cannot provide both `mistral_client` and `api_key`\\'\\n      self.client = client\\n    else:\\n      api_key = os.getenv(\\'MISTRAL_API_KEY\\') if api_key is None else api_key\\n      self.client = Mistral(api_key=api_key, async_client=http_client or cached_async_http_client())\\n  async defrequest(\\n    self,\\n    messages: list[ModelMessage],\\n    model_settings: ModelSettings | None,\\n    model_request_parameters: ModelRequestParameters,\\n  ) -> tuple[ModelResponse, Usage]:\\n\"\"\"Make a non-streaming request to the model from Pydantic AI call.\"\"\"\\n    check_'),\n",
       " Document(metadata={'chunk_id': 111, 'type': 'text', 'num_code_blocks': 0, 'url': 'https://pydantic.com'}, page_content='allow_model_requests()\\n    response = await self._completions_create(\\n      messages, cast(MistralModelSettings, model_settings or {}), model_request_parameters\\n    )\\n    return self._process_response(response), _map_usage(response)\\n  @asynccontextmanager\\n  async defrequest_stream(\\n    self,\\n    messages: list[ModelMessage],\\n    model_settings: ModelSettings | None,\\n    model_request_parameters: ModelRequestParameters,\\n  ) -> AsyncIterator[StreamedResponse]:\\n\"\"\"Make a streaming request to the model from Pydantic AI call.\"\"\"\\n    check_allow_model_requests()\\n    response = await self._stream_completions_create(\\n      messages, cast(MistralModelSettings, model_settings or {}), model_request_parameters\\n    )\\n    async with response:\\n      yield await self._process_streamed_response(model_request_parameters.result_tools, response)\\n  @property\\n  defmodel_name(self) -> MistralModelName:\\n\"\"\"The model name.\"\"\"\\n    return self._model_name\\n  @property\\n  defsystem(self) -> str | None:\\n\"\"\"The system / model provider.\"\"\"\\n    return self._system\\n  async def_completions_create(\\n    self,\\n    messages: list[ModelMessage],\\n    model_settings: MistralModelSettings,\\n    model_request_parameters: ModelRequestParameters,\\n  ) -> MistralChatCompletionResponse:\\n\"\"\"Make a non-streaming request to the model.\"\"\"\\n    response = await self.client.chat.complete_async(\\n      model=str(self._model_name),\\n      messages=list(chain(*(self._map_message(m) for m in messages))),\\n      n=1,\\n      tools=self._map_function_and_result_tools_definition(model_request_parameters) or UNSET,\\n      tool_choice=self._get_tool_choice(model_request_parameters),\\n      stream=False,\\n      max_tokens=model_settings.get(\\'max_tokens\\', UNSET),\\n      temperature=model_settings.get(\\'temperature\\', UNSET),\\n      top_p=model_settings.get(\\'top_p\\', 1),\\n      timeout_ms=self._get_timeout_ms(model_settings.get(\\'timeout\\')),\\n      random_seed=model_settings.get(\\'seed\\', UNSET),\\n    )\\n    assert response, \\'A unexpected empty response'),\n",
       " Document(metadata={'chunk_id': 112, 'type': 'text', 'num_code_blocks': 0, 'url': 'https://pydantic.com'}, page_content='from Mistral.\\'\\n    return response\\n  async def_stream_completions_create(\\n    self,\\n    messages: list[ModelMessage],\\n    model_settings: MistralModelSettings,\\n    model_request_parameters: ModelRequestParameters,\\n  ) -> MistralEventStreamAsync[MistralCompletionEvent]:\\n\"\"\"Create a streaming completion request to the Mistral model.\"\"\"\\n    response: MistralEventStreamAsync[MistralCompletionEvent] | None\\n    mistral_messages = list(chain(*(self._map_message(m) for m in messages)))\\n    if (\\n      model_request_parameters.result_tools\\n      and model_request_parameters.function_tools\\n      or model_request_parameters.function_tools\\n    ):\\n      # Function Calling\\n      response = await self.client.chat.stream_async(\\n        model=str(self._model_name),\\n        messages=mistral_messages,\\n        n=1,\\n        tools=self._map_function_and_result_tools_definition(model_request_parameters) or UNSET,\\n        tool_choice=self._get_tool_choice(model_request_parameters),\\n        temperature=model_settings.get(\\'temperature\\', UNSET),\\n        top_p=model_settings.get(\\'top_p\\', 1),\\n        max_tokens=model_settings.get(\\'max_tokens\\', UNSET),\\n        timeout_ms=self._get_timeout_ms(model_settings.get(\\'timeout\\')),\\n        presence_penalty=model_settings.get(\\'presence_penalty\\'),\\n        frequency_penalty=model_settings.get(\\'frequency_penalty\\'),\\n      )\\n    elif model_request_parameters.result_tools:\\n      # Json Mode\\n      parameters_json_schemas = [tool.parameters_json_schema for tool in model_request_parameters.result_tools]\\n      user_output_format_message = self._generate_user_output_format(parameters_json_schemas)\\n      mistral_messages.append(user_output_format_message)\\n      response = await self.client.chat.stream_async(\\n        model=str(self._model_name),\\n        messages=mistral_messages,\\n        response_format={\\'type\\': \\'json_object\\'},\\n        stream=True,\\n      )\\n    else:\\n      # Stream Mode\\n      response = await self.client.chat.stream_async(\\n        model=str(self._model_'),\n",
       " Document(metadata={'chunk_id': 113, 'type': 'text', 'num_code_blocks': 0, 'url': 'https://pydantic.com'}, page_content='name),\\n        messages=mistral_messages,\\n        stream=True,\\n      )\\n    assert response, \\'A unexpected empty response from Mistral.\\'\\n    return response\\n  def_get_tool_choice(self, model_request_parameters: ModelRequestParameters) -> MistralToolChoiceEnum | None:\\n\"\"\"Get tool choice for the model.\\n    - \"auto\": Default mode. Model decides if it uses the tool or not.\\n    - \"any\": Select any tool.\\n    - \"none\": Prevents tool use.\\n    - \"required\": Forces tool use.\\n    \"\"\"\\n    if not model_request_parameters.function_tools and not model_request_parameters.result_tools:\\n      return None\\n    elif not model_request_parameters.allow_text_result:\\n      return \\'required\\'\\n    else:\\n      return \\'auto\\'\\n  def_map_function_and_result_tools_definition(\\n    self, model_request_parameters: ModelRequestParameters\\n  ) -> list[MistralTool] | None:\\n\"\"\"Map function and result tools to MistralTool format.\\n    Returns None if both function_tools and result_tools are empty.\\n    \"\"\"\\n    all_tools: list[ToolDefinition] = (\\n      model_request_parameters.function_tools + model_request_parameters.result_tools\\n    )\\n    tools = [\\n      MistralTool(\\n        function=MistralFunction(name=r.name, parameters=r.parameters_json_schema, description=r.description)\\n      )\\n      for r in all_tools\\n    ]\\n    return tools if tools else None\\n  def_process_response(self, response: MistralChatCompletionResponse) -> ModelResponse:\\n\"\"\"Process a non-streamed response, and prepare a message to return.\"\"\"\\n    assert response.choices, \\'Unexpected empty response choice.\\'\\n    if response.created:\\n      timestamp = datetime.fromtimestamp(response.created, tz=timezone.utc)\\n    else:\\n      timestamp = _now_utc()\\n    choice = response.choices[0]\\n    content = choice.message.content\\n    tool_calls = choice.message.tool_calls\\n    parts: list[ModelResponsePart] = []\\n    if text := _map_content(content):\\n      parts.append(TextPart(content=text))\\n    if isinstance(tool_calls, list):\\n      for tool_call in tool_calls:'),\n",
       " Document(metadata={'chunk_id': 114, 'type': 'text', 'num_code_blocks': 0, 'url': 'https://pydantic.com'}, page_content='tool = self._map_mistral_to_pydantic_tool_call(tool_call=tool_call)\\n        parts.append(tool)\\n    return ModelResponse(parts, model_name=response.model, timestamp=timestamp)\\n  async def_process_streamed_response(\\n    self,\\n    result_tools: list[ToolDefinition],\\n    response: MistralEventStreamAsync[MistralCompletionEvent],\\n  ) -> StreamedResponse:\\n\"\"\"Process a streamed response, and prepare a streaming response to return.\"\"\"\\n    peekable_response = _utils.PeekableAsyncStream(response)\\n    first_chunk = await peekable_response.peek()\\n    if isinstance(first_chunk, _utils.Unset):\\n      raise UnexpectedModelBehavior(\\'Streamed response ended without content or tool calls\\')\\n    if first_chunk.data.created:\\n      timestamp = datetime.fromtimestamp(first_chunk.data.created, tz=timezone.utc)\\n    else:\\n      timestamp = datetime.now(tz=timezone.utc)\\n    return MistralStreamedResponse(\\n      _response=peekable_response,\\n      _model_name=self._model_name,\\n      _timestamp=timestamp,\\n      _result_tools={c.name: c for c in result_tools},\\n    )\\n  @staticmethod\\n  def_map_mistral_to_pydantic_tool_call(tool_call: MistralToolCall) -> ToolCallPart:\\n\"\"\"Maps a MistralToolCall to a ToolCall.\"\"\"\\n    tool_call_id = tool_call.id or None\\n    func_call = tool_call.function\\n    return ToolCallPart(func_call.name, func_call.arguments, tool_call_id)\\n  @staticmethod\\n  def_map_pydantic_to_mistral_tool_call(t: ToolCallPart) -> MistralToolCall:\\n\"\"\"Maps a pydantic-ai ToolCall to a MistralToolCall.\"\"\"\\n    return MistralToolCall(\\n      id=t.tool_call_id,\\n      type=\\'function\\',\\n      function=MistralFunctionCall(name=t.tool_name, arguments=t.args),\\n    )\\n  def_generate_user_output_format(self, schemas: list[dict[str, Any]]) -> MistralUserMessage:\\n\"\"\"Get a message with an example of the expected output format.\"\"\"\\n    examples: list[dict[str, Any]] = []\\n    for schema in schemas:\\n      typed_dict_definition: dict[str, Any] = {}\\n      for key, value in schema.get(\\'properties\\', {}).items():'),\n",
       " Document(metadata={'chunk_id': 115, 'type': 'text', 'num_code_blocks': 0, 'url': 'https://pydantic.com'}, page_content='typed_dict_definition[key] = self._get_python_type(value)\\n      examples.append(typed_dict_definition)\\n    example_schema = examples[0] if len(examples) == 1 else examples\\n    return MistralUserMessage(content=self.json_mode_schema_prompt.format(schema=example_schema))\\n  @classmethod\\n  def_get_python_type(cls, value: dict[str, Any]) -> str:\\n\"\"\"Return a string representation of the Python type for a single JSON schema property.\\n    This function handles recursion for nested arrays/objects and `anyOf`.\\n    \"\"\"\\n    # 1) Handle anyOf first, because it\\'s a different schema structure\\n    if any_of := value.get(\\'anyOf\\'):\\n      # Simplistic approach: pick the first option in anyOf\\n      # (In reality, you\\'d possibly want to merge or union types)\\n      return f\\'Optional[{cls._get_python_type(any_of[0])}]\\'\\n    # 2) If we have a top-level \"type\" field\\n    value_type = value.get(\\'type\\')\\n    if not value_type:\\n      # No explicit type; fallback\\n      return \\'Any\\'\\n    # 3) Direct simple type mapping (string, integer, float, bool, None)\\n    if value_type in SIMPLE_JSON_TYPE_MAPPING and value_type != \\'array\\' and value_type != \\'object\\':\\n      return SIMPLE_JSON_TYPE_MAPPING[value_type]\\n    # 4) Array: Recursively get the item type\\n    if value_type == \\'array\\':\\n      items = value.get(\\'items\\', {})\\n      return f\\'list[{cls._get_python_type(items)}]\\'\\n    # 5) Object: Check for additionalProperties\\n    if value_type == \\'object\\':\\n      additional_properties = value.get(\\'additionalProperties\\', {})\\n      additional_properties_type = additional_properties.get(\\'type\\')\\n      if (\\n        additional_properties_type in SIMPLE_JSON_TYPE_MAPPING\\n        and additional_properties_type != \\'array\\'\\n        and additional_properties_type != \\'object\\'\\n      ):\\n        # dict[str, bool/int/float/etc...]\\n        return f\\'dict[str, {SIMPLE_JSON_TYPE_MAPPING[additional_properties_type]}]\\'\\n      elif additional_properties_type == \\'array\\':\\n        array_items = additional_properties.get(\\'items\\', {})'),\n",
       " Document(metadata={'chunk_id': 116, 'type': 'text', 'num_code_blocks': 0, 'url': 'https://pydantic.com'}, page_content='return f\\'dict[str, list[{cls._get_python_type(array_items)}]]\\'\\n      elif additional_properties_type == \\'object\\':\\n        # nested dictionary of unknown shape\\n        return \\'dict[str, dict[str, Any]]\\'\\n      else:\\n        # If no additionalProperties type or something else, default to a generic dict\\n        return \\'dict[str, Any]\\'\\n    # 6) Fallback\\n    return \\'Any\\'\\n  @staticmethod\\n  def_get_timeout_ms(timeout: Timeout | float | None) -> int | None:\\n\"\"\"Convert a timeout to milliseconds.\"\"\"\\n    if timeout is None:\\n      return None\\n    if isinstance(timeout, float):\\n      return int(1000 * timeout)\\n    raise NotImplementedError(\\'Timeout object is not yet supported for MistralModel.\\')\\n  @classmethod\\n  def_map_user_message(cls, message: ModelRequest) -> Iterable[MistralMessages]:\\n    for part in message.parts:\\n      if isinstance(part, SystemPromptPart):\\n        yield MistralSystemMessage(content=part.content)\\n      elif isinstance(part, UserPromptPart):\\n        yield MistralUserMessage(content=part.content)\\n      elif isinstance(part, ToolReturnPart):\\n        yield MistralToolMessage(\\n          tool_call_id=part.tool_call_id,\\n          content=part.model_response_str(),\\n        )\\n      elif isinstance(part, RetryPromptPart):\\n        if part.tool_name is None:\\n          yield MistralUserMessage(content=part.model_response())\\n        else:\\n          yield MistralToolMessage(\\n            tool_call_id=part.tool_call_id,\\n            content=part.model_response(),\\n          )\\n      else:\\n        assert_never(part)\\n  @classmethod\\n  def_map_message(cls, message: ModelMessage) -> Iterable[MistralMessages]:\\n\"\"\"Just maps a `pydantic_ai.Message` to a `MistralMessage`.\"\"\"\\n    if isinstance(message, ModelRequest):\\n      yield from cls._map_user_message(message)\\n    elif isinstance(message, ModelResponse):\\n      content_chunks: list[MistralContentChunk] = []\\n      tool_calls: list[MistralToolCall] = []\\n      for part in message.parts:\\n        if isinstance(part, TextPart):'),\n",
       " Document(metadata={'chunk_id': 117, 'type': 'text', 'num_code_blocks': 6, 'url': 'https://pydantic.com'}, page_content='content_chunks.append(MistralTextChunk(text=part.content))\\n        elif isinstance(part, ToolCallPart):\\n          tool_calls.append(cls._map_pydantic_to_mistral_tool_call(part))\\n        else:\\n          assert_never(part)\\n      yield MistralAssistantMessage(content=content_chunks, tool_calls=tool_calls)\\n    else:\\n      assert_never(message)\\n\\n\\n__init__(\\n  model_name: MistralModelName,\\n  *,\\n  api_key: str | Callable[], str[ | None] | None = None,\\n  client: Mistral | None = None,\\n  http_client: AsyncClient | None = None,\\n  json_mode_schema_prompt: str = \"Answer in JSON Object, respect the format:\\\\n```\\\\n{schema}\\\\n```\\\\n\"\\n)\\n\\n\\\\n{schema}\\\\n```\\\\n\\'`  \\nSource code in `pydantic_ai_slim/pydantic_ai/models/mistral.py`\\n\\n| \\\\n{schema}\\\\n```\\\\n\"\"\",\\n):\\n\"\"\"Initialize a Mistral model.\\n  Args:\\n    model_name: The name of the model to use.\\n    api_key: The API key to use for authentication, if unset uses `MISTRAL_API_KEY` environment variable.\\n    client: An existing `Mistral` client to use, if provided, `api_key` and `http_client` must be `None`.\\n    http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.\\n    json_mode_schema_prompt: The prompt to show when the model expects a JSON object as input.\\n  \"\"\"\\n  self._model_name = model_name\\n  self.json_mode_schema_prompt = json_mode_schema_prompt\\n  if client is not None:\\n    assert http_client is None, \\'Cannot provide both `mistral_client` and `http_client`\\'\\n    assert api_key is None, \\'Cannot provide both `mistral_client` and `api_key`\\'\\n    self.client = client\\n  else:\\n    api_key = os.getenv(\\'MISTRAL_API_KEY\\') if api_key is None else api_key\\n    self.client = Mistral(api_key=api_key, async_client=http_client or cached_async_http_client())\\n\\n\\nrequest(\\n  messages: listModelMessage[],\\n  model_settings: ModelSettings | None,\\n  model_request_parameters: ModelRequestParameters,\\n) -> tupleModelResponse[, Usage]'),\n",
       " Document(metadata={'chunk_id': 118, 'type': 'text', 'num_code_blocks': 7, 'url': 'https://pydantic.com'}, page_content='async defrequest(\\n  self,\\n  messages: list[ModelMessage],\\n  model_settings: ModelSettings | None,\\n  model_request_parameters: ModelRequestParameters,\\n) -> tuple[ModelResponse, Usage]:\\n\"\"\"Make a non-streaming request to the model from Pydantic AI call.\"\"\"\\n  check_allow_model_requests()\\n  response = await self._completions_create(\\n    messages, cast(MistralModelSettings, model_settings or {}), model_request_parameters\\n  )\\n  return self._process_response(response), _map_usage(response)\\n\\n\\nrequest_stream(\\n  messages: listModelMessage[],\\n  model_settings: ModelSettings | None,\\n  model_request_parameters: ModelRequestParameters,\\n) -> AsyncIteratorStreamedResponse[]\\n\\n\\n\\n\\n@asynccontextmanager\\nasync defrequest_stream(\\n  self,\\n  messages: list[ModelMessage],\\n  model_settings: ModelSettings | None,\\n  model_request_parameters: ModelRequestParameters,\\n) -> AsyncIterator[StreamedResponse]:\\n\"\"\"Make a streaming request to the model from Pydantic AI call.\"\"\"\\n  check_allow_model_requests()\\n  response = await self._stream_completions_create(\\n    messages, cast(MistralModelSettings, model_settings or {}), model_request_parameters\\n  )\\n  async with response:\\n    yield await self._process_streamed_response(model_request_parameters.result_tools, response)\\n\\n\\nmodel_name: MistralModelName\\n\\n\\nsystem: str | None'),\n",
       " Document(metadata={'chunk_id': 119, 'type': 'text', 'num_code_blocks': 1, 'url': 'https://pydantic.com'}, page_content='@dataclass\\nclassMistralStreamedResponse(StreamedResponse):\\n\"\"\"Implementation of `StreamedResponse` for Mistral models.\"\"\"\\n  _model_name: MistralModelName\\n  _response: AsyncIterable[MistralCompletionEvent]\\n  _timestamp: datetime\\n  _result_tools: dict[str, ToolDefinition]\\n  _delta_content: str = field(default=\\'\\', init=False)\\n  async def_get_event_iterator(self) -> AsyncIterator[ModelResponseStreamEvent]:\\n    chunk: MistralCompletionEvent\\n    async for chunk in self._response:\\n      self._usage += _map_usage(chunk.data)\\n      try:\\n        choice = chunk.data.choices[0]\\n      except IndexError:\\n        continue\\n      # Handle the text part of the response\\n      content = choice.delta.content\\n      text = _map_content(content)\\n      if text:\\n        # Attempt to produce a result tool call from the received text\\n        if self._result_tools:\\n          self._delta_content += text\\n          maybe_tool_call_part = self._try_get_result_tool_from_text(self._delta_content, self._result_tools)\\n          if maybe_tool_call_part:\\n            yield self._parts_manager.handle_tool_call_part(\\n              vendor_part_id=\\'result\\',\\n              tool_name=maybe_tool_call_part.tool_name,\\n              args=maybe_tool_call_part.args_as_dict(),\\n              tool_call_id=maybe_tool_call_part.tool_call_id,\\n            )\\n        else:\\n          yield self._parts_manager.handle_text_delta(vendor_part_id=\\'content\\', content=text)\\n      # Handle the explicit tool calls\\n      for index, dtc in enumerate(choice.delta.tool_calls or []):\\n        # It seems that mistral just sends full tool calls, so we just use them directly, rather than building\\n        yield self._parts_manager.handle_tool_call_part(\\n          vendor_part_id=index, tool_name=dtc.function.name, args=dtc.function.arguments, tool_call_id=dtc.id\\n        )\\n  @property\\n  defmodel_name(self) -> MistralModelName:\\n\"\"\"Get the model name of the response.\"\"\"\\n    return self._model_name\\n  @property\\n  deftimestamp(self) -> date'),\n",
       " Document(metadata={'chunk_id': 120, 'type': 'text', 'num_code_blocks': 0, 'url': 'https://pydantic.com'}, page_content='time:\\n\"\"\"Get the timestamp of the response.\"\"\"\\n    return self._timestamp\\n  @staticmethod\\n  def_try_get_result_tool_from_text(text: str, result_tools: dict[str, ToolDefinition]) -> ToolCallPart | None:\\n    output_json: dict[str, Any] | None = pydantic_core.from_json(text, allow_partial=\\'trailing-strings\\')\\n    if output_json:\\n      for result_tool in result_tools.values():\\n        # NOTE: Additional verification to prevent JSON validation to crash in `_result.py`\\n        # Ensures required parameters in the JSON schema are respected, especially for stream-based return types.\\n        # Example with BaseModel and required fields.\\n        if not MistralStreamedResponse._validate_required_json_schema(\\n          output_json, result_tool.parameters_json_schema\\n        ):\\n          continue\\n        # The following part_id will be thrown away\\n        return ToolCallPart(tool_name=result_tool.name, args=output_json)\\n  @staticmethod\\n  def_validate_required_json_schema(json_dict: dict[str, Any], json_schema: dict[str, Any]) -> bool:\\n\"\"\"Validate that all required parameters in the JSON schema are present in the JSON dictionary.\"\"\"\\n    required_params = json_schema.get(\\'required\\', [])\\n    properties = json_schema.get(\\'properties\\', {})\\n    for param in required_params:\\n      if param not in json_dict:\\n        return False\\n      param_schema = properties.get(param, {})\\n      param_type = param_schema.get(\\'type\\')\\n      param_items_type = param_schema.get(\\'items\\', {}).get(\\'type\\')\\n      if param_type == \\'array\\' and param_items_type:\\n        if not isinstance(json_dict[param], list):\\n          return False\\n        for item in json_dict[param]:\\n          if not isinstance(item, VALID_JSON_TYPE_MAPPING[param_items_type]):\\n            return False\\n      elif param_type and not isinstance(json_dict[param], VALID_JSON_TYPE_MAPPING[param_type]):\\n        return False\\n      if isinstance(json_dict[param], dict) and \\'properties\\' in param_schema:\\n        nested_schema = param_schema\\n        if n'),\n",
       " Document(metadata={'chunk_id': 121, 'type': 'text', 'num_code_blocks': 2, 'url': 'https://pydantic.com'}, page_content='ot MistralStreamedResponse._validate_required_json_schema(json_dict[param], nested_schema):\\n          return False\\n    return True\\n\\n\\nmodel_name: MistralModelName\\n\\n\\ntimestamp: datetime\\n\\n```\\n\\nGet the timestamp of the response.\\n© Pydantic Services Inc. 2024 to present\\n\\n---'),\n",
       " Document(metadata={'chunk_id': 122, 'type': 'text', 'num_code_blocks': 7, 'url': 'https://pydantic.com'}, page_content='Setup  \\n    *  openai  \\n    *  OpenAIModelName  \\n    *  OpenAIModelSettings  \\n      *  openai_reasoning_effort  \\n    *  OpenAIModel  \\n      *  __init__  \\n      *  model_name  \\n      *  system  \\n    *  OpenAIStreamedResponse  \\n      *  model_name  \\n      *  timestamp  \\n\\n  1.  Introduction  \\n  2.  API Reference  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# `pydantic_ai.models.openai`\\n## Setup\\nFor details on how to set up authentication with this model, see model configuration for OpenAI.\\n###  OpenAIModelName `module-attribute`\\n\\n\\nPossible OpenAI model names.\\nSince OpenAI supports a variety of date-stamped models, we explicitly list the latest models but allow any name in the type hints. See the OpenAI docs for a full list.\\nUsing this more broad type for the model name instead of the ChatModel definition allows this model to be used more easily with other model types (ie, Ollama, Deepseek).\\n###  OpenAIModelSettings\\nBases: `ModelSettings`\\nSettings used for an OpenAI model request.\\nSource code in `pydantic_ai_slim/pydantic_ai/models/openai.py`\\n\\n| \\n\\n---|---  \\n####  openai_reasoning_effort `instance-attribute`\\n\\n\\nConstrains effort on reasoning for reasoning models. Currently supported values are `low`, `medium`, and `high`. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.\\n###  OpenAIModel `dataclass`\\nBases: `Model`\\nA model that uses the OpenAI API.\\nInternally, this uses the OpenAI Python client to interact with the API.\\nApart from `__init__`, all methods are private or match those of the base class.\\nSource code in `pydantic_ai_slim/pydantic_ai/models/openai.py`\\n\\n| \\n\\n---|---  \\n####  __init__'),\n",
       " Document(metadata={'chunk_id': 123, 'type': 'text', 'num_code_blocks': 8, 'url': 'https://pydantic.com'}, page_content=\"Initialize an OpenAI model.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`model_name` |  `OpenAIModelName` |  The name of the OpenAI model to use. List of model names available here (Unfortunately, despite being ask to do so, OpenAI do not provide `.inv` files for their API). |  _required_  \\n`base_url` |  `str | None` |  The base url for the OpenAI requests. If not provided, the `OPENAI_BASE_URL` environment variable will be used if available. Otherwise, defaults to OpenAI's base url. |  `None`  \\n`api_key` |  `str | None` |  The API key to use for authentication, if not provided, the `OPENAI_API_KEY` environment variable will be used if available. |  `None`  \\n`openai_client` |  `AsyncOpenAI | None` |  An existing `AsyncOpenAI` client to use. If provided, `base_url`, `api_key`, and `http_client` must be `None`. |  `None`  \\n`http_client` |  `AsyncClient | None` |  An existing `httpx.AsyncClient` to use for making HTTP requests. |  `None`  \\n`system_prompt_role` |  `OpenAISystemPromptRole | None` |  The role to use for the system prompt message. If not provided, defaults to `'system'`. In the future, this may be inferred from the model name. |  `None`  \\n`system` |  `str | None` |  The model provider used, defaults to `openai`. This is for observability purposes, you must customize the `base_url` and `api_key` to use a different provider. |  `'openai'`  \\nSource code in `pydantic_ai_slim/pydantic_ai/models/openai.py`\\n\\n| \\n\\n---|---  \\n####  model_name `property`\\n\\n\\nThe model name.\\n####  system `property`\\n\\n\\nThe system / model provider.\\n###  OpenAIStreamedResponse `dataclass`\\nBases: `StreamedResponse`\\nImplementation of `StreamedResponse` for OpenAI models.\\nSource code in `pydantic_ai_slim/pydantic_ai/models/openai.py`\\n\\n| \\n\\n---|---  \\n####  model_name `property`\\n\\n\\nGet the model name of the response.\\n####  timestamp `property`\"),\n",
       " Document(metadata={'chunk_id': 124, 'type': 'text', 'num_code_blocks': 0, 'url': 'https://pydantic.com'}, page_content='Get the timestamp of the response.\\n© Pydantic Services Inc. 2024 to present\\n\\n---'),\n",
       " Document(metadata={'chunk_id': 125, 'type': 'text', 'num_code_blocks': 8, 'url': 'https://pydantic.com'}, page_content=\"test  \\n  *  TestModel  \\n    *  call_tools  \\n    *  custom_result_text  \\n    *  custom_result_args  \\n    *  seed  \\n    *  last_model_request_parameters  \\n    *  model_name  \\n    *  system  \\n  *  TestStreamedResponse  \\n    *  model_name  \\n    *  timestamp  \\n\\n  1.  Introduction  \\n  2.  API Reference  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# `pydantic_ai.models.test`\\nUtility model for quickly testing apps built with PydanticAI.\\nHere's a minimal example:\\ntest_model_usage.py\\n\\nSee Unit testing with `TestModel` for detailed documentation.\\n###  TestModel `dataclass`\\nBases: `Model`\\nA model specifically for testing purposes.\\nThis will (by default) call all tools in the agent, then return a tool response if possible, otherwise a plain response.\\nHow useful this model is will vary significantly.\\nApart from `__init__` derived by the `dataclass` decorator, all methods are private or match those of the base class.\\nSource code in `pydantic_ai_slim/pydantic_ai/models/test.py`\\n\\n| \\n\\n---|---  \\n####  call_tools `class-attribute` `instance-attribute`\\n\\n\\nList of tools to call. If `'all'`, all tools will be called.\\n####  custom_result_text `class-attribute` `instance-attribute`\\n\\n\\nIf set, this text is returned as the final result.\\n####  custom_result_args `class-attribute` `instance-attribute`\\n\\n\\nIf set, these args will be passed to the result tool.\\n####  seed `class-attribute` `instance-attribute`\\n\\n\\nSeed for generating random data.\\n####  last_model_request_parameters `class-attribute` `instance-attribute`\"),\n",
       " Document(metadata={'chunk_id': 126, 'type': 'text', 'num_code_blocks': 6, 'url': 'https://pydantic.com'}, page_content='The last ModelRequestParameters passed to the model in a request.\\nThe ModelRequestParameters contains information about the function and result tools available during request handling.\\nThis is set when a request is made, so will reflect the function tools from the last step of the last run.\\n####  model_name `property`\\n\\n\\nThe model name.\\n####  system `property`\\n\\n\\nThe system / model provider.\\n###  TestStreamedResponse `dataclass`\\nBases: `StreamedResponse`\\nA structured response that streams test data.\\nSource code in `pydantic_ai_slim/pydantic_ai/models/test.py`\\n\\n| \\n\\n---|---  \\n####  model_name `property`\\n\\n\\nGet the model name of the response.\\n####  timestamp `property`\\n\\n\\nGet the timestamp of the response.\\n© Pydantic Services Inc. 2024 to present\\n\\n---'),\n",
       " Document(metadata={'chunk_id': 127, 'type': 'text', 'num_code_blocks': 3, 'url': 'https://pydantic.com'}, page_content='Setup  \\n  *  Example Usage  \\n    *  vertexai  \\n    *  VERTEX_AI_URL_TEMPLATE  \\n    *  VertexAIModel  \\n      *  __init__  \\n      *  ainit  \\n      *  model_name  \\n      *  system  \\n    *  BearerTokenAuth  \\n    *  VertexAiRegion  \\n\\n  1.  Introduction  \\n  2.  API Reference  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# `pydantic_ai.models.vertexai`\\nCustom interface to the `*-aiplatform.googleapis.com` API for Gemini models.\\nThis model inherits from `GeminiModel` with just the URL and auth method changed, it relies on the VertexAI `generateContent` and `streamGenerateContent` function endpoints having the same schemas as the equivalent Gemini endpoints.\\n## Setup\\nFor details on how to set up authentication with this model as well as a comparison with the `generativelanguage.googleapis.com` API used by `GeminiModel`, see model configuration for Gemini via VertexAI.\\n## Example Usage\\nWith the default google project already configured in your environment using \"application default credentials\":\\nvertex_example_env.py\\n\\nOr using a service account JSON file:\\nvertex_example_service_account.py\\n\\n###  VERTEX_AI_URL_TEMPLATE `module-attribute`\\n\\n\\nURL template for Vertex AI.\\nSee `generateContent` docs and `streamGenerateContent` docs for more information.\\nThe template is used thus:\\n  * `region` is substituted with the `region` argument, see available regions\\n  * `model_publisher` is substituted with the `model_publisher` argument\\n  * `model` is substituted with the `model_name` argument\\n  * `project_id` is substituted with the `project_id` from auth/credentials\\n  * `function` (`generateContent` or `streamGenerateContent`) is added to the end of the URL'),\n",
       " Document(metadata={'chunk_id': 128, 'type': 'text', 'num_code_blocks': 9, 'url': 'https://pydantic.com'}, page_content=\"###  VertexAIModel `dataclass`\\nBases: `GeminiModel`\\nA model that uses Gemini via the `*-aiplatform.googleapis.com` VertexAI API.\\nSource code in `pydantic_ai_slim/pydantic_ai/models/vertexai.py`\\n\\n| \\n\\n---|---  \\n####  __init__\\n\\n\\nInitialize a Vertex AI Gemini model.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`model_name` |  `GeminiModelName` |  The name of the model to use. I couldn't find a list of supported Google models, in VertexAI so for now this uses the same models as the Gemini model. |  _required_  \\n`service_account_file` |  `Path | str | None` |  Path to a service account file. If not provided, the default environment credentials will be used. |  `None`  \\n`project_id` |  `str | None` |  The project ID to use, if not provided it will be taken from the credentials. |  `None`  \\n`region` |  `VertexAiRegion` |  The region to make requests to. |  `'us-central1'`  \\n`model_publisher` |  `Literal['google']` |  The model publisher to use, I couldn't find a good list of available publishers, and from trial and error it seems non-google models don't work with the `generateContent` and `streamGenerateContent` functions, hence only `google` is currently supported. Please create an issue or PR if you know how to use other publishers. |  `'google'`  \\n`http_client` |  `AsyncClient | None` |  An existing `httpx.AsyncClient` to use for making HTTP requests. |  `None`  \\n`url_template` |  `str` |  URL template for Vertex AI, see `VERTEX_AI_URL_TEMPLATE` docs for more information. |  `VERTEX_AI_URL_TEMPLATE`  \\nSource code in `pydantic_ai_slim/pydantic_ai/models/vertexai.py`\\n\\n| \\n\\n---|---  \\n####  ainit `async`\\n\\n\\nInitialize the model, setting the URL and auth.\\nThis will raise an error if authentication fails.\\nSource code in `pydantic_ai_slim/pydantic_ai/models/vertexai.py`\\n\\n| \\n\\n---|---  \\n####  model_name `property`\"),\n",
       " Document(metadata={'chunk_id': 129, 'type': 'text', 'num_code_blocks': 4, 'url': 'https://pydantic.com'}, page_content='The model name.\\n####  system `property`\\n\\n\\nThe system / model provider.\\n###  BearerTokenAuth `dataclass`\\nAuthentication using a bearer token generated by google-auth.\\nSource code in `pydantic_ai_slim/pydantic_ai/models/vertexai.py`\\n\\n| \\n\\n---|---  \\n###  VertexAiRegion `module-attribute`\\n\\n\\nRegions available for Vertex AI.\\nMore details here.\\n© Pydantic Services Inc. 2024 to present\\n\\n---'),\n",
       " Document(metadata={'chunk_id': 130, 'type': 'text', 'num_code_blocks': 6, 'url': 'https://pydantic.com'}, page_content='exceptions  \\n  *  GraphSetupError  \\n    *  message  \\n  *  GraphRuntimeError  \\n    *  message  \\n\\n  1.  Introduction  \\n  2.  API Reference  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# `pydantic_graph.exceptions`\\n###  GraphSetupError\\nBases: `TypeError`\\nError caused by an incorrectly configured graph.\\nSource code in `pydantic_graph/pydantic_graph/exceptions.py`\\n\\n| \\n\\n---|---  \\n####  message `instance-attribute`\\n\\n\\nDescription of the mistake.\\n###  GraphRuntimeError\\nBases: `RuntimeError`\\nError caused by an issue during graph execution.\\nSource code in `pydantic_graph/pydantic_graph/exceptions.py`\\n\\n| \\n\\n---|---  \\n####  message `instance-attribute`\\n\\n\\nThe error message.\\n© Pydantic Services Inc. 2024 to present\\n\\n---'),\n",
       " Document(metadata={'chunk_id': 131, 'type': 'text', 'num_code_blocks': 1, 'url': 'https://pydantic.com'}, page_content=\"graph  \\n  *  Graph  \\n    *  __init__  \\n    *  run  \\n    *  iter  \\n    *  run_sync  \\n    *  next  \\n    *  dump_history  \\n    *  load_history  \\n    *  mermaid_code  \\n    *  mermaid_image  \\n    *  mermaid_save  \\n  *  GraphRun  \\n    *  __init__  \\n    *  next_node  \\n    *  result  \\n    *  next  \\n    *  __anext__  \\n  *  GraphRunResult  \\n\\n  1.  Introduction  \\n  2.  API Reference  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# `pydantic_graph`\\n###  Graph `dataclass`\\nBases: `GenericStateT[, DepsT, RunEndT]`\\nDefinition of a graph.\\nIn `pydantic-graph`, a graph is a collection of nodes that can be run in sequence. The nodes define their outgoing edges — e.g. which nodes may be run next, and thereby the structure of the graph.\\nHere's a very simple example of a graph which increments a number by 1, but makes sure the number is never 42 at the end.\\nnever_42.py\"),\n",
       " Document(metadata={'chunk_id': 132, 'type': 'text', 'num_code_blocks': 5, 'url': 'https://pydantic.com'}, page_content='_(This example is complete, it can be run \"as is\")_\\nSee `run` For an example of running graph, and `mermaid_code` for an example of generating a mermaid diagram from the graph.\\nSource code in `pydantic_graph/pydantic_graph/graph.py`\\n\\n| py {title=\"never_42.py\" noqa=\"I001\" py=\"3.10\"}\\n  from __future__ import annotations\\n  from dataclasses import dataclass\\n  from pydantic_graph import BaseNode, End, Graph, GraphRunContext\\n  @dataclass\\n  class MyState:\\n    number: int\\n  @dataclass\\n  class Increment(BaseNode[MyState]):\\n    async def run(self, ctx: GraphRunContext) -> Check42:\\n      ctx.state.number += 1\\n      return Check42()\\n  @dataclass\\n  class Check42(BaseNode[MyState, None, int]):\\n    async def run(self, ctx: GraphRunContext) -> Increment | End[int]:\\n      if ctx.state.number == 42:\\n        return Increment()\\n      else:\\n        return End(ctx.state.number)\\n  never_42_graph = Graph(nodes=(Increment, Check42))\\n  py {title=\"run_never_42.py\" noqa=\"I001\" py=\"3.10\"}\\n    from never_42 import Increment, MyState, never_42_graph\\n    async def main():\\n      state = MyState(1)\\n      graph_run_result = await never_42_graph.run(Increment(), state=state)\\n      print(state)\\n      #> MyState(number=2)\\n      print(len(graph_run_result.history))\\n      #> 3\\n      state = MyState(41)\\n      graph_run_result = await never_42_graph.run(Increment(), state=state)\\n      print(state)\\n      #> MyState(number=43)\\n      print(len(graph_run_result.history))\\n      #> 5\\npy {title=\"mermaid_never_42.py\" py=\"3.10\"}\\n    from never_42 import Increment, never_42_graph\\n    print(never_42_graph.mermaid_code(start_node=Increment))\\n    \\'\\'\\'\\n    ---\\n    title: never_42_graph\\n    ---\\n    stateDiagram-v2\\n     [*] --> Increment\\n     Increment --> Check42\\n     Check42 --> Increment\\n     Check42 --> [*]\\n    \\'\\'\\'\\nmermaid\\n    ---\\n    title: never_42_graph\\n    ---\\n    stateDiagram-v2\\n     [*] --> Increment\\n     Increment --> Check42\\n     Check42 --> Increment'),\n",
       " Document(metadata={'chunk_id': 133, 'type': 'text', 'num_code_blocks': 5, 'url': 'https://pydantic.com'}, page_content=\"Check42 --> [*]\\n\\n\\n---|---  \\n####  __init__\\n\\n\\nCreate a graph from a sequence of nodes.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`nodes` |  `Sequencetype[BaseNode[StateT[, DepsT, RunEndT]]]` |  The nodes which make up the graph, nodes need to be unique and all be generic in the same state type. |  _required_  \\n`name` |  `str | None` |  Optional name for the graph, if not provided the name will be inferred from the calling frame on the first call to a graph method. |  `None`  \\n`state_type` |  `typeStateT[] | Unset` |  The type of the state for the graph, this can generally be inferred from `nodes`. |  `UNSET`  \\n`run_end_type` |  `typeRunEndT[] | Unset` |  The type of the result of running the graph, this can generally be inferred from `nodes`. |  `UNSET`  \\n`snapshot_state` |  `Callable[StateT[], StateT]` |  A function to snapshot the state of the graph, this is used in `NodeStep` and `EndStep` to record the state before each step. |  `deep_copy_state`  \\n`auto_instrument` |  `bool` |  Whether to create a span for the graph run and the execution of each node's run method. |  `True`  \\nSource code in `pydantic_graph/pydantic_graph/graph.py`\\n\\n| \\n\\n---|---  \\n####  run `async`\\n\\n\\nRun the graph from a starting node until it ends.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`start_node` |  `BaseNodeStateT[, DepsT, T]` |  the first node to run, since the graph definition doesn't define the entry point in the graph, you need to provide the starting node. |  _required_  \\n`state` |  `StateT` |  The initial state of the graph. |  `None`  \\n`deps` |  `DepsT` |  The dependencies of the graph. |  `None`  \\n`infer_name` |  `bool` |  Whether to infer the graph name from the calling frame. |  `True`  \\n`span` |  `LogfireSpan | None` |  The span to use for the graph run. If not provided, a span will be created depending on the value of the `_auto_instrument` field.\"),\n",
       " Document(metadata={'chunk_id': 134, 'type': 'text', 'num_code_blocks': 5, 'url': 'https://pydantic.com'}, page_content='|  `None`  \\nReturns:\\nType | Description  \\n---|---  \\n`GraphRunResultStateT[, T]` |  A `GraphRunResult` containing information about the run, including its final result.  \\nHere\\'s an example of running the graph from above:\\nrun_never_42.py\\n\\nSource code in `pydantic_graph/pydantic_graph/graph.py`\\n\\n| py {title=\"run_never_42.py\" noqa=\"I001\" py=\"3.10\"}\\n  from never_42 import Increment, MyState, never_42_graph\\n  async def main():\\n    state = MyState(1)\\n    graph_run_result = await never_42_graph.run(Increment(), state=state)\\n    print(state)\\n    #> MyState(number=2)\\n    print(len(graph_run_result.history))\\n    #> 3\\n    state = MyState(41)\\n    graph_run_result = await never_42_graph.run(Increment(), state=state)\\n    print(state)\\n    #> MyState(number=43)\\n    print(len(graph_run_result.history))\\n    #> 5\\n  \\n\\n---|---  \\n####  iter\\n\\n\\nA contextmanager which can be used to iterate over the graph\\'s nodes as they are executed.\\nThis method returns a `GraphRun` object which can be used to async-iterate over the nodes of this `Graph` as they are executed. This is the API to use if you want to record or interact with the nodes as the graph execution unfolds.\\nThe `GraphRun` can also be used to manually drive the graph execution by calling `GraphRun.next`.\\nThe `GraphRun` provides access to the full run history, state, deps, and the final result of the run once it has completed.\\nFor more details, see the API documentation of `GraphRun`.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`start_node` |  `BaseNodeStateT[, DepsT, T]` |  the first node to run. Since the graph definition doesn\\'t define the entry point in the graph, you need to provide the starting node. |  _required_  \\n`state` |  `StateT` |  The initial state of the graph. |  `None`  \\n`deps` |  `DepsT` |  The dependencies of the graph. |  `None`  \\n`infer_name` |  `bool` |  Whether to infer the graph name from the calling frame.'),\n",
       " Document(metadata={'chunk_id': 135, 'type': 'text', 'num_code_blocks': 6, 'url': 'https://pydantic.com'}, page_content=\"|  `True`  \\n`span` |  `LogfireSpan | None` |  The span to use for the graph run. If not provided, a new span will be created. |  `None`  \\nYields:\\nType | Description  \\n---|---  \\n`GraphRunStateT[, DepsT, T]` |  A GraphRun that can be async iterated over to drive the graph to completion.  \\nSource code in `pydantic_graph/pydantic_graph/graph.py`\\n\\n| \\n\\n---|---  \\n####  run_sync\\n\\n\\nSynchronously run the graph.\\nThis is a convenience method that wraps `self.run` with `loop.run_until_complete(...)`. You therefore can't use this method inside async code or if there's an active event loop.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`start_node` |  `BaseNodeStateT[, DepsT, T]` |  the first node to run, since the graph definition doesn't define the entry point in the graph, you need to provide the starting node. |  _required_  \\n`state` |  `StateT` |  The initial state of the graph. |  `None`  \\n`deps` |  `DepsT` |  The dependencies of the graph. |  `None`  \\n`infer_name` |  `bool` |  Whether to infer the graph name from the calling frame. |  `True`  \\nReturns:\\nType | Description  \\n---|---  \\n`GraphRunResultStateT[, T]` |  The result type from ending the run and the history of the run.  \\nSource code in `pydantic_graph/pydantic_graph/graph.py`\\n\\n| \\n\\n---|---  \\n####  next `async`\\n\\n\\nRun a node in the graph and return the next node to run.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`node` |  `BaseNodeStateT[, DepsT, T]` |  The node to run. |  _required_  \\n`history` |  `listHistoryStep[StateT[, T]]` |  The history of the graph run so far. NOTE: this will be mutated to add the new step. |  _required_  \\n`state` |  `StateT` |  The current state of the graph. |  `None`  \\n`deps` |  `DepsT` |  The dependencies of the graph. |  `None`  \\n`infer_name` |  `bool` |  Whether to infer the graph name from the calling frame.\"),\n",
       " Document(metadata={'chunk_id': 136, 'type': 'text', 'num_code_blocks': 9, 'url': 'https://pydantic.com'}, page_content='|  `True`  \\nReturns:\\nType | Description  \\n---|---  \\n`BaseNodeStateT[, DepsT, Any] | EndT]` |  The next node to run or `End`[ if the graph has finished.  \\nSource code in `pydantic_graph/pydantic_graph/graph.py`\\n\\n| \\n\\n---|---  \\n####  dump_history\\n\\n\\nDump the history of a graph run as JSON.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`history` |  `listHistoryStep[StateT[, T]]` |  The history of the graph run. |  _required_  \\n`indent` |  `int | None` |  The number of spaces to indent the JSON. |  `None`  \\nReturns:\\nType | Description  \\n---|---  \\n`bytes` |  The JSON representation of the history.  \\nSource code in `pydantic_graph/pydantic_graph/graph.py`\\n\\n| \\n\\n---|---  \\n####  load_history\\n\\n\\nLoad the history of a graph run from JSON.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`json_bytes` |  `str | bytes | bytearray` |  The JSON representation of the history. |  _required_  \\nReturns:\\nType | Description  \\n---|---  \\n`listHistoryStep[StateT[, RunEndT]]` |  The history of the graph run.  \\nSource code in `pydantic_graph/pydantic_graph/graph.py`\\n\\n| \\n\\n---|---  \\n####  mermaid_code\\n\\n\\nGenerate a diagram representing the graph as mermaid diagram.\\nThis method calls `pydantic_graph.mermaid.generate_code`.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`start_node` |  `SequenceNodeIdent[] | NodeIdent | None` |  The node or nodes which can start the graph. |  `None`  \\n`title` |  `str | None | Literal[False]` |  The title of the diagram, use `False` to not include a title. |  `None`  \\n`edge_labels` |  `bool` |  Whether to include edge labels. |  `True`  \\n`notes` |  `bool` |  Whether to include notes on each node. |  `True`  \\n`highlighted_nodes` |  `SequenceNodeIdent[] | NodeIdent | None` |  Optional node or nodes to highlight. |  `None`  \\n`highlight_css` |  `str` |  The CSS to use for highlighting nodes.'),\n",
       " Document(metadata={'chunk_id': 137, 'type': 'text', 'num_code_blocks': 10, 'url': 'https://pydantic.com'}, page_content='|  `DEFAULT_HIGHLIGHT_CSS`  \\n`infer_name` |  `bool` |  Whether to infer the graph name from the calling frame. |  `True`  \\n`direction` |  `StateDiagramDirection | None` |  The direction of flow. |  `None`  \\nReturns:\\nType | Description  \\n---|---  \\n`str` |  The mermaid code for the graph, which can then be rendered as a diagram.  \\nHere\\'s an example of generating a diagram for the graph from above:\\nmermaid_never_42.py\\n\\nThe rendered diagram will look like this:\\n\\nSource code in `pydantic_graph/pydantic_graph/graph.py`\\n\\n| py {title=\"mermaid_never_42.py\" py=\"3.10\"}\\n  from never_42 import Increment, never_42_graph\\n  print(never_42_graph.mermaid_code(start_node=Increment))\\n  \\'\\'\\'\\n  ---\\n  title: never_42_graph\\n  ---\\n  stateDiagram-v2\\n   [*] --> Increment\\n   Increment --> Check42\\n   Check42 --> Increment\\n   Check42 --> [*]\\n  \\'\\'\\'\\n  mermaid\\n  ---\\n  title: never_42_graph\\n  ---\\n  stateDiagram-v2\\n   [*] --> Increment\\n   Increment --> Check42\\n   Check42 --> Increment\\n   Check42 --> [*]\\n  \\n\\n---|---  \\n####  mermaid_image\\n\\n\\nGenerate a diagram representing the graph as an image.\\nThe format and diagram can be customized using `kwargs`, see `pydantic_graph.mermaid.MermaidConfig`.\\nUses external service\\nThis method makes a request to mermaid.ink to render the image, `mermaid.ink` is a free service not affiliated with Pydantic.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`infer_name` |  `bool` |  Whether to infer the graph name from the calling frame. |  `True`  \\n`**kwargs` |  `UnpackMermaidConfig[]` |  Additional arguments to pass to `mermaid.request_image`. |  `{}`  \\nReturns:\\nType | Description  \\n---|---  \\n`bytes` |  The image bytes.  \\nSource code in `pydantic_graph/pydantic_graph/graph.py`\\n\\n| \\n\\n---|---  \\n####  mermaid_save'),\n",
       " Document(metadata={'chunk_id': 138, 'type': 'text', 'num_code_blocks': 3, 'url': 'https://pydantic.com'}, page_content=\"Generate a diagram representing the graph and save it as an image.\\nThe format and diagram can be customized using `kwargs`, see `pydantic_graph.mermaid.MermaidConfig`.\\nUses external service\\nThis method makes a request to mermaid.ink to render the image, `mermaid.ink` is a free service not affiliated with Pydantic.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`path` |  `Path | str` |  The path to save the image to. |  _required_  \\n`infer_name` |  `bool` |  Whether to infer the graph name from the calling frame. |  `True`  \\n`**kwargs` |  `UnpackMermaidConfig[]` |  Additional arguments to pass to `mermaid.save_image`. |  `{}`  \\nSource code in `pydantic_graph/pydantic_graph/graph.py`\\n\\n| \\n\\n---|---  \\n###  GraphRun\\nBases: `GenericStateT[, DepsT, RunEndT]`\\nA stateful, async-iterable run of a `Graph`.\\nYou typically get a `GraphRun` instance from calling `with [my_graph.iter(...)][pydantic_graph.graph.Graph.iter] as graph_run:`. That gives you the ability to iterate through nodes as they run, either by `async for` iteration or by repeatedly calling `.next(...)`.\\nHere's an example of iterating over the graph from above: \\niter_never_42.py\"),\n",
       " Document(metadata={'chunk_id': 139, 'type': 'text', 'num_code_blocks': 3, 'url': 'https://pydantic.com'}, page_content='See the `GraphRun.next` documentation for an example of how to manually drive the graph run.\\nSource code in `pydantic_graph/pydantic_graph/graph.py`\\n\\n| py {title=\"iter_never_42.py\" noqa=\"I001\" py=\"3.10\"}\\n  from copy import deepcopy\\n  from never_42 import Increment, MyState, never_42_graph\\n  async def main():\\n    state = MyState(1)\\n    with never_42_graph.iter(Increment(), state=state) as graph_run:\\n      node_states = [(graph_run.next_node, deepcopy(graph_run.state))]\\n      async for node in graph_run:\\n        node_states.append((node, deepcopy(graph_run.state)))\\n      print(node_states)\\n      \\'\\'\\'\\n      [\\n        (Increment(), MyState(number=1)),\\n        (Check42(), MyState(number=2)),\\n        (End(data=2), MyState(number=2)),\\n      ]\\n      \\'\\'\\'\\n    state = MyState(41)\\n    with never_42_graph.iter(Increment(), state=state) as graph_run:\\n      node_states = [(graph_run.next_node, deepcopy(graph_run.state))]\\n      async for node in graph_run:\\n        node_states.append((node, deepcopy(graph_run.state)))\\n      print(node_states)\\n      \\'\\'\\'\\n      [\\n        (Increment(), MyState(number=41)),\\n        (Check42(), MyState(number=42)),\\n        (Increment(), MyState(number=42)),\\n        (Check42(), MyState(number=43)),\\n        (End(data=43), MyState(number=43)),\\n      ]\\n      \\'\\'\\'\\n  py {title=\"next_never_42.py\" noqa=\"I001\" py=\"3.10\"}\\n    from copy import deepcopy\\n    from pydantic_graph import End\\n    from never_42 import Increment, MyState, never_42_graph\\n    async def main():\\n      state = MyState(48)\\n      with never_42_graph.iter(Increment(), state=state) as graph_run:\\n        next_node = graph_run.next_node # start with the first node\\n        node_states = [(next_node, deepcopy(graph_run.state))]\\n        while not isinstance(next_node, End):\\n          if graph_run.state.number == 50:\\n            graph_run.state.number = 42\\n          next_node = await graph_run.next(next_node)\\n          node_states.append((next_node, deepcopy(graph_run.st'),\n",
       " Document(metadata={'chunk_id': 140, 'type': 'text', 'num_code_blocks': 7, 'url': 'https://pydantic.com'}, page_content=\"ate)))\\n        print(node_states)\\n        '''\\n        [\\n          (Increment(), MyState(number=48)),\\n          (Check42(), MyState(number=49)),\\n          (End(data=49), MyState(number=49)),\\n        ]\\n        '''\\n\\n\\n---|---  \\n####  __init__\\n\\n\\nCreate a new run for a given graph, starting at the specified node.\\nTypically, you'll use `Graph.iter` rather than calling this directly.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`graph` |  `GraphStateT[, DepsT, RunEndT]` |  The `Graph` to run. |  _required_  \\n`start_node` |  `BaseNodeStateT[, DepsT, RunEndT]` |  The node where execution will begin. |  _required_  \\n`history` |  `listHistoryStep[StateT[, RunEndT]]` |  A list of `HistoryStep` objects that describe each step of the run. Usually starts empty; can be populated if resuming. |  _required_  \\n`state` |  `StateT` |  A shared state object or primitive (like a counter, dataclass, etc.) that is available to all nodes via `ctx.state`. |  _required_  \\n`deps` |  `DepsT` |  Optional dependencies that each node can access via `ctx.deps`, e.g. database connections, configuration, or logging clients. |  _required_  \\n`auto_instrument` |  `bool` |  Whether to automatically create instrumentation spans during the run. |  _required_  \\n`span` |  `LogfireSpan | None` |  An optional existing Logfire span to nest node-level spans under (advanced usage). |  `None`  \\nSource code in `pydantic_graph/pydantic_graph/graph.py`\\n\\n| \\n\\n---|---  \\n####  next_node `property`\\n\\n\\nThe next node that will be run in the graph.\\nThis is the next node that will be used during async iteration, or if a node is not passed to `self.next(...)`.\\n####  result `property`\\n\\n\\nThe final result of the graph run if the run is completed, otherwise `None`.\\n####  next `async`\"),\n",
       " Document(metadata={'chunk_id': 141, 'type': 'text', 'num_code_blocks': 7, 'url': 'https://pydantic.com'}, page_content='Manually drive the graph run by passing in the node you want to run next.\\nThis lets you inspect or mutate the node before continuing execution, or skip certain nodes under dynamic conditions. The graph run should stop when you return an `End` node.\\nHere\\'s an example of using `next` to drive the graph from above: \\nnext_never_42.py\\n\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`node` |  `BaseNodeStateT[, DepsT, T] | None` |  The node to run next in the graph. If not specified, uses `self.next_node`, which is initialized to the `start_node` of the run and updated each time a new node is returned. |  `None`  \\nReturns:\\nType | Description  \\n---|---  \\n`BaseNodeStateT[, DepsT, T] | EndT]` |  The next node returned by the graph logic, or an `End`[ node if  \\n`BaseNodeStateT[, DepsT, T] | End[T]` |  the run has completed.  \\nSource code in `pydantic_graph/pydantic_graph/graph.py`\\n\\n| py {title=\"next_never_42.py\" noqa=\"I001\" py=\"3.10\"}\\n  from copy import deepcopy\\n  from pydantic_graph import End\\n  from never_42 import Increment, MyState, never_42_graph\\n  async def main():\\n    state = MyState(48)\\n    with never_42_graph.iter(Increment(), state=state) as graph_run:\\n      next_node = graph_run.next_node # start with the first node\\n      node_states = [(next_node, deepcopy(graph_run.state))]\\n      while not isinstance(next_node, End):\\n        if graph_run.state.number == 50:\\n          graph_run.state.number = 42\\n        next_node = await graph_run.next(next_node)\\n        node_states.append((next_node, deepcopy(graph_run.state)))\\n      print(node_states)\\n      \\'\\'\\'\\n      [\\n        (Increment(), MyState(number=48)),\\n        (Check42(), MyState(number=49)),\\n        (End(data=49), MyState(number=49)),\\n      ]\\n      \\'\\'\\'\\n  \\n\\n---|---  \\n####  __anext__ `async`\\n\\n\\nUse the last returned node as the input to `Graph.next`.\\nSource code in `pydantic_graph/pydantic_graph/graph.py`\\n\\n|'),\n",
       " Document(metadata={'chunk_id': 142, 'type': 'text', 'num_code_blocks': 2, 'url': 'https://pydantic.com'}, page_content='---|---  \\n###  GraphRunResult `dataclass`\\nBases: `GenericStateT[, RunEndT]`\\nThe final result of running a graph.\\nSource code in `pydantic_graph/pydantic_graph/graph.py`\\n\\n| \\n\\n---|---  \\n© Pydantic Services Inc. 2024 to present\\n\\n---'),\n",
       " Document(metadata={'chunk_id': 143, 'type': 'text', 'num_code_blocks': 3, 'url': 'https://pydantic.com'}, page_content=\"mermaid  \\n  *  DEFAULT_HIGHLIGHT_CSS  \\n  *  StateDiagramDirection  \\n  *  generate_code  \\n  *  request_image  \\n  *  save_image  \\n  *  MermaidConfig  \\n    *  start_node  \\n    *  highlighted_nodes  \\n    *  highlight_css  \\n    *  title  \\n    *  edge_labels  \\n    *  notes  \\n    *  image_type  \\n    *  pdf_fit  \\n    *  pdf_landscape  \\n    *  pdf_paper  \\n    *  background_color  \\n    *  theme  \\n    *  width  \\n    *  height  \\n    *  scale  \\n    *  httpx_client  \\n    *  direction  \\n  *  NodeIdent  \\n\\n  1.  Introduction  \\n  2.  API Reference  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# `pydantic_graph.mermaid`\\n###  DEFAULT_HIGHLIGHT_CSS `module-attribute`\\n\\n\\nThe default CSS to use for highlighting nodes.\\n###  StateDiagramDirection `module-attribute`\\n\\n\\nUsed to specify the direction of the state diagram generated by mermaid.\\n  * `'TB'`: Top to bottom, this is the default for mermaid charts.\\n  * `'LR'`: Left to right\\n  * `'RL'`: Right to left\\n  * `'BT'`: Bottom to top\\n\\n###  generate_code\\n\\n\\nGenerate Mermaid state diagram code for a graph.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`graph` |  `GraphAny[, Any, Any]` |  The graph to generate the image for. |  _required_  \\n`start_node` |  `SequenceNodeIdent[] | NodeIdent | None` |  Identifiers of nodes that start the graph. |  `None`  \\n`highlighted_nodes` |  `SequenceNodeIdent[] | NodeIdent | None` |  Identifiers of nodes to highlight. |  `None`  \\n`highlight_css` |  `str` |  CSS to use for highlighting nodes. |  `DEFAULT_HIGHLIGHT_CSS`  \\n`title` |  `str | None` |  The title of the diagram. |  `None`  \\n`edge_labels` |  `bool` |  Whether to include edge labels in the diagram. |  `True`  \\n`notes` |  `bool` |  Whether to include notes in the diagram. |  `True`  \\n`direction` |  `StateDiagramDirection | None` |  The direction of flow.\"),\n",
       " Document(metadata={'chunk_id': 144, 'type': 'text', 'num_code_blocks': 16, 'url': 'https://pydantic.com'}, page_content='|  _required_  \\nReturns:\\nType | Description  \\n---|---  \\n`str` |  The Mermaid code for the graph.  \\nSource code in `pydantic_graph/pydantic_graph/mermaid.py`\\n\\n| \\n\\n---|---  \\n###  request_image\\n\\n\\nGenerate an image of a Mermaid diagram using mermaid.ink.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`graph` |  `GraphAny[, Any, Any]` |  The graph to generate the image for. |  _required_  \\n`**kwargs` |  `UnpackMermaidConfig[]` |  Additional parameters to configure mermaid chart generation. |  `{}`  \\nReturns:\\nType | Description  \\n---|---  \\n`bytes` |  The image data.  \\nSource code in `pydantic_graph/pydantic_graph/mermaid.py`\\n\\n| \\n\\n---|---  \\n###  save_image\\n\\n\\nGenerate an image of a Mermaid diagram using mermaid.ink and save it to a local file.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`path` |  `Path | str` |  The path to save the image to. |  _required_  \\n`graph` |  `GraphAny[, Any, Any]` |  The graph to generate the image for. |  _required_  \\n`**kwargs` |  `UnpackMermaidConfig[]` |  Additional parameters to configure mermaid chart generation. |  `{}`  \\nSource code in `pydantic_graph/pydantic_graph/mermaid.py`\\n\\n| \\n\\n---|---  \\n###  MermaidConfig\\nBases: `TypedDict`\\nParameters to configure mermaid chart generation.\\nSource code in `pydantic_graph/pydantic_graph/mermaid.py`\\n\\n| \\n\\n---|---  \\n####  start_node `instance-attribute`\\n\\n\\nIdentifiers of nodes that start the graph.\\n####  highlighted_nodes `instance-attribute`\\n\\n\\nIdentifiers of nodes to highlight.\\n####  highlight_css `instance-attribute`\\n\\n\\nCSS to use for highlighting nodes.\\n####  title `instance-attribute`\\n\\n\\nThe title of the diagram.\\n####  edge_labels `instance-attribute`\\n\\n\\nWhether to include edge labels in the diagram.\\n####  notes `instance-attribute`'),\n",
       " Document(metadata={'chunk_id': 145, 'type': 'text', 'num_code_blocks': 12, 'url': 'https://pydantic.com'}, page_content=\"Whether to include notes on nodes in the diagram, defaults to true.\\n####  image_type `instance-attribute`\\n\\n\\nThe image type to generate. If unspecified, the default behavior is `'jpeg'`.\\n####  pdf_fit `instance-attribute`\\n\\n\\nWhen using image_type='pdf', whether to fit the diagram to the PDF page.\\n####  pdf_landscape `instance-attribute`\\n\\n\\nWhen using image_type='pdf', whether to use landscape orientation for the PDF.\\nThis has no effect if using `pdf_fit`.\\n####  pdf_paper `instance-attribute`\\n\\n\\nWhen using image_type='pdf', the paper size of the PDF.\\n####  background_color `instance-attribute`\\n\\n\\nThe background color of the diagram.\\nIf None, the default transparent background is used. The color value is interpreted as a hexadecimal color code by default (and should not have a leading '#'), but you can also use named colors by prefixing the value with `'!'`. For example, valid choices include `background_color='!white'` or `background_color='FF0000'`.\\n####  theme `instance-attribute`\\n\\n\\nThe theme of the diagram. Defaults to 'default'.\\n####  width `instance-attribute`\\n\\n\\nThe width of the diagram.\\n####  height `instance-attribute`\\n\\n\\nThe height of the diagram.\\n####  scale `instance-attribute`\\n\\n\\nThe scale of the diagram.\\nThe scale must be a number between 1 and 3, and you can only set a scale if one or both of width and height are set.\\n####  httpx_client `instance-attribute`\\n\\n\\nAn HTTPX client to use for requests, mostly for testing purposes.\\n####  direction `instance-attribute`\\n\\n\\nThe direction of the state diagram.\\n###  NodeIdent `module-attribute`\\n\\n\\nA type alias for a node identifier.\\nThis can be:\\n  * A node instance (instance of a subclass of `BaseNode`).\\n  * A node class (subclass of `BaseNode`).\\n  * A string representing the node ID.\\n\\n© Pydantic Services Inc. 2024 to present\\n\\n---\"),\n",
       " Document(metadata={'chunk_id': 146, 'type': 'text', 'num_code_blocks': 8, 'url': 'https://pydantic.com'}, page_content=\"nodes  \\n  *  GraphRunContext  \\n    *  state  \\n    *  deps  \\n  *  BaseNode  \\n    *  docstring_notes  \\n    *  run  \\n    *  get_id  \\n    *  get_note  \\n    *  get_node_def  \\n  *  End  \\n    *  data  \\n  *  Edge  \\n    *  label  \\n  *  DepsT  \\n  *  RunEndT  \\n  *  NodeRunEndT  \\n\\n  1.  Introduction  \\n  2.  API Reference  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# `pydantic_graph.nodes`\\n###  GraphRunContext `dataclass`\\nBases: `GenericStateT[, DepsT]`\\nContext for a graph.\\nSource code in `pydantic_graph/pydantic_graph/nodes.py`\\n\\n| \\n\\n---|---  \\n####  state `instance-attribute`\\n\\n\\nThe state of the graph.\\n####  deps `instance-attribute`\\n\\n\\nDependencies for the graph.\\n###  BaseNode\\nBases: `ABC`, `GenericStateT[, DepsT, NodeRunEndT]`\\nBase class for a node.\\nSource code in `pydantic_graph/pydantic_graph/nodes.py`\\n\\n| \\n\\n---|---  \\n####  docstring_notes `class-attribute`\\n\\n\\nSet to `True` to generate mermaid diagram notes from the class's docstring.\\nWhile this can add valuable information to the diagram, it can make diagrams harder to view, hence it is disabled by default. You can also customise notes overriding the `get_note` method.\\n####  run `abstractmethod` `async`\\n\\n\\nRun the node.\\nThis is an abstract method that must be implemented by subclasses.\\nReturn types used at runtime\\nThe return type of this method are read by `pydantic_graph` at runtime and used to define which nodes can be called next in the graph. This is displayed in mermaid diagrams and enforced when running the graph.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`ctx` |  `GraphRunContextStateT[, DepsT]` |  The graph context.\"),\n",
       " Document(metadata={'chunk_id': 147, 'type': 'text', 'num_code_blocks': 20, 'url': 'https://pydantic.com'}, page_content='|  _required_  \\nReturns:\\nType | Description  \\n---|---  \\n`BaseNodeStateT[, DepsT, Any] | EndNodeRunEndT[]` |  The next node to run or `End` to signal the end of the graph.  \\nSource code in `pydantic_graph/pydantic_graph/nodes.py`\\n\\n| \\n\\n---|---  \\n####  get_id `cached` `classmethod`\\n\\n\\nGet the ID of the node.\\nSource code in `pydantic_graph/pydantic_graph/nodes.py`\\n\\n| \\n\\n---|---  \\n####  get_note `classmethod`\\n\\n\\nGet a note about the node to render on mermaid charts.\\nBy default, this returns a note only if `docstring_notes` is `True`. You can override this method to customise the node notes.\\nSource code in `pydantic_graph/pydantic_graph/nodes.py`\\n\\n| \\n\\n---|---  \\n####  get_node_def `classmethod`\\n\\n\\nGet the node definition.\\nSource code in `pydantic_graph/pydantic_graph/nodes.py`\\n\\n| \\n\\n---|---  \\n###  End `dataclass`\\nBases: `GenericRunEndT[]`\\nType to return from a node to signal the end of the graph.\\nSource code in `pydantic_graph/pydantic_graph/nodes.py`\\n\\n| \\n\\n---|---  \\n####  data `instance-attribute`\\n\\n\\nData to return from the graph.\\n###  Edge `dataclass`\\nAnnotation to apply a label to an edge in a graph.\\nSource code in `pydantic_graph/pydantic_graph/nodes.py`\\n\\n| \\n\\n---|---  \\n####  label `instance-attribute`\\n\\n\\nLabel for the edge.\\n###  DepsT `module-attribute`\\n\\n\\nType variable for the dependencies of a graph and node.\\n###  RunEndT `module-attribute`\\n\\n\\nCovariant type variable for the return type of a graph `run`.\\n###  NodeRunEndT `module-attribute`\\n\\n\\nCovariant type variable for the return type of a node `run`.\\n© Pydantic Services Inc. 2024 to present\\n\\n---'),\n",
       " Document(metadata={'chunk_id': 148, 'type': 'text', 'num_code_blocks': 15, 'url': 'https://pydantic.com'}, page_content='state  \\n  *  StateT  \\n  *  deep_copy_state  \\n  *  NodeStep  \\n    *  state  \\n    *  node  \\n    *  start_ts  \\n    *  duration  \\n    *  kind  \\n    *  snapshot_state  \\n    *  data_snapshot  \\n  *  EndStep  \\n    *  result  \\n    *  ts  \\n    *  kind  \\n    *  data_snapshot  \\n  *  HistoryStep  \\n\\n  1.  Introduction  \\n  2.  API Reference  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# `pydantic_graph.state`\\n###  StateT `module-attribute`\\n\\n\\nType variable for the state in a graph.\\n###  deep_copy_state\\n\\n\\nDefault method for snapshotting the state in a graph run, uses `copy.deepcopy`.\\nSource code in `pydantic_graph/pydantic_graph/state.py`\\n\\n| \\n\\n---|---  \\n###  NodeStep `dataclass`\\nBases: `GenericStateT[, RunEndT]`\\nHistory step describing the execution of a node in a graph.\\nSource code in `pydantic_graph/pydantic_graph/state.py`\\n\\n| \\n\\n---|---  \\n####  state `instance-attribute`\\n\\n\\nThe state of the graph after the node has been run.\\n####  node `instance-attribute`\\n\\n\\nThe node that was run.\\n####  start_ts `class-attribute` `instance-attribute`\\n\\n\\nThe timestamp when the node started running.\\n####  duration `class-attribute` `instance-attribute`\\n\\n\\nThe duration of the node run in seconds.\\n####  kind `class-attribute` `instance-attribute`\\n\\n\\nThe kind of history step, can be used as a discriminator when deserializing history.\\n####  snapshot_state `class-attribute` `instance-attribute`\\n\\n\\nFunction to snapshot the state of the graph.\\n####  data_snapshot\\n\\n\\nReturns a deep copy of `self.node`.\\nUseful for summarizing history.\\nSource code in `pydantic_graph/pydantic_graph/state.py`\\n\\n|'),\n",
       " Document(metadata={'chunk_id': 149, 'type': 'text', 'num_code_blocks': 9, 'url': 'https://pydantic.com'}, page_content='---|---  \\n###  EndStep `dataclass`\\nBases: `GenericRunEndT[]`\\nHistory step describing the end of a graph run.\\nSource code in `pydantic_graph/pydantic_graph/state.py`\\n\\n| \\n\\n---|---  \\n####  result `instance-attribute`\\n\\n\\nThe result of the graph run.\\n####  ts `class-attribute` `instance-attribute`\\n\\n\\nThe timestamp when the graph run ended.\\n####  kind `class-attribute` `instance-attribute`\\n\\n\\nThe kind of history step, can be used as a discriminator when deserializing history.\\n####  data_snapshot\\n\\n\\nReturns a deep copy of `self.result`.\\nUseful for summarizing history.\\nSource code in `pydantic_graph/pydantic_graph/state.py`\\n\\n| \\n\\n---|---  \\n###  HistoryStep `module-attribute`\\n\\n\\nA step in the history of a graph run.\\n`Graph.run` returns a list of these steps describing the execution of the graph, together with the run return value.\\n© Pydantic Services Inc. 2024 to present\\n\\n---'),\n",
       " Document(metadata={'chunk_id': 150, 'type': 'text', 'num_code_blocks': 11, 'url': 'https://pydantic.com'}, page_content=\"Usage  \\n    *  Installing required dependencies  \\n    *  Setting model environment variables  \\n    *  Running Examples  \\n\\n  1.  Introduction  \\n  2.  Examples  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# Examples\\nExamples of how to use PydanticAI and what it can do.\\n## Usage\\nThese examples are distributed with `pydantic-ai` so you can run them either by cloning the pydantic-ai repo or by simply installing `pydantic-ai` from PyPI with `pip` or `uv`.\\n### Installing required dependencies\\nEither way you'll need to install extra dependencies to run some examples, you just need to install the `examples` optional dependency group.\\nIf you've installed `pydantic-ai` via pip/uv, you can install the extra dependencies with:\\npipuv\\n\\n\\n\\n\\nIf you clone the repo, you should instead use `uv sync --extra examples` to install extra dependencies.\\n### Setting model environment variables\\nThese examples will need you to set up authentication with one or more of the LLMs, see the model configuration docs for details on how to do this.\\nTL;DR: in most cases you'll need to set one of the following environment variables:\\nOpenAIGoogle Gemini\\n\\n\\n\\n\\n### Running Examples\\nTo run the examples (this will work whether you installed `pydantic_ai`, or cloned the repo), run:\\npipuv\\n\\n\\n\\n\\nFor examples, to run the very simple `pydantic_model` example:\\npipuv\\n\\n\\n\\n\\nIf you like one-liners and you're using uv, you can run a pydantic-ai example with zero setup:\\n\\n\\nYou'll probably want to edit examples in addition to just running them. You can copy the examples to a new directory with:\\npipuv\\n\\n\\n\\n\\n© Pydantic Services Inc. 2024 to present\\n\\n---\"),\n",
       " Document(metadata={'chunk_id': 151, 'type': 'text', 'num_code_blocks': 3, 'url': 'https://pydantic.com'}, page_content='Running the Example  \\n  *  Example Code  \\n\\n  1.  Introduction  \\n  2.  Examples  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# Bank support\\nSmall but complete example of using PydanticAI to build a support agent for a bank.\\nDemonstrates:\\n  * dynamic system prompt\\n  * structured `result_type`\\n  * tools\\n\\n## Running the Example\\nWith dependencies installed and environment variables set, run:\\npipuv\\n\\n\\n\\n\\n(or `PYDANTIC_AI_MODEL=gemini-1.5-flash ...`)\\n## Example Code\\nbank_support.py\\n\\n© Pydantic Services Inc. 2024 to present\\n\\n---'),\n",
       " Document(metadata={'chunk_id': 152, 'type': 'text', 'num_code_blocks': 5, 'url': 'https://pydantic.com'}, page_content='Running the Example  \\n  *  Example Code  \\n\\n  1.  Introduction  \\n  2.  Examples  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# Chat App with FastAPI\\nSimple chat app example build with FastAPI.\\nDemonstrates:\\n  * reusing chat history\\n  * serializing messages\\n  * streaming responses\\n\\nThis demonstrates storing chat history between requests and using it to give the model context for new responses.\\nMost of the complex logic here is between `chat_app.py` which streams the response to the browser, and `chat_app.ts` which renders messages in the browser.\\n## Running the Example\\nWith dependencies installed and environment variables set, run:\\npipuv\\n\\n\\n\\n\\nThen open the app at localhost:8000.\\n\\n## Example Code\\nPython code that runs the chat app:\\nchat_app.py\\n\\nSimple HTML page to render the app:\\nchat_app.html\\n\\nTypeScript to handle rendering the messages, to keep this simple (and at the risk of offending frontend developers) the typescript code is passed to the browser as plain text and transpiled in the browser.\\nchat_app.ts\\n\\n© Pydantic Services Inc. 2024 to present\\n\\n---'),\n",
       " Document(metadata={'chunk_id': 153, 'type': 'text', 'num_code_blocks': 4, 'url': 'https://pydantic.com'}, page_content='Running the Example  \\n  *  Example Code  \\n\\n  1.  Introduction  \\n  2.  Examples  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# Flight booking\\nExample of a multi-agent flow where one agent delegates work to another, then hands off control to a third agent.\\nDemonstrates:\\n  * agent delegation\\n  * programmatic agent hand-off\\n  * usage limits\\n\\nIn this scenario, a group of agents work together to find the best flight for a user.\\nThe control flow for this example can be summarised as follows:\\n\\n\\n## Running the Example\\nWith dependencies installed and environment variables set, run:\\npipuv\\n\\n\\n\\n\\n## Example Code\\nflight_booking.py\\n\\n© Pydantic Services Inc. 2024 to present\\n\\n---'),\n",
       " Document(metadata={'chunk_id': 154, 'type': 'text', 'num_code_blocks': 5, 'url': 'https://pydantic.com'}, page_content='Running the Example  \\n  *  Example Code  \\n\\n  1.  Introduction  \\n  2.  Examples  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# Pydantic Model\\nSimple example of using PydanticAI to construct a Pydantic model from a text input.\\nDemonstrates:\\n  * structured `result_type`\\n\\n## Running the Example\\nWith dependencies installed and environment variables set, run:\\npipuv\\n\\n\\n\\n\\nThis examples uses `openai:gpt-4o` by default, but it works well with other models, e.g. you can run it with Gemini using:\\npipuv\\n\\n\\n\\n\\n(or `PYDANTIC_AI_MODEL=gemini-1.5-flash ...`)\\n## Example Code\\npydantic_model.py\\n\\n© Pydantic Services Inc. 2024 to present\\n\\n---'),\n",
       " Document(metadata={'chunk_id': 155, 'type': 'text', 'num_code_blocks': 4, 'url': 'https://pydantic.com'}, page_content='Running the Example  \\n  *  Example Code  \\n\\n  1.  Introduction  \\n  2.  Examples  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# Question Graph\\nExample of a graph for asking and evaluating questions.\\nDemonstrates:\\n  * `pydantic_graph`\\n\\n## Running the Example\\nWith dependencies installed and environment variables set, run:\\npipuv\\n\\n\\n\\n\\n## Example Code\\nquestion_graph.py\\n\\nThe mermaid diagram generated in this example looks like this:\\n\\n\\n© Pydantic Services Inc. 2024 to present\\n\\n---'),\n",
       " Document(metadata={'chunk_id': 156, 'type': 'text', 'num_code_blocks': 6, 'url': 'https://pydantic.com'}, page_content=\"Example Code  \\n\\n  1.  Introduction  \\n  2.  Examples  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# RAG\\nRAG search example. This demo allows you to ask question of the logfire documentation.\\nDemonstrates:\\n  * tools\\n  * agent dependencies\\n  * RAG search\\n\\nThis is done by creating a database containing each section of the markdown documentation, then registering the search tool with the PydanticAI agent.\\nLogic for extracting sections from markdown files and a JSON file with that data is available in this gist.\\nPostgreSQL with pgvector is used as the search database, the easiest way to download and run pgvector is using Docker:\\n\\n\\nAs with the SQL gen example, we run postgres on port `54320` to avoid conflicts with any other postgres instances you may have running. We also mount the PostgreSQL `data` directory locally to persist the data if you need to stop and restart the container.\\nWith that running and dependencies installed and environment variables set, we can build the search database with (**WARNING** : this requires the `OPENAI_API_KEY` env variable and will calling the OpenAI embedding API around 300 times to generate embeddings for each section of the documentation):\\npipuv\\n\\n\\n\\n\\n(Note building the database doesn't use PydanticAI right now, instead it uses the OpenAI SDK directly.)\\nYou can then ask the agent a question with:\\npipuv\\n\\n\\n\\n\\n## Example Code\\nrag.py\\n\\n© Pydantic Services Inc. 2024 to present\\n\\n---\"),\n",
       " Document(metadata={'chunk_id': 157, 'type': 'text', 'num_code_blocks': 6, 'url': 'https://pydantic.com'}, page_content='Running the Example  \\n  *  Example Code  \\n\\n  1.  Introduction  \\n  2.  Examples  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# SQL Generation\\nExample demonstrating how to use PydanticAI to generate SQL queries based on user input.\\nDemonstrates:\\n  * dynamic system prompt\\n  * structured `result_type`\\n  * result validation\\n  * agent dependencies\\n\\n## Running the Example\\nThe resulting SQL is validated by running it as an `EXPLAIN` query on PostgreSQL. To run the example, you first need to run PostgreSQL, e.g. via Docker:\\n\\n\\n_(we run postgres on port`54320` to avoid conflicts with any other postgres instances you may have running)_\\nWith dependencies installed and environment variables set, run:\\npipuv\\n\\n\\n\\n\\nor to use a custom prompt:\\npipuv\\n\\n\\n\\n\\nThis model uses `gemini-1.5-flash` by default since Gemini is good at single shot queries of this kind.\\n## Example Code\\nsql_gen.py\\n\\n© Pydantic Services Inc. 2024 to present\\n\\n---'),\n",
       " Document(metadata={'chunk_id': 158, 'type': 'text', 'num_code_blocks': 3, 'url': 'https://pydantic.com'}, page_content=\"Running the Example  \\n  *  Example Code  \\n\\n  1.  Introduction  \\n  2.  Examples  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# Stream markdown\\nThis example shows how to stream markdown from an agent, using the `rich` library to highlight the output in the terminal.\\nIt'll run the example with both OpenAI and Google Gemini models if the required environment variables are set.\\nDemonstrates:\\n  * streaming text responses\\n\\n## Running the Example\\nWith dependencies installed and environment variables set, run:\\npipuv\\n\\n\\n\\n\\n## Example Code\\n\\n\\n© Pydantic Services Inc. 2024 to present\\n\\n---\"),\n",
       " Document(metadata={'chunk_id': 159, 'type': 'text', 'num_code_blocks': 3, 'url': 'https://pydantic.com'}, page_content='Running the Example  \\n  *  Example Code  \\n\\n  1.  Introduction  \\n  2.  Examples  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# Stream whales\\nInformation about whales — an example of streamed structured response validation.\\nDemonstrates:\\n  * streaming structured responses\\n\\nThis script streams structured responses from GPT-4 about whales, validates the data and displays it as a dynamic table using `rich` as the data is received.\\n## Running the Example\\nWith dependencies installed and environment variables set, run:\\npipuv\\n\\n\\n\\n\\nShould give an output like this:\\n## Example Code\\nstream_whales.py\\n\\n© Pydantic Services Inc. 2024 to present\\n\\n---'),\n",
       " Document(metadata={'chunk_id': 160, 'type': 'text', 'num_code_blocks': 5, 'url': 'https://pydantic.com'}, page_content='Running the Example  \\n  *  Example Code  \\n  *  Running the UI  \\n  *  UI Code  \\n\\n  1.  Introduction  \\n  2.  Examples  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# Weather agent\\nExample of PydanticAI with multiple tools which the LLM needs to call in turn to answer a question.\\nDemonstrates:\\n  * tools\\n  * agent dependencies\\n  * streaming text responses\\n  * Building a Gradio UI for the agent\\n\\nIn this case the idea is a \"weather\" agent — the user can ask for the weather in multiple locations, the agent will use the `get_lat_lng` tool to get the latitude and longitude of the locations, then use the `get_weather` tool to get the weather for those locations.\\n## Running the Example\\nTo run this example properly, you might want to add two extra API keys **(Note if either key is missing, the code will fall back to dummy data, so they\\'re not required)** :\\n  * A weather API key from tomorrow.io set via `WEATHER_API_KEY`\\n  * A geocoding API key from geocode.maps.co set via `GEO_API_KEY`\\n\\nWith dependencies installed and environment variables set, run:\\npipuv\\n\\n\\n\\n\\n## Example Code\\npydantic_ai_examples/weather_agent.py\\n\\n## Running the UI\\nYou can build multi-turn chat applications for your agent with Gradio, a framework for building AI web applications entirely in python. Gradio comes with built-in chat components and agent support so the entire UI will be implemented in a single python file!\\nHere\\'s what the UI looks like for the weather agent:\\nNote, to run the UI, you\\'ll need Python 3.10+.\\n\\n\\n## UI Code\\npydantic_ai_examples/weather_agent_gradio.py\\n\\n© Pydantic Services Inc. 2024 to present')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'type': 'code', 'parent_text_id': 1, 'url': 'https://pydantic.com'}, page_content='frompydantic_aiimport Agent\\nagent = Agent( \\nWe configure the agent to use Gemini 1.5\\'s Flash model, but you can also set the model when running the agent.\\n\\n  \\'google-gla:gemini-1.5-flash\\',\\n  system_prompt=\\'Be concise, reply with one sentence.\\', \\nRegister a static system prompt using a keyword argument to the agent.\\n\\n)\\nresult = agent.run_sync(\\'Where does \"hello world\" come from?\\') \\nRun the agent synchronously, conducting a conversation with the LLM.\\n\\nprint(result.data)\\n\"\"\"\\nThe first known use of \"hello, world\" was in a 1974 textbook about the C programming language.\\n\"\"\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 1, 'url': 'https://pydantic.com'}, page_content='fromdataclassesimport dataclass\\nfrompydanticimport BaseModel, Field\\nfrompydantic_aiimport Agent, RunContext\\nfrombank_databaseimport DatabaseConn\\n\\n@dataclass\\nclassSupportDependencies: \\nThe SupportDependencies dataclass is used to pass data, connections, and logic into the model that will be needed when running system prompt and tool functions. PydanticAI\\'s system of dependency injection provides a type-safe way to customise the behavior of your agents, and can be especially useful when running unit tests and evals.\\n\\n  customer_id: int\\n  db: DatabaseConn \\nThis is a simple sketch of a database connection, used to keep the example short and readable. In reality, you\\'d be connecting to an external database (e.g. PostgreSQL) to get information about customers.\\n\\nclassSupportResult(BaseModel): \\nThis Pydantic model is used to constrain the structured data returned by the agent. From this simple definition, Pydantic builds the JSON Schema that tells the LLM how to return the data, and performs validation to guarantee the data is correct at the end of the run.\\n\\n  support_advice: str = Field(description=\\'Advice returned to the customer\\')\\n  block_card: bool = Field(description=\"Whether to block the customer\\'s card\")\\n  risk: int = Field(description=\\'Risk level of query\\', ge=0, le=10)\\n\\nsupport_agent = Agent( \\nThis agent will act as first-tier support in a bank. Agents are generic in the type of dependencies they accept and the type of result they return. In this case, the support agent has type Agent[SupportDependencies, SupportResult].\\n\\n  \\'openai:gpt-4o\\', \\nHere we configure the agent to use OpenAI\\'s GPT-4o model, you can also set the model when running the agent.\\n\\n  deps_type=SupportDependencies,\\n  result_type=SupportResult, \\nThe response from the agent will, be guaranteed to be a SupportResult, if validation fails reflection will mean the agent is prompted to try again.\\n\\n  system_prompt=( \\nStatic system prompts can be registered with the system_prompt keyword argument to the agent.\\n\\n    \\'You are a support agent in our bank, give the \\'\\n    \\'customer support and judge the risk level of their query.\\'\\n  ),\\n)\\n\\n@support_agent.system_prompt \\nDynamic system prompts can be registered with the @agent.system_prompt decorator, and can make use of dependency injection. Dependencies are carried via the RunContext argument, which is parameterized with the deps_type from above. If the type annotation here is wrong, static type checkers will catch it.\\n\\nasync defadd_customer_name(ctx: RunContext[SupportDependencies]) -> str:\\n  customer_name = await ctx.deps.db.customer_name(id=ctx.deps.customer_id)\\n  return f\"The customer\\'s name is {customer_name!r}\"\\n\\n@support_agent.tool \\ntool let you register functions which the LLM may call while responding to a user. Again, dependencies are carried via RunContext, any other arguments become the tool schema passed to the LLM. Pydantic is used to validate these arguments, and errors are passed back to the LLM so it can retry.\\n\\nasync defcustomer_balance(\\n  ctx: RunContext[SupportDependencies], include_pending: bool\\n) -> float:\\n\"\"\"Returns the customer\\'s current account balance.\"\"\" \\nThe docstring of a tool is also passed to the LLM as the description of the tool. Parameter descriptions are extracted from the docstring and added to the parameter schema sent to the LLM.\\n\\n  return await ctx.deps.db.customer_balance(\\n    id=ctx.deps.customer_id,\\n    include_pending=include_pending,\\n  )\\n\\n... \\nIn a real use case, you\\'d add more tools and a longer system prompt to the agent to extend the context it\\'s equipped with and support it can provide.\\n\\nasync defmain():\\n  deps = SupportDependencies(customer_id=123, db=DatabaseConn())\\n  result = await support_agent.run(\\'What is my balance?\\', deps=deps) \\nRun the agent asynchronously, conducting a conversation with the LLM until a final response is reached. Even in this fairly simple case, the agent will exchange multiple messages with the LLM as tools are called to retrieve a result.\\n\\n  print(result.data) \\nThe result will be validated with Pydantic to guarantee it is a SupportResult, since the agent is generic, it\\'ll also be typed as a SupportResult to aid with static type checking.\\n\\n\"\"\"\\n  support_advice=\\'Hello John, your current account balance, including pending transactions, is $123.45.\\' block_card=False risk=1\\n  \"\"\"\\n  result = await support_agent.run(\\'I just lost my card!\\', deps=deps)\\n  print(result.data)\\n\"\"\"\\n  support_advice=\"I\\'m sorry to hear that, John. We are temporarily blocking your card to prevent unauthorized transactions.\" block_card=True risk=8\\n  \"\"\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 1, 'url': 'https://pydantic.com'}, page_content='...\\nfrombank_databaseimport DatabaseConn\\nimportlogfire\\nlogfire.configure() \\nConfigure logfire, this will fail if project is not set up.\\n\\nlogfire.instrument_asyncpg() \\nIn our demo, DatabaseConn uses asyncpg to connect to a PostgreSQL database, so logfire.instrument_asyncpg() is used to log the database queries.\\n\\n...'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 4, 'url': 'https://pydantic.com'}, page_content='frompydantic_aiimport Agent, RunContext\\nroulette_agent = Agent( \\n  \\'openai:gpt-4o\\',\\n  deps_type=int,\\n  result_type=bool,\\n  system_prompt=(\\n    \\'Use the `roulette_wheel` function to see if the \\'\\n    \\'customer has won based on the number they provide.\\'\\n  ),\\n)\\n\\n@roulette_agent.tool\\nasync defroulette_wheel(ctx: RunContextint], square: int) -> str: [\\n\"\"\"check if the square is a winner\"\"\"\\n  return \\'winner\\' if square == ctx.deps else \\'loser\\'\\n\\n# Run the agent\\nsuccess_number = 18 \\nresult = roulette_agent.run_sync(\\'Put my money on square eighteen\\', deps=success_number)\\nprint(result.data) \\n#> True\\nresult = roulette_agent.run_sync(\\'I bet five is the winner\\', deps=success_number)\\nprint(result.data)\\n#> False'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 4, 'url': 'https://pydantic.com'}, page_content=\"frompydantic_aiimport Agent\\nagent = Agent('openai:gpt-4o')\\nresult_sync = agent.run_sync('What is the capital of Italy?')\\nprint(result_sync.data)\\n#> Rome\\n\\nasync defmain():\\n  result = await agent.run('What is the capital of France?')\\n  print(result.data)\\n  #> Paris\\n  async with agent.run_stream('What is the capital of the UK?') as response:\\n    print(await response.get_data())\\n    #> London\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 5, 'url': 'https://pydantic.com'}, page_content='frompydantic_aiimport Agent\\nagent = Agent(\\'openai:gpt-4o\\')\\n\\nasync defmain():\\n  nodes = []\\n  # Begin an AgentRun, which is an async-iterable over the nodes of the agent\\'s graph\\n  with agent.iter(\\'What is the capital of France?\\') as agent_run:\\n    async for node in agent_run:\\n      # Each node represents a step in the agent\\'s execution\\n      nodes.append(node)\\n  print(nodes)\\n\"\"\"\\n  [\\n    ModelRequestNode(\\n      request=ModelRequest(\\n        parts=[\\n          UserPromptPart(\\n            content=\\'What is the capital of France?\\',\\n            timestamp=datetime.datetime(...),\\n            part_kind=\\'user-prompt\\',\\n          )\\n        ],\\n        kind=\\'request\\',\\n      )\\n    ),\\n    HandleResponseNode(\\n      model_response=ModelResponse(\\n        parts=[TextPart(content=\\'Paris\\', part_kind=\\'text\\')],\\n        model_name=\\'function:model_logic\\',\\n        timestamp=datetime.datetime(...),\\n        kind=\\'response\\',\\n      )\\n    ),\\n    End(data=FinalResult(data=\\'Paris\\', tool_name=None)),\\n  ]\\n  \"\"\"\\n  print(agent_run.result.data)\\n  #> Paris'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 5, 'url': 'https://pydantic.com'}, page_content='frompydantic_aiimport Agent\\nfrompydantic_graphimport End\\nagent = Agent(\\'openai:gpt-4o\\')\\n\\nasync defmain():\\n  with agent.iter(\\'What is the capital of France?\\') as agent_run:\\n    node = agent_run.next_node \\nWe start by grabbing the first node that will be run in the agent\\'s graph.\\n\\n    all_nodes = [node]\\n    # Drive the iteration manually:\\n    while not isinstance(node, End): \\nThe agent run is finished once an End node has been produced; instances of End cannot be passed to next.\\n\\n      node = await agent_run.next(node) \\nWhen you call await agent_run.next(node), it executes that node in the agent\\'s graph, updates the run\\'s history, and returns the  _next_ node to run.\\n\\n      all_nodes.append(node) \\nYou could also inspect or mutate the new node here as needed.\\n\\n    print(all_nodes)\\n\"\"\"\\n    [\\n      UserPromptNode(\\n        user_prompt=\\'What is the capital of France?\\',\\n        system_prompts=(),\\n        system_prompt_functions=[],\\n        system_prompt_dynamic_functions={},\\n      ),\\n      ModelRequestNode(\\n        request=ModelRequest(\\n          parts=[\\n            UserPromptPart(\\n              content=\\'What is the capital of France?\\',\\n              timestamp=datetime.datetime(...),\\n              part_kind=\\'user-prompt\\',\\n            )\\n          ],\\n          kind=\\'request\\',\\n        )\\n      ),\\n      HandleResponseNode(\\n        model_response=ModelResponse(\\n          parts=[TextPart(content=\\'Paris\\', part_kind=\\'text\\')],\\n          model_name=\\'function:model_logic\\',\\n          timestamp=datetime.datetime(...),\\n          kind=\\'response\\',\\n        )\\n      ),\\n      End(data=FinalResult(data=\\'Paris\\', tool_name=None)),\\n    ]\\n    \"\"\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 6, 'url': 'https://pydantic.com'}, page_content='frompydantic_aiimport Agent\\nfrompydantic_ai.exceptionsimport UsageLimitExceeded\\nfrompydantic_ai.usageimport UsageLimits\\nagent = Agent(\\'anthropic:claude-3-5-sonnet-latest\\')\\nresult_sync = agent.run_sync(\\n  \\'What is the capital of Italy? Answer with just the city.\\',\\n  usage_limits=UsageLimits(response_tokens_limit=10),\\n)\\nprint(result_sync.data)\\n#> Rome\\nprint(result_sync.usage())\\n\"\"\"\\nUsage(requests=1, request_tokens=62, response_tokens=1, total_tokens=63, details=None)\\n\"\"\"\\ntry:\\n  result_sync = agent.run_sync(\\n    \\'What is the capital of Italy? Answer with a paragraph.\\',\\n    usage_limits=UsageLimits(response_tokens_limit=10),\\n  )\\nexcept UsageLimitExceeded as e:\\n  print(e)\\n  #> Exceeded the response_tokens_limit of 10 (response_tokens=32)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 6, 'url': 'https://pydantic.com'}, page_content='fromtyping_extensionsimport TypedDict\\nfrompydantic_aiimport Agent, ModelRetry\\nfrompydantic_ai.exceptionsimport UsageLimitExceeded\\nfrompydantic_ai.usageimport UsageLimits\\n\\nclassNeverResultType(TypedDict):\\n\"\"\"\\n  Never ever coerce data to this type.\\n  \"\"\"\\n  never_use_this: str\\n\\nagent = Agent(\\n  \\'anthropic:claude-3-5-sonnet-latest\\',\\n  retries=3,\\n  result_type=NeverResultType,\\n  system_prompt=\\'Any time you get a response, call the `infinite_retry_tool` to produce another response.\\',\\n)\\n\\n@agent.tool_plain(retries=5) \\nThis tool has the ability to retry 5 times before erroring, simulating a tool that might get stuck in a loop.\\n\\ndefinfinite_retry_tool() -> int:\\n  raise ModelRetry(\\'Please try again.\\')\\n\\ntry:\\n  result_sync = agent.run_sync(\\n    \\'Begin infinite retry loop!\\', usage_limits=UsageLimits(request_limit=3) \\nThis run will error after 3 requests, preventing the infinite tool calling.\\n\\n  )\\nexcept UsageLimitExceeded as e:\\n  print(e)\\n  #> The next request would exceed the request_limit of 3'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 6, 'url': 'https://pydantic.com'}, page_content=\"frompydantic_aiimport Agent\\nagent = Agent('openai:gpt-4o')\\nresult_sync = agent.run_sync(\\n  'What is the capital of Italy?', model_settings={'temperature': 0.0}\\n)\\nprint(result_sync.data)\\n#> Rome\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 6, 'url': 'https://pydantic.com'}, page_content='frompydantic_aiimport Agent, UnexpectedModelBehavior\\nfrompydantic_ai.models.geminiimport GeminiModelSettings\\nagent = Agent(\\'google-gla:gemini-1.5-flash\\')\\ntry:\\n  result = agent.run_sync(\\n    \\'Write a list of 5 very rude things that I might say to the universe after stubbing my toe in the dark:\\',\\n    model_settings=GeminiModelSettings(\\n      temperature=0.0, # general model settings can also be specified\\n      gemini_safety_settings=[\\n        {\\n          \\'category\\': \\'HARM_CATEGORY_HARASSMENT\\',\\n          \\'threshold\\': \\'BLOCK_LOW_AND_ABOVE\\',\\n        },\\n        {\\n          \\'category\\': \\'HARM_CATEGORY_HATE_SPEECH\\',\\n          \\'threshold\\': \\'BLOCK_LOW_AND_ABOVE\\',\\n        },\\n      ],\\n    ),\\n  )\\nexcept UnexpectedModelBehavior as e:\\n  print(e) \\nThis error is raised because the safety thresholds were exceeded.\\nGenerally, result would contain a normal ModelResponse.\\n\\n\"\"\"\\n  Safety settings triggered, body:\\n  <safety settings details>\\n  \"\"\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 7, 'url': 'https://pydantic.com'}, page_content='frompydantic_aiimport Agent\\nagent = Agent(\\'openai:gpt-4o\\')\\n# First run\\nresult1 = agent.run_sync(\\'Who was Albert Einstein?\\')\\nprint(result1.data)\\n#> Albert Einstein was a German-born theoretical physicist.\\n# Second run, passing previous messages\\nresult2 = agent.run_sync(\\n  \\'What was his most famous equation?\\',\\n  message_history=result1.new_messages(), \\nContinue the conversation; without message_history the model would not know who \"his\" was referring to.\\n\\n)\\nprint(result2.data)\\n#> Albert Einstein\\'s most famous equation is (E = mc^2).'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 7, 'url': 'https://pydantic.com'}, page_content='fromdataclassesimport dataclass\\nfrompydantic_aiimport Agent, RunContext\\n\\n@dataclass\\nclassUser:\\n  name: str\\n\\nagent = Agent(\\n  \\'test\\',\\n  deps_type=User, \\nThe agent is defined as expecting an instance of User as deps.\\n\\n  result_type=bool,\\n)\\n\\n@agent.system_prompt\\ndefadd_user_name(ctx: RunContext[str]) -> str: \\nBut here add_user_name is defined as taking a str as the dependency, not a User.\\n\\n  return f\"The user\\'s name is {ctx.deps}.\"\\n\\ndeffoobar(x: bytes) -> None:\\n  pass\\n\\nresult = agent.run_sync(\\'Does their name start with \"A\"?\\', deps=User(\\'Anne\\'))\\nfoobar(result.data) \\nSince the agent is defined as returning a bool, this will raise a type error since foobar expects bytes.'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 7, 'url': 'https://pydantic.com'}, page_content='➤uvrunmypytype_mistakes.py\\ntype_mistakes.py:18:error:Argument1to\"system_prompt\"of\"Agent\"hasincompatibletype\"Callable[[RunContext[str]], str]\";expected\"Callable[[RunContext[User]], str]\"[arg-type]\\ntype_mistakes.py:28:error:Argument1to\"foobar\"hasincompatibletype\"bool\";expected\"bytes\"[arg-type]\\nFound2errorsin1file(checked1sourcefile)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 8, 'url': 'https://pydantic.com'}, page_content='fromdatetimeimport date\\nfrompydantic_aiimport Agent, RunContext\\nagent = Agent(\\n  \\'openai:gpt-4o\\',\\n  deps_type=str, \\nThe agent expects a string dependency.\\n\\n  system_prompt=\"Use the customer\\'s name while replying to them.\", \\nStatic system prompt defined at agent creation time.\\n\\n)\\n\\n@agent.system_prompt \\nDynamic system prompt defined via a decorator with RunContext, this is called just after run_sync, not when the agent is created, so can benefit from runtime information like the dependencies used on that run.\\n\\ndefadd_the_users_name(ctx: RunContext[str]) -> str:\\n  return f\"The user\\'s name is {ctx.deps}.\"\\n\\n@agent.system_prompt\\ndefadd_the_date() -> str: \\nAnother dynamic system prompt, system prompts don\\'t have to have the RunContext parameter.\\n\\n  return f\\'The date is {date.today()}.\\'\\n\\nresult = agent.run_sync(\\'What is the date?\\', deps=\\'Frank\\')\\nprint(result.data)\\n#> Hello Frank, the date today is 2032-01-02.'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 8, 'url': 'https://pydantic.com'}, page_content='frompydanticimport BaseModel\\nfrompydantic_aiimport Agent, RunContext, ModelRetry\\nfromfake_databaseimport DatabaseConn\\n\\nclassChatResult(BaseModel):\\n  user_id: int\\n  message: str\\n\\nagent = Agent(\\n  \\'openai:gpt-4o\\',\\n  deps_type=DatabaseConn,\\n  result_type=ChatResult,\\n)\\n\\n@agent.tool(retries=2)\\ndefget_user_by_name(ctx: RunContext[DatabaseConn], name: str) -> int:\\n\"\"\"Get a user\\'s ID from their full name.\"\"\"\\n  print(name)\\n  #> John\\n  #> John Doe\\n  user_id = ctx.deps.users.get(name=name)\\n  if user_id is None:\\n    raise ModelRetry(\\n      f\\'No user found with name {name!r}, remember to provide their full name\\'\\n    )\\n  return user_id\\n\\nresult = agent.run_sync(\\n  \\'Send a message to John Doe asking for coffee next week\\', deps=DatabaseConn()\\n)\\nprint(result.data)\\n\"\"\"\\nuser_id=123 message=\\'Hello John, would you be free for coffee sometime next week? Let me know what works for you!\\'\\n\"\"\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 8, 'url': 'https://pydantic.com'}, page_content='frompydantic_aiimport Agent, ModelRetry, UnexpectedModelBehavior, capture_run_messages\\nagent = Agent(\\'openai:gpt-4o\\')\\n\\n@agent.tool_plain\\ndefcalc_volume(size: int) -> int: \\nDefine a tool that will raise ModelRetry repeatedly in this case.\\n\\n  if size == 42:\\n    return size**3\\n  else:\\n    raise ModelRetry(\\'Please try again.\\')\\n\\nwith capture_run_messages() as messages: \\ncapture_run_messages is used to capture the messages exchanged during the run.\\n\\n  try:\\n    result = agent.run_sync(\\'Please get me the volume of a box with size 6.\\')\\n  except UnexpectedModelBehavior as e:\\n    print(\\'An error occurred:\\', e)\\n    #> An error occurred: Tool exceeded max retries count of 1\\n    print(\\'cause:\\', repr(e.__cause__))\\n    #> cause: ModelRetry(\\'Please try again.\\')\\n    print(\\'messages:\\', messages)\\n\"\"\"\\n    messages:\\n    [\\n      ModelRequest(\\n        parts=[\\n          UserPromptPart(\\n            content=\\'Please get me the volume of a box with size 6.\\',\\n            timestamp=datetime.datetime(...),\\n            part_kind=\\'user-prompt\\',\\n          )\\n        ],\\n        kind=\\'request\\',\\n      ),\\n      ModelResponse(\\n        parts=[\\n          ToolCallPart(\\n            tool_name=\\'calc_volume\\',\\n            args={\\'size\\': 6},\\n            tool_call_id=None,\\n            part_kind=\\'tool-call\\',\\n          )\\n        ],\\n        model_name=\\'function:model_logic\\',\\n        timestamp=datetime.datetime(...),\\n        kind=\\'response\\',\\n      ),\\n      ModelRequest(\\n        parts=[\\n          RetryPromptPart(\\n            content=\\'Please try again.\\',\\n            tool_name=\\'calc_volume\\',\\n            tool_call_id=None,\\n            timestamp=datetime.datetime(...),\\n            part_kind=\\'retry-prompt\\',\\n          )\\n        ],\\n        kind=\\'request\\',\\n      ),\\n      ModelResponse(\\n        parts=[\\n          ToolCallPart(\\n            tool_name=\\'calc_volume\\',\\n            args={\\'size\\': 6},\\n            tool_call_id=None,\\n            part_kind=\\'tool-call\\',\\n          )\\n        ],\\n        model_name=\\'function:model_logic\\',\\n        timestamp=datetime.datetime(...),\\n        kind=\\'response\\',\\n      ),\\n    ]\\n    \"\"\"\\n  else:\\n    print(result.data)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 9, 'url': 'https://pydantic.com'}, page_content='gitclonegit@github.com:<yourusername>/pydantic-ai.git\\ncdpydantic-ai'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 9, 'url': 'https://pydantic.com'}, page_content='pipxinstalluvpre-commit'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 9, 'url': 'https://pydantic.com'}, page_content='makeinstall'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 9, 'url': 'https://pydantic.com'}, page_content='makehelp'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 9, 'url': 'https://pydantic.com'}, page_content='make'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 9, 'url': 'https://pydantic.com'}, page_content='uvrunmkdocsserve'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 11, 'url': 'https://pydantic.com'}, page_content=\"fromdataclassesimport dataclass\\nimporthttpx\\nfrompydantic_aiimport Agent\\n\\n@dataclass\\nclassMyDeps: \\nDefine a dataclass to hold dependencies.\\n\\n  api_key: str\\n  http_client: httpx.AsyncClient\\n\\nagent = Agent(\\n  'openai:gpt-4o',\\n  deps_type=MyDeps, \\nPass the dataclass type to the deps_type argument of the Agent constructor. **Note**: we're passing the type here, NOT an instance, this parameter is not actually used at runtime, it's here so we can get full type checking of the agent.\\n\\n)\\n\\nasync defmain():\\n  async with httpx.AsyncClient() as client:\\n    deps = MyDeps('foobar', client)\\n    result = await agent.run(\\n      'Tell me a joke.',\\n      deps=deps, \\nWhen running the agent, pass an instance of the dataclass to the deps parameter.\\n\\n    )\\n    print(result.data)\\n    #> Did you hear about the toothpaste scandal? They called it Colgate.\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 11, 'url': 'https://pydantic.com'}, page_content=\"fromdataclassesimport dataclass\\nimporthttpx\\nfrompydantic_aiimport Agent, RunContext\\n\\n@dataclass\\nclassMyDeps:\\n  api_key: str\\n  http_client: httpx.AsyncClient\\n\\nagent = Agent(\\n  'openai:gpt-4o',\\n  deps_type=MyDeps,\\n)\\n\\n@agent.system_prompt \\nRunContext may optionally be passed to a system_prompt function as the only argument.\\n\\nasync defget_system_prompt(ctx: RunContext[MyDeps]) -> str: \\nRunContext is parameterized with the type of the dependencies, if this type is incorrect, static type checkers will raise an error.\\n\\n  response = await ctx.deps.http_client.get( \\nAccess dependencies through the .deps attribute.\\n\\n    'https://example.com',\\n    headers={'Authorization': f'Bearer {ctx.deps.api_key}'}, \\nAccess dependencies through the .deps attribute.\\n\\n  )\\n  response.raise_for_status()\\n  return f'Prompt: {response.text}'\\n\\nasync defmain():\\n  async with httpx.AsyncClient() as client:\\n    deps = MyDeps('foobar', client)\\n    result = await agent.run('Tell me a joke.', deps=deps)\\n    print(result.data)\\n    #> Did you hear about the toothpaste scandal? They called it Colgate.\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 12, 'url': 'https://pydantic.com'}, page_content=\"fromdataclassesimport dataclass\\nimporthttpx\\nfrompydantic_aiimport Agent, RunContext\\n\\n@dataclass\\nclassMyDeps:\\n  api_key: str\\n  http_client: httpx.Client \\nHere we use a synchronous httpx.Client instead of an asynchronous httpx.AsyncClient.\\n\\nagent = Agent(\\n  'openai:gpt-4o',\\n  deps_type=MyDeps,\\n)\\n\\n@agent.system_prompt\\ndefget_system_prompt(ctx: RunContext[MyDeps]) -> str: \\nTo match the synchronous dependency, the system prompt function is now a plain function, not a coroutine.\\n\\n  response = ctx.deps.http_client.get(\\n    'https://example.com', headers={'Authorization': f'Bearer {ctx.deps.api_key}'}\\n  )\\n  response.raise_for_status()\\n  return f'Prompt: {response.text}'\\n\\nasync defmain():\\n  deps = MyDeps('foobar', httpx.Client())\\n  result = await agent.run(\\n    'Tell me a joke.',\\n    deps=deps,\\n  )\\n  print(result.data)\\n  #> Did you hear about the toothpaste scandal? They called it Colgate.\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 12, 'url': 'https://pydantic.com'}, page_content=\"fromdataclassesimport dataclass\\nimporthttpx\\nfrompydantic_aiimport Agent, ModelRetry, RunContext\\n\\n@dataclass\\nclassMyDeps:\\n  api_key: str\\n  http_client: httpx.AsyncClient\\n\\nagent = Agent(\\n  'openai:gpt-4o',\\n  deps_type=MyDeps,\\n)\\n\\n@agent.system_prompt\\nasync defget_system_prompt(ctx: RunContext[MyDeps]) -> str:\\n  response = await ctx.deps.http_client.get('https://example.com')\\n  response.raise_for_status()\\n  return f'Prompt: {response.text}'\\n\\n@agent.tool \\nTo pass RunContext to a tool, use the tool decorator.\\n\\nasync defget_joke_material(ctx: RunContext[MyDeps], subject: str) -> str:\\n  response = await ctx.deps.http_client.get(\\n    'https://example.com#jokes',\\n    params={'subject': subject},\\n    headers={'Authorization': f'Bearer {ctx.deps.api_key}'},\\n  )\\n  response.raise_for_status()\\n  return response.text\\n\\n@agent.result_validator \\nRunContext may optionally be passed to a result_validator function as the first argument.\\n\\nasync defvalidate_result(ctx: RunContext[MyDeps], final_response: str) -> str:\\n  response = await ctx.deps.http_client.post(\\n    'https://example.com#validate',\\n    headers={'Authorization': f'Bearer {ctx.deps.api_key}'},\\n    params={'query': final_response},\\n  )\\n  if response.status_code == 400:\\n    raise ModelRetry(f'invalid response: {response.text}')\\n  response.raise_for_status()\\n  return final_response\\n\\nasync defmain():\\n  async with httpx.AsyncClient() as client:\\n    deps = MyDeps('foobar', client)\\n    result = await agent.run('Tell me a joke.', deps=deps)\\n    print(result.data)\\n    #> Did you hear about the toothpaste scandal? They called it Colgate.\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 12, 'url': 'https://pydantic.com'}, page_content=\"fromdataclassesimport dataclass\\nimporthttpx\\nfrompydantic_aiimport Agent, RunContext\\n\\n@dataclass\\nclassMyDeps:\\n  api_key: str\\n  http_client: httpx.AsyncClient\\n  async defsystem_prompt_factory(self) -> str: \\nDefine a method on the dependency to make the system prompt easier to customise.\\n\\n    response = await self.http_client.get('https://example.com')\\n    response.raise_for_status()\\n    return f'Prompt: {response.text}'\\n\\njoke_agent = Agent('openai:gpt-4o', deps_type=MyDeps)\\n\\n@joke_agent.system_prompt\\nasync defget_system_prompt(ctx: RunContext[MyDeps]) -> str:\\n  return await ctx.deps.system_prompt_factory() \\nCall the system prompt factory from within the system prompt function.\\n\\nasync defapplication_code(prompt: str) -> str: \\nApplication code that calls the agent, in a real application this might be an API endpoint.\\n\\n  ...\\n  ...\\n  # now deep within application code we call our agent\\n  async with httpx.AsyncClient() as client:\\n    app_deps = MyDeps('foobar', client)\\n    result = await joke_agent.run(prompt, deps=app_deps) \\nCall the agent from within the application code, in a real application this call might be deep within a call stack. Note app_deps here will NOT be used when deps are overridden.\\n\\n  return result.data\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 12, 'url': 'https://pydantic.com'}, page_content=\"fromjoke_appimport MyDeps, application_code, joke_agent\\n\\nclassTestMyDeps(MyDeps): \\nDefine a subclass of MyDeps in tests to customise the system prompt factory.\\n\\n  async defsystem_prompt_factory(self) -> str:\\n    return 'test prompt'\\n\\nasync deftest_application_code():\\n  test_deps = TestMyDeps('test_key', None) \\nCreate an instance of the test dependency, we don't need to pass an http_client here as it's not used.\\n\\n  with joke_agent.override(deps=test_deps): \\nOverride the dependencies of the agent for the duration of the with block, test_deps will be used when the agent is run.\\n\\n    joke = await application_code('Tell me a joke.') \\nNow we can safely call our application code, the agent will use the overridden dependencies.\\n\\n  assert joke.startswith('Did you hear about the toothpaste scandal?')\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 14, 'url': 'https://pydantic.com'}, page_content='pipinstallpydantic-graph'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 14, 'url': 'https://pydantic.com'}, page_content='uvaddpydantic-graph'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 15, 'url': 'https://pydantic.com'}, page_content='fromdataclassesimport dataclass\\nfrompydantic_graphimport BaseNode, GraphRunContext\\n\\n@dataclass\\nclassMyNode(BaseNodeMyState]): [\\n  foo: int \\n  async defrun(\\n    self,\\n    ctx: GraphRunContextMyState], [\\n  ) -> AnotherNode: \\n    ...\\n    return AnotherNode()'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 15, 'url': 'https://pydantic.com'}, page_content='fromdataclassesimport dataclass\\nfrompydantic_graphimport BaseNode, End, GraphRunContext\\n\\n@dataclass\\nclassMyNode(BaseNodeMyState, None, int]): [\\n  foo: int\\n  async defrun(\\n    self,\\n    ctx: GraphRunContext[MyState],\\n  ) -> AnotherNode | Endint]: [\\n    if self.foo % 5 == 0:\\n      return End(self.foo)\\n    else:\\n      return AnotherNode()'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 16, 'url': 'https://pydantic.com'}, page_content=\"from__future__import annotations\\nfromdataclassesimport dataclass\\nfrompydantic_graphimport BaseNode, End, Graph, GraphRunContext\\n\\n@dataclass\\nclassDivisibleBy5(BaseNodeNone, None, int]): [\\n  foo: int\\n  async defrun(\\n    self,\\n    ctx: GraphRunContext,\\n  ) -> Increment | End[int]:\\n    if self.foo % 5 == 0:\\n      return End(self.foo)\\n    else:\\n      return Increment(self.foo)\\n\\n@dataclass\\nclassIncrement(BaseNode): \\n  foo: int\\n  async defrun(self, ctx: GraphRunContext) -> DivisibleBy5:\\n    return DivisibleBy5(self.foo + 1)\\n\\nfives_graph = Graph(nodes=DivisibleBy5, Increment]) [\\nresult = fives_graph.run_sync(DivisibleBy5(4)) \\nprint(result.output)\\n#> 5\\n# the full history is quite verbose (see below), so we'll just print the summary\\nprint([item.data_snapshot() for item in result.history])\\n#> [DivisibleBy5(foo=4), Increment(foo=4), DivisibleBy5(foo=5), End(data=5)]\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 16, 'url': 'https://pydantic.com'}, page_content='fromgraph_exampleimport DivisibleBy5, fives_graph\\nfives_graph.mermaid_code(start_node=DivisibleBy5)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 16, 'url': 'https://pydantic.com'}, page_content='---\\ntitle: fives_graph\\n---\\nstateDiagram-v2\\n [*] --> DivisibleBy5\\n DivisibleBy5 --> Increment\\n DivisibleBy5 --> [*]\\n Increment --> DivisibleBy5'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 16, 'url': 'https://pydantic.com'}, page_content='fromgraph_exampleimport DivisibleBy5, fives_graph\\nfromIPython.displayimport Image, display\\ndisplay(Image(fives_graph.mermaid_image(start_node=DivisibleBy5)))'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 16, 'url': 'https://pydantic.com'}, page_content=\"from__future__import annotations\\nfromdataclassesimport dataclass\\nfromrich.promptimport Prompt\\nfrompydantic_graphimport BaseNode, End, Graph, GraphRunContext\\n\\n@dataclass\\nclassMachineState: \\nThe state of the vending machine is defined as a dataclass with the user's balance and the product they've selected, if any.\\n\\n  user_balance: float = 0.0\\n  product: str | None = None\\n\\n@dataclass\\nclassInsertCoin(BaseNode[MachineState]): \\nThe InsertCoin node, BaseNode is parameterized with MachineState as that's the state used in this graph.\\n\\n  async defrun(self, ctx: GraphRunContext[MachineState]) -> CoinsInserted: \\nThe return type of the node's run method is important as it is used to determine the outgoing edges of the node. This information in turn is used to render mermaid diagrams and is enforced at runtime to detect misbehavior as soon as possible.\\n\\n    return CoinsInserted(float(Prompt.ask('Insert coins'))) \\nThe InsertCoin node prompts the user to insert coins. We keep things simple by just entering a monetary amount as a float. Before you start thinking this is a toy too since it's using rich's Prompt.ask within nodes, see below for how control flow can be managed when nodes require external input.\\n\\n@dataclass\\nclassCoinsInserted(BaseNode[MachineState]):\\n  amount: float \\nThe CoinsInserted node; again this is a dataclass with one field amount.\\n\\n  async defrun(\\n    self, ctx: GraphRunContext[MachineState]\\n  ) -> SelectProduct | Purchase: \\nThe return type of CoinsInserted's run method is a union, meaning multiple outgoing edges are possible.\\n\\n    ctx.state.user_balance += self.amount \\nUpdate the user's balance with the amount inserted.\\n\\n    if ctx.state.product is not None: \\nIf the user has already selected a product, go to Purchase, otherwise go to SelectProduct.\\n\\n      return Purchase(ctx.state.product)\\n    else:\\n      return SelectProduct()\\n\\n@dataclass\\nclassSelectProduct(BaseNode[MachineState]):\\n  async defrun(self, ctx: GraphRunContext[MachineState]) -> Purchase:\\n    return Purchase(Prompt.ask('Select product'))\\n\\nPRODUCT_PRICES = { \\nA dictionary of products mapped to prices.\\n\\n  'water': 1.25,\\n  'soda': 1.50,\\n  'crisps': 1.75,\\n  'chocolate': 2.00,\\n}\\n\\n@dataclass\\nclassPurchase(BaseNode[MachineState, None, None]): \\nUnlike other nodes, Purchase can end the run, so the RunEndT generic parameter must be set. In this case it's None since the graph run return type is None.\\n\\n  product: str\\n  async defrun(\\n    self, ctx: GraphRunContext[MachineState]\\n  ) -> End | InsertCoin | SelectProduct:\\n    if price := PRODUCT_PRICES.get(self.product): \\nIn the Purchase node, look up the price of the product if the user entered a valid product.\\n\\n      ctx.state.product = self.product \\nIf the user did enter a valid product, set the product in the state so we don't revisit SelectProduct.\\n\\n      if ctx.state.user_balance >= price: \\nIf the balance is enough to purchase the product, adjust the balance to reflect the purchase and return End to end the graph. We're not using the run return type, so we call End with None.\\n\\n        ctx.state.user_balance -= price\\n        return End(None)\\n      else:\\n        diff = price - ctx.state.user_balance\\n        print(f'Not enough money for {self.product}, need {diff:0.2f} more')\\n        #> Not enough money for crisps, need 0.75 more\\n        return InsertCoin() \\nIf the balance is insufficient, go to InsertCoin to prompt the user to insert more coins.\\n\\n    else:\\n      print(f'No such product: {self.product}, try again')\\n      return SelectProduct() \\nIf the product is invalid, go to SelectProduct to prompt the user to select a product again.\\n\\nvending_machine_graph = Graph( \\nThe graph is created by passing a list of nodes to Graph. Order of nodes is not important, but it can affect how diagrams are displayed.\\n\\n  nodes=[InsertCoin, CoinsInserted, SelectProduct, Purchase]\\n)\\n\\nasync defmain():\\n  state = MachineState() \\nInitialize the state. This will be passed to the graph run and mutated as the graph runs.\\n\\n  await vending_machine_graph.run(InsertCoin(), state=state) \\nRun the graph with the initial state. Since the graph can be run from any node, we must pass the start node — in this case, InsertCoin. Graph.run returns a GraphRunResult that provides the final data and a history of the run.\\n\\n  print(f'purchase successful item={state.product} change={state.user_balance:0.2f}')\\n  #> purchase successful item=crisps change=0.25\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 16, 'url': 'https://pydantic.com'}, page_content='fromvending_machineimport InsertCoin, vending_machine_graph\\nvending_machine_graph.mermaid_code(start_node=InsertCoin)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 16, 'url': 'https://pydantic.com'}, page_content='---\\ntitle: vending_machine_graph\\n---\\nstateDiagram-v2\\n [*] --> InsertCoin\\n InsertCoin --> CoinsInserted\\n CoinsInserted --> SelectProduct\\n CoinsInserted --> Purchase\\n SelectProduct --> Purchase\\n Purchase --> InsertCoin\\n Purchase --> SelectProduct\\n Purchase --> [*]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 16, 'url': 'https://pydantic.com'}, page_content='---\\ntitle: feedback_graph\\n---\\nstateDiagram-v2\\n [*] --> WriteEmail\\n WriteEmail --> Feedback\\n Feedback --> WriteEmail\\n Feedback --> [*]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 17, 'url': 'https://pydantic.com'}, page_content='from__future__import annotations as _annotations\\nfromdataclassesimport dataclass, field\\nfrompydanticimport BaseModel, EmailStr\\nfrompydantic_aiimport Agent\\nfrompydantic_ai.format_as_xmlimport format_as_xml\\nfrompydantic_ai.messagesimport ModelMessage\\nfrompydantic_graphimport BaseNode, End, Graph, GraphRunContext\\n\\n@dataclass\\nclassUser:\\n  name: str\\n  email: EmailStr\\n  interests: list[str]\\n\\n@dataclass\\nclassEmail:\\n  subject: str\\n  body: str\\n\\n@dataclass\\nclassState:\\n  user: User\\n  write_agent_messages: list[ModelMessage] = field(default_factory=list)\\n\\nemail_writer_agent = Agent(\\n  \\'google-vertex:gemini-1.5-pro\\',\\n  result_type=Email,\\n  system_prompt=\\'Write a welcome email to our tech blog.\\',\\n)\\n\\n@dataclass\\nclassWriteEmail(BaseNode[State]):\\n  email_feedback: str | None = None\\n  async defrun(self, ctx: GraphRunContext[State]) -> Feedback:\\n    if self.email_feedback:\\n      prompt = (\\n        f\\'Rewrite the email for the user:\\\\n\\'\\n        f\\'{format_as_xml(ctx.state.user)}\\\\n\\'\\n        f\\'Feedback: {self.email_feedback}\\'\\n      )\\n    else:\\n      prompt = (\\n        f\\'Write a welcome email for the user:\\\\n\\'\\n        f\\'{format_as_xml(ctx.state.user)}\\'\\n      )\\n    result = await email_writer_agent.run(\\n      prompt,\\n      message_history=ctx.state.write_agent_messages,\\n    )\\n    ctx.state.write_agent_messages += result.all_messages()\\n    return Feedback(result.data)\\n\\nclassEmailRequiresWrite(BaseModel):\\n  feedback: str\\n\\nclassEmailOk(BaseModel):\\n  pass\\n\\nfeedback_agent = Agent[None, EmailRequiresWrite | EmailOk](\\n  \\'openai:gpt-4o\\',\\n  result_type=EmailRequiresWrite | EmailOk, # type: ignore\\n  system_prompt=(\\n    \\'Review the email and provide feedback, email must reference the users specific interests.\\'\\n  ),\\n)\\n\\n@dataclass\\nclassFeedback(BaseNode[State, None, Email]):\\n  email: Email\\n  async defrun(\\n    self,\\n    ctx: GraphRunContext[State],\\n  ) -> WriteEmail | End[Email]:\\n    prompt = format_as_xml({\\'user\\': ctx.state.user, \\'email\\': self.email})\\n    result = await feedback_agent.run(prompt)\\n    if isinstance(result.data, EmailRequiresWrite):\\n      return WriteEmail(email_feedback=result.data.feedback)\\n    else:\\n      return End(self.email)\\n\\nasync defmain():\\n  user = User(\\n    name=\\'John Doe\\',\\n    email=\\'john.joe@example.com\\',\\n    interests=[\\'Haskel\\', \\'Lisp\\', \\'Fortran\\'],\\n  )\\n  state = State(user)\\n  feedback_graph = Graph(nodes=(WriteEmail, Feedback))\\n  result = await feedback_graph.run(WriteEmail(), state=state)\\n  print(result.output)\\n\"\"\"\\n  Email(\\n    subject=\\'Welcome to our tech blog!\\',\\n    body=\\'Hello John, Welcome to our tech blog! ...\\',\\n  )\\n  \"\"\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 17, 'url': 'https://pydantic.com'}, page_content=\"from__future__import annotations as _annotations\\nfromdataclassesimport dataclass, field\\nfrompydantic_graphimport BaseNode, End, Graph, GraphRunContext\\nfrompydantic_aiimport Agent\\nfrompydantic_ai.format_as_xmlimport format_as_xml\\nfrompydantic_ai.messagesimport ModelMessage\\nask_agent = Agent('openai:gpt-4o', result_type=str)\\n\\n@dataclass\\nclassQuestionState:\\n  question: str | None = None\\n  ask_agent_messages: list[ModelMessage] = field(default_factory=list)\\n  evaluate_agent_messages: list[ModelMessage] = field(default_factory=list)\\n\\n@dataclass\\nclassAsk(BaseNode[QuestionState]):\\n  async defrun(self, ctx: GraphRunContext[QuestionState]) -> Answer:\\n    result = await ask_agent.run(\\n      'Ask a simple question with a single correct answer.',\\n      message_history=ctx.state.ask_agent_messages,\\n    )\\n    ctx.state.ask_agent_messages += result.all_messages()\\n    ctx.state.question = result.data\\n    return Answer(result.data)\\n\\n@dataclass\\nclassAnswer(BaseNode[QuestionState]):\\n  question: str\\n  answer: str | None = None\\n  async defrun(self, ctx: GraphRunContext[QuestionState]) -> Evaluate:\\n    assert self.answer is not None\\n    return Evaluate(self.answer)\\n\\n@dataclass\\nclassEvaluationResult:\\n  correct: bool\\n  comment: str\\n\\nevaluate_agent = Agent(\\n  'openai:gpt-4o',\\n  result_type=EvaluationResult,\\n  system_prompt='Given a question and answer, evaluate if the answer is correct.',\\n)\\n\\n@dataclass\\nclassEvaluate(BaseNode[QuestionState]):\\n  answer: str\\n  async defrun(\\n    self,\\n    ctx: GraphRunContext[QuestionState],\\n  ) -> End[str] | Reprimand:\\n    assert ctx.state.question is not None\\n    result = await evaluate_agent.run(\\n      format_as_xml({'question': ctx.state.question, 'answer': self.answer}),\\n      message_history=ctx.state.evaluate_agent_messages,\\n    )\\n    ctx.state.evaluate_agent_messages += result.all_messages()\\n    if result.data.correct:\\n      return End(result.data.comment)\\n    else:\\n      return Reprimand(result.data.comment)\\n\\n@dataclass\\nclassReprimand(BaseNode[QuestionState]):\\n  comment: str\\n  async defrun(self, ctx: GraphRunContext[QuestionState]) -> Ask:\\n    print(f'Comment: {self.comment}')\\n    ctx.state.question = None\\n    return Ask()\\n\\nquestion_graph = Graph(nodes=(Ask, Answer, Evaluate, Reprimand))\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 17, 'url': 'https://pydantic.com'}, page_content='fromrich.promptimport Prompt\\nfrompydantic_graphimport End, HistoryStep\\nfromai_q_and_a_graphimport Ask, question_graph, QuestionState, Answer\\n\\nasync defmain():\\n  state = QuestionState() \\nCreate the state object which will be mutated by next.\\n\\n  node = Ask() \\nThe start node is Ask but will be updated by next as the graph runs.\\n\\n  history: list[HistoryStep[QuestionState]] = [] \\nThe history of the graph run is stored in a list of HistoryStep objects. Again next will update this list in place.\\n\\n  while True:\\n    node = await question_graph.next(node, history, state=state) \\nRun the graph one node at a time, updating the state, current node and history as the graph runs.\\n\\n    if isinstance(node, Answer):\\n      node.answer = Prompt.ask(node.question) \\nIf the current node is an Answer node, prompt the user for an answer.\\n\\n    elif isinstance(node, End): \\nSince we\\'re using next we have to manually check for an End and exit the loop if we get one.\\n\\n      print(f\\'Correct answer! {node.data}\\')\\n      #> Correct answer! Well done, 1 + 1 = 2\\n      print([e.data_snapshot() for e in history])\\n\"\"\"\\n      [\\n        Ask(),\\n        Answer(question=\\'What is the capital of France?\\', answer=\\'Vichy\\'),\\n        Evaluate(answer=\\'Vichy\\'),\\n        Reprimand(comment=\\'Vichy is no longer the capital of France.\\'),\\n        Ask(),\\n        Answer(question=\\'what is 1 + 1?\\', answer=\\'2\\'),\\n        Evaluate(answer=\\'2\\'),\\n        End(data=\\'Well done, 1 + 1 = 2\\'),\\n      ]\\n      \"\"\"\\n      return\\n    # otherwise just continue'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 17, 'url': 'https://pydantic.com'}, page_content='fromai_q_and_a_graphimport Ask, question_graph\\nquestion_graph.mermaid_code(start_node=Ask)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 17, 'url': 'https://pydantic.com'}, page_content='---\\ntitle: question_graph\\n---\\nstateDiagram-v2\\n [*] --> Ask\\n Ask --> Answer\\n Answer --> Evaluate\\n Evaluate --> Reprimand\\n Evaluate --> [*]\\n Reprimand --> Ask'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 17, 'url': 'https://pydantic.com'}, page_content='from__future__import annotations as _annotations\\nfromdataclassesimport dataclass\\nfrompydantic_graphimport Graph, BaseNode, End, GraphRunContext\\n\\n@dataclass\\nclassCountDownState:\\n  counter: int\\n\\n@dataclass\\nclassCountDown(BaseNode[CountDownState]):\\n  async defrun(self, ctx: GraphRunContext[CountDownState]) -> CountDown | End[int]:\\n    if ctx.state.counter <= 0:\\n      return End(ctx.state.counter)\\n    ctx.state.counter -= 1\\n    return CountDown()\\n\\ncount_down_graph = Graph(nodes=[CountDown])\\n\\nasync defmain():\\n  state = CountDownState(counter=3)\\n  with count_down_graph.iter(CountDown(), state=state) as run: \\nGraph.iter(...) returns a GraphRun.\\n\\n    async for node in run: \\nHere, we step through each node as it is executed.\\n\\n      print(\\'Node:\\', node)\\n      #> Node: CountDown()\\n      #> Node: CountDown()\\n      #> Node: CountDown()\\n      #> Node: End(data=0)\\n  print(\\'Final result:\\', run.result.output) \\nOnce the graph returns an End, the loop ends, and run.final_result becomes a GraphRunResult containing the final outcome (0 here).\\n\\n  #> Final result: 0\\n  print(\\'History snapshots:\\', [step.data_snapshot() for step in run.history])\\n\"\"\"\\n  History snapshots:\\n  [CountDown(), CountDown(), CountDown(), CountDown(), End(data=0)]\\n  \"\"\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 18, 'url': 'https://pydantic.com'}, page_content=\"frompydantic_graphimport End\\nfromcount_downimport CountDown, CountDownState, count_down_graph\\n\\nasync defmain():\\n  state = CountDownState(counter=5)\\n  with count_down_graph.iter(CountDown(), state=state) as run:\\n    node = run.next_node \\nWe start by grabbing the first node that will be run in the agent's graph.\\n\\n    while not isinstance(node, End): \\nThe agent run is finished once an End node has been produced; instances of End cannot be passed to next.\\n\\n      print('Node:', node)\\n      #> Node: CountDown()\\n      #> Node: CountDown()\\n      #> Node: CountDown()\\n      #> Node: CountDown()\\n      if state.counter == 2:\\n        break \\nIf the user decides to stop early, we break out of the loop. The graph run won't have a real final result in that case (run.final_result remains None).\\n\\n      node = await run.next(node) \\nAt each step, we call await run.next(node) to run it and get the next node (or an End).\\n\\n    print(run.result) \\nBecause we did not continue the run until it finished, the result is not set.\\n\\n    #> None\\n    for step in run.history: \\nThe run's history is still populated with the steps we executed so far.\\n\\n      print('History Step:', step.data_snapshot(), step.state)\\n      #> History Step: CountDown() CountDownState(counter=4)\\n      #> History Step: CountDown() CountDownState(counter=3)\\n      #> History Step: CountDown() CountDownState(counter=2)\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 18, 'url': 'https://pydantic.com'}, page_content='from__future__import annotations\\nimportasyncio\\nfromconcurrent.futuresimport ProcessPoolExecutor\\nfromdataclassesimport dataclass\\nfrompydantic_graphimport BaseNode, End, Graph, GraphRunContext\\n\\n@dataclass\\nclassGraphDeps:\\n  executor: ProcessPoolExecutor\\n\\n@dataclass\\nclassDivisibleBy5(BaseNode[None, GraphDeps, int]):\\n  foo: int\\n  async defrun(\\n    self,\\n    ctx: GraphRunContext[None, GraphDeps],\\n  ) -> Increment | End[int]:\\n    if self.foo % 5 == 0:\\n      return End(self.foo)\\n    else:\\n      return Increment(self.foo)\\n\\n@dataclass\\nclassIncrement(BaseNode[None, GraphDeps]):\\n  foo: int\\n  async defrun(self, ctx: GraphRunContext[None, GraphDeps]) -> DivisibleBy5:\\n    loop = asyncio.get_running_loop()\\n    compute_result = await loop.run_in_executor(\\n      ctx.deps.executor,\\n      self.compute,\\n    )\\n    return DivisibleBy5(compute_result)\\n  defcompute(self) -> int:\\n    return self.foo + 1\\n\\nfives_graph = Graph(nodes=[DivisibleBy5, Increment])\\n\\nasync defmain():\\n  with ProcessPoolExecutor() as executor:\\n    deps = GraphDeps(executor)\\n    result = await fives_graph.run(DivisibleBy5(3), deps=deps)\\n  print(result.output)\\n  #> 5\\n  # the full history is quite verbose (see below), so we\\'ll just print the summary\\n  print([item.data_snapshot() for item in result.history])\\n\"\"\"\\n  [\\n    DivisibleBy5(foo=3),\\n    Increment(foo=3),\\n    DivisibleBy5(foo=4),\\n    Increment(foo=4),\\n    DivisibleBy5(foo=5),\\n    End(data=5),\\n  ]\\n  \"\"\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 18, 'url': 'https://pydantic.com'}, page_content='...\\nfromtypingimport Annotated\\n\\nfrompydantic_graphimport BaseNode, End, Graph, GraphRunContext, Edge\\n\\n...\\n@dataclass\\nclassAsk(BaseNode[QuestionState]):\\n\"\"\"Generate question using GPT-4o.\"\"\"\\n  docstring_notes = True\\n  async defrun(\\n    self, ctx: GraphRunContext[QuestionState]\\n  ) -> Annotated[Answer, Edge(label=\\'Ask the question\\')]:\\n    ...\\n...\\n@dataclass\\nclassEvaluate(BaseNode[QuestionState]):\\n  answer: str\\n  async defrun(\\n      self,\\n      ctx: GraphRunContext[QuestionState],\\n  ) -> Annotated[End[str], Edge(label=\\'success\\')] | Reprimand:\\n    ...\\n...\\nquestion_graph.mermaid_save(\\'image.png\\', highlighted_nodes=[Answer])'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 19, 'url': 'https://pydantic.com'}, page_content='---\\ntitle: question_graph\\n---\\nstateDiagram-v2\\n Ask --> Answer: Ask the question\\n note right of Ask\\n  Judge the answer.\\n  Decide on next step.\\n end note\\n Answer --> Evaluate\\n Evaluate --> Reprimand\\n Evaluate --> [*]: success\\n Reprimand --> Ask\\nclassDef highlighted fill:#fdff32\\nclass Answer highlighted'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 19, 'url': 'https://pydantic.com'}, page_content=\"fromvending_machineimport InsertCoin, vending_machine_graph\\nvending_machine_graph.mermaid_code(start_node=InsertCoin, direction='LR')\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 19, 'url': 'https://pydantic.com'}, page_content='---\\ntitle: vending_machine_graph\\n---\\nstateDiagram-v2\\n direction LR\\n [*] --> InsertCoin\\n InsertCoin --> CoinsInserted\\n CoinsInserted --> SelectProduct\\n CoinsInserted --> Purchase\\n SelectProduct --> Purchase\\n Purchase --> InsertCoin\\n Purchase --> SelectProduct\\n Purchase --> [*]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 21, 'url': 'https://pydantic.com'}, page_content='pipinstallpydantic-ai'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 21, 'url': 'https://pydantic.com'}, page_content='uvaddpydantic-ai'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 21, 'url': 'https://pydantic.com'}, page_content=\"pipinstall'pydantic-ai[logfire]'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 21, 'url': 'https://pydantic.com'}, page_content=\"uvadd'pydantic-ai[logfire]'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 21, 'url': 'https://pydantic.com'}, page_content=\"pipinstall'pydantic-ai[examples]'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 21, 'url': 'https://pydantic.com'}, page_content=\"uvadd'pydantic-ai[examples]'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 21, 'url': 'https://pydantic.com'}, page_content=\"pipinstall'pydantic-ai-slim[openai]'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 21, 'url': 'https://pydantic.com'}, page_content=\"uvadd'pydantic-ai-slim[openai]'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 22, 'url': 'https://pydantic.com'}, page_content=\"pipinstall'pydantic-ai-slim[openai,vertexai,logfire]'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 22, 'url': 'https://pydantic.com'}, page_content=\"uvadd'pydantic-ai-slim[openai,vertexai,logfire]'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 24, 'url': 'https://pydantic.com'}, page_content=\"pipinstall'pydantic-ai[logfire]'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 24, 'url': 'https://pydantic.com'}, page_content=\"uvadd'pydantic-ai[logfire]'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 24, 'url': 'https://pydantic.com'}, page_content='logfireauth'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 24, 'url': 'https://pydantic.com'}, page_content='uvrunlogfireauth'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 24, 'url': 'https://pydantic.com'}, page_content='logfireprojectsnew'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 24, 'url': 'https://pydantic.com'}, page_content='uvrunlogfireprojectsnew'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 24, 'url': 'https://pydantic.com'}, page_content='importlogfire\\nlogfire.configure()'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 25, 'url': 'https://pydantic.com'}, page_content='importlogfire\\nlogfire.configure()\\nlogfire.instrument_httpx(capture_all=True) \\nSee the logfire docs for more httpx instrumentation details.'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 25, 'url': 'https://pydantic.com'}, page_content=\"importlogfire\\nfrompydantic_aiimport Agent\\nlogfire.configure()\\nlogfire.instrument_httpx(capture_all=True) \\nCapture all of headers, request body, and response body.\\n\\nagent = Agent('openai:gpt-4o')\\nresult = agent.run_sync('What is the capital of France?')\\nprint(result.data)\\n#> The capital of France is Paris.\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 26, 'url': 'https://pydantic.com'}, page_content='frompydantic_aiimport Agent\\nagent = Agent(\\'openai:gpt-4o\\', system_prompt=\\'Be a helpful assistant.\\')\\nresult = agent.run_sync(\\'Tell me a joke.\\')\\nprint(result.data)\\n#> Did you hear about the toothpaste scandal? They called it Colgate.\\n# all messages from the run\\nprint(result.all_messages())\\n\"\"\"\\n[\\n  ModelRequest(\\n    parts=[\\n      SystemPromptPart(\\n        content=\\'Be a helpful assistant.\\',\\n        dynamic_ref=None,\\n        part_kind=\\'system-prompt\\',\\n      ),\\n      UserPromptPart(\\n        content=\\'Tell me a joke.\\',\\n        timestamp=datetime.datetime(...),\\n        part_kind=\\'user-prompt\\',\\n      ),\\n    ],\\n    kind=\\'request\\',\\n  ),\\n  ModelResponse(\\n    parts=[\\n      TextPart(\\n        content=\\'Did you hear about the toothpaste scandal? They called it Colgate.\\',\\n        part_kind=\\'text\\',\\n      )\\n    ],\\n    model_name=\\'function:model_logic\\',\\n    timestamp=datetime.datetime(...),\\n    kind=\\'response\\',\\n  ),\\n]\\n\"\"\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 26, 'url': 'https://pydantic.com'}, page_content='frompydantic_aiimport Agent\\nagent = Agent(\\'openai:gpt-4o\\', system_prompt=\\'Be a helpful assistant.\\')\\n\\nasync defmain():\\n  async with agent.run_stream(\\'Tell me a joke.\\') as result:\\n    # incomplete messages before the stream finishes\\n    print(result.all_messages())\\n\"\"\"\\n    [\\n      ModelRequest(\\n        parts=[\\n          SystemPromptPart(\\n            content=\\'Be a helpful assistant.\\',\\n            dynamic_ref=None,\\n            part_kind=\\'system-prompt\\',\\n          ),\\n          UserPromptPart(\\n            content=\\'Tell me a joke.\\',\\n            timestamp=datetime.datetime(...),\\n            part_kind=\\'user-prompt\\',\\n          ),\\n        ],\\n        kind=\\'request\\',\\n      )\\n    ]\\n    \"\"\"\\n    async for text in result.stream_text():\\n      print(text)\\n      #> Did you hear\\n      #> Did you hear about the toothpaste\\n      #> Did you hear about the toothpaste scandal? They called\\n      #> Did you hear about the toothpaste scandal? They called it Colgate.\\n    # complete messages once the stream finishes\\n    print(result.all_messages())\\n\"\"\"\\n    [\\n      ModelRequest(\\n        parts=[\\n          SystemPromptPart(\\n            content=\\'Be a helpful assistant.\\',\\n            dynamic_ref=None,\\n            part_kind=\\'system-prompt\\',\\n          ),\\n          UserPromptPart(\\n            content=\\'Tell me a joke.\\',\\n            timestamp=datetime.datetime(...),\\n            part_kind=\\'user-prompt\\',\\n          ),\\n        ],\\n        kind=\\'request\\',\\n      ),\\n      ModelResponse(\\n        parts=[\\n          TextPart(\\n            content=\\'Did you hear about the toothpaste scandal? They called it Colgate.\\',\\n            part_kind=\\'text\\',\\n          )\\n        ],\\n        model_name=\\'function:stream_model_logic\\',\\n        timestamp=datetime.datetime(...),\\n        kind=\\'response\\',\\n      ),\\n    ]\\n    \"\"\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 27, 'url': 'https://pydantic.com'}, page_content='frompydantic_aiimport Agent\\nagent = Agent(\\'openai:gpt-4o\\', system_prompt=\\'Be a helpful assistant.\\')\\nresult1 = agent.run_sync(\\'Tell me a joke.\\')\\nprint(result1.data)\\n#> Did you hear about the toothpaste scandal? They called it Colgate.\\nresult2 = agent.run_sync(\\'Explain?\\', message_history=result1.new_messages())\\nprint(result2.data)\\n#> This is an excellent joke invented by Samuel Colvin, it needs no explanation.\\nprint(result2.all_messages())\\n\"\"\"\\n[\\n  ModelRequest(\\n    parts=[\\n      SystemPromptPart(\\n        content=\\'Be a helpful assistant.\\',\\n        dynamic_ref=None,\\n        part_kind=\\'system-prompt\\',\\n      ),\\n      UserPromptPart(\\n        content=\\'Tell me a joke.\\',\\n        timestamp=datetime.datetime(...),\\n        part_kind=\\'user-prompt\\',\\n      ),\\n    ],\\n    kind=\\'request\\',\\n  ),\\n  ModelResponse(\\n    parts=[\\n      TextPart(\\n        content=\\'Did you hear about the toothpaste scandal? They called it Colgate.\\',\\n        part_kind=\\'text\\',\\n      )\\n    ],\\n    model_name=\\'function:model_logic\\',\\n    timestamp=datetime.datetime(...),\\n    kind=\\'response\\',\\n  ),\\n  ModelRequest(\\n    parts=[\\n      UserPromptPart(\\n        content=\\'Explain?\\',\\n        timestamp=datetime.datetime(...),\\n        part_kind=\\'user-prompt\\',\\n      )\\n    ],\\n    kind=\\'request\\',\\n  ),\\n  ModelResponse(\\n    parts=[\\n      TextPart(\\n        content=\\'This is an excellent joke invented by Samuel Colvin, it needs no explanation.\\',\\n        part_kind=\\'text\\',\\n      )\\n    ],\\n    model_name=\\'function:model_logic\\',\\n    timestamp=datetime.datetime(...),\\n    kind=\\'response\\',\\n  ),\\n]\\n\"\"\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 27, 'url': 'https://pydantic.com'}, page_content='frompydantic_aiimport Agent\\nagent = Agent(\\'openai:gpt-4o\\', system_prompt=\\'Be a helpful assistant.\\')\\nresult1 = agent.run_sync(\\'Tell me a joke.\\')\\nprint(result1.data)\\n#> Did you hear about the toothpaste scandal? They called it Colgate.\\nresult2 = agent.run_sync(\\n  \\'Explain?\\',\\n  model=\\'google-gla:gemini-1.5-pro\\',\\n  message_history=result1.new_messages(),\\n)\\nprint(result2.data)\\n#> This is an excellent joke invented by Samuel Colvin, it needs no explanation.\\nprint(result2.all_messages())\\n\"\"\"\\n[\\n  ModelRequest(\\n    parts=[\\n      SystemPromptPart(\\n        content=\\'Be a helpful assistant.\\',\\n        dynamic_ref=None,\\n        part_kind=\\'system-prompt\\',\\n      ),\\n      UserPromptPart(\\n        content=\\'Tell me a joke.\\',\\n        timestamp=datetime.datetime(...),\\n        part_kind=\\'user-prompt\\',\\n      ),\\n    ],\\n    kind=\\'request\\',\\n  ),\\n  ModelResponse(\\n    parts=[\\n      TextPart(\\n        content=\\'Did you hear about the toothpaste scandal? They called it Colgate.\\',\\n        part_kind=\\'text\\',\\n      )\\n    ],\\n    model_name=\\'function:model_logic\\',\\n    timestamp=datetime.datetime(...),\\n    kind=\\'response\\',\\n  ),\\n  ModelRequest(\\n    parts=[\\n      UserPromptPart(\\n        content=\\'Explain?\\',\\n        timestamp=datetime.datetime(...),\\n        part_kind=\\'user-prompt\\',\\n      )\\n    ],\\n    kind=\\'request\\',\\n  ),\\n  ModelResponse(\\n    parts=[\\n      TextPart(\\n        content=\\'This is an excellent joke invented by Samuel Colvin, it needs no explanation.\\',\\n        part_kind=\\'text\\',\\n      )\\n    ],\\n    model_name=\\'function:model_logic\\',\\n    timestamp=datetime.datetime(...),\\n    kind=\\'response\\',\\n  ),\\n]\\n\"\"\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 29, 'url': 'https://pydantic.com'}, page_content=\"pipinstall'pydantic-ai-slim[openai]'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 29, 'url': 'https://pydantic.com'}, page_content=\"uvadd'pydantic-ai-slim[openai]'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 29, 'url': 'https://pydantic.com'}, page_content=\"exportOPENAI_API_KEY='your-api-key'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 29, 'url': 'https://pydantic.com'}, page_content=\"frompydantic_aiimport Agent\\nagent = Agent('openai:gpt-4o')\\n...\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 29, 'url': 'https://pydantic.com'}, page_content=\"frompydantic_aiimport Agent\\nfrompydantic_ai.models.openaiimport OpenAIModel\\nmodel = OpenAIModel('gpt-4o')\\nagent = Agent(model)\\n...\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 29, 'url': 'https://pydantic.com'}, page_content=\"frompydantic_aiimport Agent\\nfrompydantic_ai.models.openaiimport OpenAIModel\\nmodel = OpenAIModel('gpt-4o', api_key='your-api-key')\\nagent = Agent(model)\\n...\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 29, 'url': 'https://pydantic.com'}, page_content=\"fromopenaiimport AsyncAzureOpenAI\\nfrompydantic_aiimport Agent\\nfrompydantic_ai.models.openaiimport OpenAIModel\\nclient = AsyncAzureOpenAI(\\n  azure_endpoint='...',\\n  api_version='2024-07-01-preview',\\n  api_key='your-api-key',\\n)\\nmodel = OpenAIModel('gpt-4o', openai_client=client)\\nagent = Agent(model)\\n...\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 29, 'url': 'https://pydantic.com'}, page_content=\"pipinstall'pydantic-ai-slim[anthropic]'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 29, 'url': 'https://pydantic.com'}, page_content=\"uvadd'pydantic-ai-slim[anthropic]'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 30, 'url': 'https://pydantic.com'}, page_content=\"exportANTHROPIC_API_KEY='your-api-key'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 30, 'url': 'https://pydantic.com'}, page_content=\"frompydantic_aiimport Agent\\nagent = Agent('anthropic:claude-3-5-sonnet-latest')\\n...\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 30, 'url': 'https://pydantic.com'}, page_content=\"frompydantic_aiimport Agent\\nfrompydantic_ai.models.anthropicimport AnthropicModel\\nmodel = AnthropicModel('claude-3-5-sonnet-latest')\\nagent = Agent(model)\\n...\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 30, 'url': 'https://pydantic.com'}, page_content=\"frompydantic_aiimport Agent\\nfrompydantic_ai.models.anthropicimport AnthropicModel\\nmodel = AnthropicModel('claude-3-5-sonnet-latest', api_key='your-api-key')\\nagent = Agent(model)\\n...\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 30, 'url': 'https://pydantic.com'}, page_content='exportGEMINI_API_KEY=your-api-key'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 30, 'url': 'https://pydantic.com'}, page_content=\"frompydantic_aiimport Agent\\nagent = Agent('google-gla:gemini-1.5-flash')\\n...\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 30, 'url': 'https://pydantic.com'}, page_content=\"frompydantic_aiimport Agent\\nfrompydantic_ai.models.geminiimport GeminiModel\\nmodel = GeminiModel('gemini-1.5-flash')\\nagent = Agent(model)\\n...\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 31, 'url': 'https://pydantic.com'}, page_content=\"frompydantic_aiimport Agent\\nfrompydantic_ai.models.geminiimport GeminiModel\\nmodel = GeminiModel('gemini-1.5-flash', api_key='your-api-key')\\nagent = Agent(model)\\n...\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 31, 'url': 'https://pydantic.com'}, page_content=\"pipinstall'pydantic-ai-slim[vertexai]'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 31, 'url': 'https://pydantic.com'}, page_content=\"uvadd'pydantic-ai-slim[vertexai]'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 31, 'url': 'https://pydantic.com'}, page_content=\"frompydantic_aiimport Agent\\nfrompydantic_ai.models.vertexaiimport VertexAIModel\\nmodel = VertexAIModel('gemini-1.5-flash')\\nagent = Agent(model)\\n...\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 32, 'url': 'https://pydantic.com'}, page_content=\"frompydantic_aiimport Agent\\nfrompydantic_ai.models.vertexaiimport VertexAIModel\\nmodel = VertexAIModel(\\n  'gemini-1.5-flash',\\n  service_account_file='path/to/service-account.json',\\n)\\nagent = Agent(model)\\n...\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 32, 'url': 'https://pydantic.com'}, page_content=\"frompydantic_aiimport Agent\\nfrompydantic_ai.models.vertexaiimport VertexAIModel\\nmodel = VertexAIModel('gemini-1.5-flash', region='asia-east1')\\nagent = Agent(model)\\n...\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 32, 'url': 'https://pydantic.com'}, page_content=\"pipinstall'pydantic-ai-slim[groq]'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 32, 'url': 'https://pydantic.com'}, page_content=\"uvadd'pydantic-ai-slim[groq]'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 32, 'url': 'https://pydantic.com'}, page_content=\"exportGROQ_API_KEY='your-api-key'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 33, 'url': 'https://pydantic.com'}, page_content=\"frompydantic_aiimport Agent\\nagent = Agent('groq:llama-3.3-70b-versatile')\\n...\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 33, 'url': 'https://pydantic.com'}, page_content=\"frompydantic_aiimport Agent\\nfrompydantic_ai.models.groqimport GroqModel\\nmodel = GroqModel('llama-3.3-70b-versatile')\\nagent = Agent(model)\\n...\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 33, 'url': 'https://pydantic.com'}, page_content=\"frompydantic_aiimport Agent\\nfrompydantic_ai.models.groqimport GroqModel\\nmodel = GroqModel('llama-3.3-70b-versatile', api_key='your-api-key')\\nagent = Agent(model)\\n...\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 33, 'url': 'https://pydantic.com'}, page_content=\"pipinstall'pydantic-ai-slim[mistral]'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 33, 'url': 'https://pydantic.com'}, page_content=\"uvadd'pydantic-ai-slim[mistral]'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 33, 'url': 'https://pydantic.com'}, page_content=\"exportMISTRAL_API_KEY='your-api-key'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 33, 'url': 'https://pydantic.com'}, page_content=\"frompydantic_aiimport Agent\\nagent = Agent('mistral:mistral-large-latest')\\n...\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 33, 'url': 'https://pydantic.com'}, page_content=\"frompydantic_aiimport Agent\\nfrompydantic_ai.models.mistralimport MistralModel\\nmodel = MistralModel('mistral-small-latest')\\nagent = Agent(model)\\n...\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 33, 'url': 'https://pydantic.com'}, page_content=\"frompydantic_aiimport Agent\\nfrompydantic_ai.models.mistralimport MistralModel\\nmodel = MistralModel('mistral-small-latest', api_key='your-api-key')\\nagent = Agent(model)\\n...\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 33, 'url': 'https://pydantic.com'}, page_content=\"pipinstall'pydantic-ai-slim[cohere]'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 33, 'url': 'https://pydantic.com'}, page_content=\"uvadd'pydantic-ai-slim[cohere]'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 33, 'url': 'https://pydantic.com'}, page_content=\"exportCO_API_KEY='your-api-key'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 33, 'url': 'https://pydantic.com'}, page_content=\"frompydantic_aiimport Agent\\nagent = Agent('cohere:command')\\n...\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 33, 'url': 'https://pydantic.com'}, page_content=\"frompydantic_aiimport Agent\\nfrompydantic_ai.models.cohereimport CohereModel\\nmodel = CohereModel('command', api_key='your-api-key')\\nagent = Agent(model)\\n...\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 34, 'url': 'https://pydantic.com'}, page_content=\"frompydantic_aiimport Agent\\nfrompydantic_ai.models.cohereimport CohereModel\\nmodel = CohereModel('command', api_key='your-api-key')\\nagent = Agent(model)\\n...\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 34, 'url': 'https://pydantic.com'}, page_content=\"frompydantic_ai.models.openaiimport OpenAIModel\\nmodel = OpenAIModel(\\n  'model_name',\\n  base_url='https://<openai-compatible-api-endpoint>.com',\\n  api_key='your-api-key',\\n)\\n...\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 34, 'url': 'https://pydantic.com'}, page_content='ollamarunllama3.2'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 34, 'url': 'https://pydantic.com'}, page_content='frompydanticimport BaseModel\\nfrompydantic_aiimport Agent\\nfrompydantic_ai.models.openaiimport OpenAIModel\\n\\nclassCityLocation(BaseModel):\\n  city: str\\n  country: str\\n\\nollama_model = OpenAIModel(model_name=\\'llama3.2\\', base_url=\\'http://localhost:11434/v1\\')\\nagent = Agent(ollama_model, result_type=CityLocation)\\nresult = agent.run_sync(\\'Where were the olympics held in 2012?\\')\\nprint(result.data)\\n#> city=\\'London\\' country=\\'United Kingdom\\'\\nprint(result.usage())\\n\"\"\"\\nUsage(requests=1, request_tokens=57, response_tokens=8, total_tokens=65, details=None)\\n\"\"\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 34, 'url': 'https://pydantic.com'}, page_content='frompydanticimport BaseModel\\nfrompydantic_aiimport Agent\\nfrompydantic_ai.models.openaiimport OpenAIModel\\nollama_model = OpenAIModel(\\n  model_name=\\'qwen2.5-coder:7b\\', \\nThe name of the model running on the remote server\\n\\n  base_url=\\'http://192.168.1.74:11434/v1\\', \\nThe url of the remote server\\n\\n)\\n\\nclassCityLocation(BaseModel):\\n  city: str\\n  country: str\\n\\nagent = Agent(model=ollama_model, result_type=CityLocation)\\nresult = agent.run_sync(\\'Where were the olympics held in 2012?\\')\\nprint(result.data)\\n#> city=\\'London\\' country=\\'United Kingdom\\'\\nprint(result.usage())\\n\"\"\"\\nUsage(requests=1, request_tokens=57, response_tokens=8, total_tokens=65, details=None)\\n\"\"\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 34, 'url': 'https://pydantic.com'}, page_content=\"frompydantic_aiimport Agent\\nfrompydantic_ai.models.openaiimport OpenAIModel\\nmodel = OpenAIModel(\\n  'anthropic/claude-3.5-sonnet',\\n  base_url='https://openrouter.ai/api/v1',\\n  api_key='your-openrouter-api-key',\\n)\\nagent = Agent(model)\\n...\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 34, 'url': 'https://pydantic.com'}, page_content=\"frompydantic_aiimport Agent\\nfrompydantic_ai.models.openaiimport OpenAIModel\\nmodel = OpenAIModel(\\n  'grok-2-1212',\\n  base_url='https://api.x.ai/v1',\\n  api_key='your-xai-api-key',\\n)\\nagent = Agent(model)\\n...\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 34, 'url': 'https://pydantic.com'}, page_content=\"frompydantic_aiimport Agent\\nfrompydantic_ai.models.openaiimport OpenAIModel\\nmodel = OpenAIModel(\\n  'deepseek-chat',\\n  base_url='https://api.deepseek.com',\\n  api_key='your-deepseek-api-key',\\n)\\nagent = Agent(model)\\n...\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 35, 'url': 'https://pydantic.com'}, page_content=\"frompydantic_aiimport Agent\\nfrompydantic_ai.models.openaiimport OpenAIModel\\nmodel = OpenAIModel(\\n  'sonar-pro',\\n  base_url='https://api.perplexity.ai',\\n  api_key='your-perplexity-api-key',\\n)\\nagent = Agent(model)\\n...\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 36, 'url': 'https://pydantic.com'}, page_content='frompydantic_aiimport Agent, RunContext\\nfrompydantic_ai.usageimport UsageLimits\\njoke_selection_agent = Agent( \\nThe \"parent\" or controlling agent.\\n\\n  \\'openai:gpt-4o\\',\\n  system_prompt=(\\n    \\'Use the `joke_factory` to generate some jokes, then choose the best. \\'\\n    \\'You must return just a single joke.\\'\\n  ),\\n)\\njoke_generation_agent = Agent( \\nThe \"delegate\" agent, which is called from within a tool of the parent agent.\\n\\n  \\'google-gla:gemini-1.5-flash\\', result_type=list[str]\\n)\\n\\n@joke_selection_agent.tool\\nasync defjoke_factory(ctx: RunContext[None], count: int) -> list[str]:\\n  r = await joke_generation_agent.run( \\nCall the delegate agent from within a tool of the parent agent.\\n\\n    f\\'Please generate {count} jokes.\\',\\n    usage=ctx.usage, \\nPass the usage from the parent agent to the delegate agent so the final result.usage() includes the usage from both agents.\\n\\n  )\\n  return r.data \\nSince the function returns list[str], and the result_type of joke_generation_agent is also list[str], we can simply return r.data from the tool.\\n\\nresult = joke_selection_agent.run_sync(\\n  \\'Tell me a joke.\\',\\n  usage_limits=UsageLimits(request_limit=5, total_tokens_limit=300),\\n)\\nprint(result.data)\\n#> Did you hear about the toothpaste scandal? They called it Colgate.\\nprint(result.usage())\\n\"\"\"\\nUsage(\\n  requests=3, request_tokens=204, response_tokens=24, total_tokens=228, details=None\\n)\\n\"\"\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 36, 'url': 'https://pydantic.com'}, page_content='graph TD\\n START --> joke_selection_agent\\n joke_selection_agent --> joke_factory[\"joke_factory (tool)\"]\\n joke_factory --> joke_generation_agent\\n joke_generation_agent --> joke_factory\\n joke_factory --> joke_selection_agent\\n joke_selection_agent --> END'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 37, 'url': 'https://pydantic.com'}, page_content='fromdataclassesimport dataclass\\nimporthttpx\\nfrompydantic_aiimport Agent, RunContext\\n\\n@dataclass\\nclassClientAndKey: \\nDefine a dataclass to hold the client and API key dependencies.\\n\\n  http_client: httpx.AsyncClient\\n  api_key: str\\n\\njoke_selection_agent = Agent(\\n  \\'openai:gpt-4o\\',\\n  deps_type=ClientAndKey, \\nSet the deps_type of the calling agent — joke_selection_agent here.\\n\\n  system_prompt=(\\n    \\'Use the `joke_factory` tool to generate some jokes on the given subject, \\'\\n    \\'then choose the best. You must return just a single joke.\\'\\n  ),\\n)\\njoke_generation_agent = Agent(\\n  \\'gemini-1.5-flash\\',\\n  deps_type=ClientAndKey, \\nAlso set the deps_type of the delegate agent — joke_generation_agent here.\\n\\n  result_type=list[str],\\n  system_prompt=(\\n    \\'Use the \"get_jokes\" tool to get some jokes on the given subject, \\'\\n    \\'then extract each joke into a list.\\'\\n  ),\\n)\\n\\n@joke_selection_agent.tool\\nasync defjoke_factory(ctx: RunContext[ClientAndKey], count: int) -> list[str]:\\n  r = await joke_generation_agent.run(\\n    f\\'Please generate {count} jokes.\\',\\n    deps=ctx.deps, \\nPass the dependencies to the delegate agent\\'s run method within the tool call.\\n\\n    usage=ctx.usage,\\n  )\\n  return r.data\\n\\n@joke_generation_agent.tool \\nDefine a tool on the delegate agent that uses the dependencies to make an HTTP request.\\n\\nasync defget_jokes(ctx: RunContext[ClientAndKey], count: int) -> str:\\n  response = await ctx.deps.http_client.get(\\n    \\'https://example.com\\',\\n    params={\\'count\\': count},\\n    headers={\\'Authorization\\': f\\'Bearer {ctx.deps.api_key}\\'},\\n  )\\n  response.raise_for_status()\\n  return response.text\\n\\nasync defmain():\\n  async with httpx.AsyncClient() as client:\\n    deps = ClientAndKey(client, \\'foobar\\')\\n    result = await joke_selection_agent.run(\\'Tell me a joke.\\', deps=deps)\\n    print(result.data)\\n    #> Did you hear about the toothpaste scandal? They called it Colgate.\\n    print(result.usage()) \\nUsage now includes 4 requests — 2 from the calling agent and 2 from the delegate agent.\\n\\n\"\"\"\\n    Usage(\\n      requests=4,\\n      request_tokens=309,\\n      response_tokens=32,\\n      total_tokens=341,\\n      details=None,\\n    )\\n    \"\"\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 37, 'url': 'https://pydantic.com'}, page_content='graph TD\\n START --> joke_selection_agent\\n joke_selection_agent --> joke_factory[\"joke_factory (tool)\"]\\n joke_factory --> joke_generation_agent\\n joke_generation_agent --> get_jokes[\"get_jokes (tool)\"]\\n get_jokes --> http_request[\"HTTP request\"]\\n http_request --> get_jokes\\n get_jokes --> joke_generation_agent\\n joke_generation_agent --> joke_factory\\n joke_factory --> joke_selection_agent\\n joke_selection_agent --> END'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 37, 'url': 'https://pydantic.com'}, page_content='fromtypingimport Literal, Union\\nfrompydanticimport BaseModel, Field\\nfromrich.promptimport Prompt\\nfrompydantic_aiimport Agent, RunContext\\nfrompydantic_ai.messagesimport ModelMessage\\nfrompydantic_ai.usageimport Usage, UsageLimits\\n\\nclassFlightDetails(BaseModel):\\n  flight_number: str\\n\\nclassFailed(BaseModel):\\n\"\"\"Unable to find a satisfactory choice.\"\"\"\\n\\nflight_search_agent = Agent[None, Union[FlightDetails, Failed]]( \\nDefine the first agent, which finds a flight. We use an explicit type annotation until PEP-747 lands, see structured results. We use a union as the result type so the model can communicate if it\\'s unable to find a satisfactory choice; internally, each member of the union will be registered as a separate tool.\\n\\n  \\'openai:gpt-4o\\',\\n  result_type=Union[FlightDetails, Failed], # type: ignore\\n  system_prompt=(\\n    \\'Use the \"flight_search\" tool to find a flight \\'\\n    \\'from the given origin to the given destination.\\'\\n  ),\\n)\\n\\n@flight_search_agent.tool \\nDefine a tool on the agent to find a flight. In this simple case we could dispense with the tool and just define the agent to return structured data, then search for a flight, but in more complex scenarios the tool would be necessary.\\n\\nasync defflight_search(\\n  ctx: RunContext[None], origin: str, destination: str\\n) -> Union[FlightDetails, None]:\\n  # in reality, this would call a flight search API or\\n  # use a browser to scrape a flight search website\\n  return FlightDetails(flight_number=\\'AK456\\')\\n\\nusage_limits = UsageLimits(request_limit=15) \\nDefine usage limits for the entire app.\\n\\nasync deffind_flight(usage: Usage) -> Union[FlightDetails, None]: \\nDefine a function to find a flight, which asks the user for their preferences and then calls the agent to find a flight.\\n\\n  message_history: Union[list[ModelMessage], None] = None\\n  for _ in range(3):\\n    prompt = Prompt.ask(\\n      \\'Where would you like to fly from and to?\\',\\n    )\\n    result = await flight_search_agent.run(\\n      prompt,\\n      message_history=message_history,\\n      usage=usage,\\n      usage_limits=usage_limits,\\n    )\\n    if isinstance(result.data, FlightDetails):\\n      return result.data\\n    else:\\n      message_history = result.all_messages(\\n        result_tool_return_content=\\'Please try again.\\'\\n      )\\n\\nclassSeatPreference(BaseModel):\\n  row: int = Field(ge=1, le=30)\\n  seat: Literal[\\'A\\', \\'B\\', \\'C\\', \\'D\\', \\'E\\', \\'F\\']\\n\\n# This agent is responsible for extracting the user\\'s seat selection\\nseat_preference_agent = Agent[None, Union[SeatPreference, Failed]]( \\nAs with flight_search_agent above, we use an explicit type annotation to define the agent.\\n\\n  \\'openai:gpt-4o\\',\\n  result_type=Union[SeatPreference, Failed], # type: ignore\\n  system_prompt=(\\n    \"Extract the user\\'s seat preference. \"\\n    \\'Seats A and F are window seats. \\'\\n    \\'Row 1 is the front row and has extra leg room. \\'\\n    \\'Rows 14, and 20 also have extra leg room. \\'\\n  ),\\n)\\n\\nasync deffind_seat(usage: Usage) -> SeatPreference: \\nDefine a function to find the user\\'s seat preference, which asks the user for their seat preference and then calls the agent to extract the seat preference.\\n\\n  message_history: Union[list[ModelMessage], None] = None\\n  while True:\\n    answer = Prompt.ask(\\'What seat would you like?\\')\\n    result = await seat_preference_agent.run(\\n      answer,\\n      message_history=message_history,\\n      usage=usage,\\n      usage_limits=usage_limits,\\n    )\\n    if isinstance(result.data, SeatPreference):\\n      return result.data\\n    else:\\n      print(\\'Could not understand seat preference. Please try again.\\')\\n      message_history = result.all_messages()\\n\\nasync defmain(): \\nNow that we\\'ve put our logic for running each agent into separate functions, our main app becomes very simple.\\n\\n  usage: Usage = Usage()\\n  opt_flight_details = await find_flight(usage)\\n  if opt_flight_details is not None:\\n    print(f\\'Flight found: {opt_flight_details.flight_number}\\')\\n    #> Flight found: AK456\\n    seat_preference = await find_seat(usage)\\n    print(f\\'Seat preference: {seat_preference}\\')\\n    #> Seat preference: row=1 seat=\\'A\\''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 37, 'url': 'https://pydantic.com'}, page_content='graph TB\\n START --> ask_user_flight[\"ask user for flight\"]\\n subgraph find_flight\\n  flight_search_agent --> ask_user_flight\\n  ask_user_flight --> flight_search_agent\\n end\\n flight_search_agent --> ask_user_seat[\"ask user for seat\"]\\n flight_search_agent --> END\\n subgraph find_seat\\n  seat_preference_agent --> ask_user_seat\\n  ask_user_seat --> seat_preference_agent\\n end\\n seat_preference_agent --> END'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 38, 'url': 'https://pydantic.com'}, page_content='frompydanticimport BaseModel\\nfrompydantic_aiimport Agent\\n\\nclassCityLocation(BaseModel):\\n  city: str\\n  country: str\\n\\nagent = Agent(\\'google-gla:gemini-1.5-flash\\', result_type=CityLocation)\\nresult = agent.run_sync(\\'Where were the olympics held in 2012?\\')\\nprint(result.data)\\n#> city=\\'London\\' country=\\'United Kingdom\\'\\nprint(result.usage())\\n\"\"\"\\nUsage(requests=1, request_tokens=57, response_tokens=8, total_tokens=65, details=None)\\n\"\"\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 39, 'url': 'https://pydantic.com'}, page_content='fromtypingimport Union\\nfrompydanticimport BaseModel\\nfrompydantic_aiimport Agent\\n\\nclassBox(BaseModel):\\n  width: int\\n  height: int\\n  depth: int\\n  units: str\\n\\nagent: Agent[None, Union[Box, str]] = Agent(\\n  \\'openai:gpt-4o-mini\\',\\n  result_type=Union[Box, str], # type: ignore\\n  system_prompt=(\\n    \"Extract me the dimensions of a box, \"\\n    \"if you can\\'t extract all data, ask the user to try again.\"\\n  ),\\n)\\nresult = agent.run_sync(\\'The box is 10x20x30\\')\\nprint(result.data)\\n#> Please provide the units for the dimensions (e.g., cm, in, m).\\nresult = agent.run_sync(\\'The box is 10x20x30 cm\\')\\nprint(result.data)\\n#> width=10 height=20 depth=30 units=\\'cm\\''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 39, 'url': 'https://pydantic.com'}, page_content=\"fromtypingimport Union\\nfrompydantic_aiimport Agent\\nagent: Agent[None, Union[list[str], list[int]]] = Agent(\\n  'openai:gpt-4o-mini',\\n  result_type=Union[list[str], list[int]], # type: ignore\\n  system_prompt='Extract either colors or sizes from the shapes provided.',\\n)\\nresult = agent.run_sync('red square, blue circle, green triangle')\\nprint(result.data)\\n#> ['red', 'blue', 'green']\\nresult = agent.run_sync('square size 10, circle size 20, triangle size 30')\\nprint(result.data)\\n#> [10, 20, 30]\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 39, 'url': 'https://pydantic.com'}, page_content=\"fromtypingimport Union\\nfromfake_databaseimport DatabaseConn, QueryError\\nfrompydanticimport BaseModel\\nfrompydantic_aiimport Agent, RunContext, ModelRetry\\n\\nclassSuccess(BaseModel):\\n  sql_query: str\\n\\nclassInvalidRequest(BaseModel):\\n  error_message: str\\n\\nResponse = Union[Success, InvalidRequest]\\nagent: Agent[DatabaseConn, Response] = Agent(\\n  'google-gla:gemini-1.5-flash',\\n  result_type=Response, # type: ignore\\n  deps_type=DatabaseConn,\\n  system_prompt='Generate PostgreSQL flavored SQL queries based on user input.',\\n)\\n\\n@agent.result_validator\\nasync defvalidate_result(ctx: RunContext[DatabaseConn], result: Response) -> Response:\\n  if isinstance(result, InvalidRequest):\\n    return result\\n  try:\\n    await ctx.deps.execute(f'EXPLAIN {result.sql_query}')\\n  except QueryError as e:\\n    raise ModelRetry(f'Invalid query: {e}') frome\\n  else:\\n    return result\\n\\nresult = agent.run_sync(\\n  'get me users who were last active yesterday.', deps=DatabaseConn()\\n)\\nprint(result.data)\\n#> sql_query='SELECT * FROM users WHERE last_active::date = today() - interval 1 day'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 39, 'url': 'https://pydantic.com'}, page_content='frompydantic_aiimport Agent\\nagent = Agent(\\'google-gla:gemini-1.5-flash\\') \\n\\nasync defmain():\\n  async with agent.run_stream(\\'Where does \"hello world\" come from?\\') as result: \\n    async for message in result.stream_text(): \\n      print(message)\\n      #> The first known\\n      #> The first known use of \"hello,\\n      #> The first known use of \"hello, world\" was in\\n      #> The first known use of \"hello, world\" was in a 1974 textbook\\n      #> The first known use of \"hello, world\" was in a 1974 textbook about the C\\n      #> The first known use of \"hello, world\" was in a 1974 textbook about the C programming language.'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 39, 'url': 'https://pydantic.com'}, page_content='frompydantic_aiimport Agent\\nagent = Agent(\\'google-gla:gemini-1.5-flash\\')\\n\\nasync defmain():\\n  async with agent.run_stream(\\'Where does \"hello world\" come from?\\') as result:\\n    async for message in result.stream_text(delta=True): \\n      print(message)\\n      #> The first known\\n      #> use of \"hello,\\n      #> world\" was in\\n      #> a 1974 textbook\\n      #> about the C\\n      #> programming language.'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 40, 'url': 'https://pydantic.com'}, page_content=\"fromdatetimeimport date\\nfromtyping_extensionsimport TypedDict\\nfrompydantic_aiimport Agent\\n\\nclassUserProfile(TypedDict, total=False):\\n  name: str\\n  dob: date\\n  bio: str\\n\\nagent = Agent(\\n  'openai:gpt-4o',\\n  result_type=UserProfile,\\n  system_prompt='Extract a user profile from the input',\\n)\\n\\nasync defmain():\\n  user_input = 'My name is Ben, I was born on January 28th 1990, I like the chain the dog and the pyramid.'\\n  async with agent.run_stream(user_input) as result:\\n    async for profile in result.stream():\\n      print(profile)\\n      #> {'name': 'Ben'}\\n      #> {'name': 'Ben'}\\n      #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes'}\\n      #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the '}\\n      #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the dog and the pyr'}\\n      #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the dog and the pyramid'}\\n      #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the dog and the pyramid'}\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 40, 'url': 'https://pydantic.com'}, page_content=\"fromdatetimeimport date\\nfrompydanticimport ValidationError\\nfromtyping_extensionsimport TypedDict\\nfrompydantic_aiimport Agent\\n\\nclassUserProfile(TypedDict, total=False):\\n  name: str\\n  dob: date\\n  bio: str\\n\\nagent = Agent('openai:gpt-4o', result_type=UserProfile)\\n\\nasync defmain():\\n  user_input = 'My name is Ben, I was born on January 28th 1990, I like the chain the dog and the pyramid.'\\n  async with agent.run_stream(user_input) as result:\\n    async for message, last in result.stream_structured(debounce_by=0.01): \\nstream_structured streams the data as ModelResponse objects, thus iteration can't fail with a ValidationError.\\n\\n      try:\\n        profile = await result.validate_structured_result( \\nvalidate_structured_result validates the data, allow_partial=True enables pydantic's experimental_allow_partial flag on TypeAdapter.\\n\\n          message,\\n          allow_partial=not last,\\n        )\\n      except ValidationError:\\n        continue\\n      print(profile)\\n      #> {'name': 'Ben'}\\n      #> {'name': 'Ben'}\\n      #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes'}\\n      #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the '}\\n      #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the dog and the pyr'}\\n      #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the dog and the pyramid'}\\n      #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the dog and the pyramid'}\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 42, 'url': 'https://pydantic.com'}, page_content='importasyncio\\nfromdatetimeimport date\\nfrompydantic_aiimport Agent, RunContext\\nfromfake_databaseimport DatabaseConn \\nDatabaseConn is a class that holds a database connection\\n\\nfromweather_serviceimport WeatherService \\nWeatherService has methods to get weather forecasts and historic data about the weather\\n\\nweather_agent = Agent(\\n  \\'openai:gpt-4o\\',\\n  deps_type=WeatherService,\\n  system_prompt=\\'Providing a weather forecast at the locations the user provides.\\',\\n)\\n\\n@weather_agent.tool\\ndefweather_forecast(\\n  ctx: RunContext[WeatherService], location: str, forecast_date: date\\n) -> str:\\n  if forecast_date < date.today(): \\nWe need to call a different endpoint depending on whether the date is in the past or the future, you\\'ll see why this nuance is important below\\n\\n    return ctx.deps.get_historic_weather(location, forecast_date)\\n  else:\\n    return ctx.deps.get_forecast(location, forecast_date)\\n\\nasync defrun_weather_forecast( \\nThis function is the code we want to test, together with the agent it uses\\n\\n  user_prompts: list[tuple[str, int]], conn: DatabaseConn\\n):\\n\"\"\"Run weather forecast for a list of user prompts and save.\"\"\"\\n  async with WeatherService() as weather_service:\\n    async defrun_forecast(prompt: str, user_id: int):\\n      result = await weather_agent.run(prompt, deps=weather_service)\\n      await conn.store_forecast(user_id, result.data)\\n    # run all prompts in parallel\\n    await asyncio.gather(\\n      *(run_forecast(prompt, user_id) for (prompt, user_id) in user_prompts)\\n    )'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 42, 'url': 'https://pydantic.com'}, page_content='fromdatetimeimport timezone\\nimportpytest\\nfromdirty_equalsimport IsNow\\nfrompydantic_aiimport models, capture_run_messages\\nfrompydantic_ai.models.testimport TestModel\\nfrompydantic_ai.messagesimport (\\n  ModelResponse,\\n  SystemPromptPart,\\n  TextPart,\\n  ToolCallPart,\\n  ToolReturnPart,\\n  UserPromptPart,\\n  ModelRequest,\\n)\\nfromfake_databaseimport DatabaseConn\\nfromweather_appimport run_weather_forecast, weather_agent\\npytestmark = pytest.mark.anyio \\nWe\\'re using anyio to run async tests.\\n\\nmodels.ALLOW_MODEL_REQUESTS = False \\nThis is a safety measure to make sure we don\\'t accidentally make real requests to the LLM while testing, see ALLOW_MODEL_REQUESTS for more details.\\n\\nasync deftest_forecast():\\n  conn = DatabaseConn()\\n  user_id = 1\\n  with capture_run_messages() as messages:\\n    with weather_agent.override(model=TestModel()): \\nWe\\'re using Agent.override to replace the agent\\'s model with TestModel, the nice thing about override is that we can replace the model inside agent without needing access to the agent run* methods call site.\\n\\n      prompt = \\'What will the weather be like in London on 2024-11-28?\\'\\n      await run_weather_forecast([(prompt, user_id)], conn) \\nNow we call the function we want to test inside the override context manager.\\n\\n  forecast = await conn.get_forecast(user_id)\\n  assert forecast == \\'{\"weather_forecast\":\"Sunny with a chance of rain\"}\\' \\nBut default, TestModel will return a JSON string summarising the tools calls made, and what was returned. If you wanted to customise the response to something more closely aligned with the domain, you could add custom_result_text=\\'Sunny\\' when defining TestModel.\\n\\n  assert messages == [ \\nSo far we don\\'t actually know which tools were called and with which values, we can use capture_run_messages to inspect messages from the most recent run and assert the exchange between the agent and the model occurred as expected.\\n\\n    ModelRequest(\\n      parts=[\\n        SystemPromptPart(\\n          content=\\'Providing a weather forecast at the locations the user provides.\\',\\n        ),\\n        UserPromptPart(\\n          content=\\'What will the weather be like in London on 2024-11-28?\\',\\n          timestamp=IsNow(tz=timezone.utc), \\nThe IsNow helper allows us to use declarative asserts even with data which will contain timestamps that change over time.\\n\\n        ),\\n      ]\\n    ),\\n    ModelResponse(\\n      parts=[\\n        ToolCallPart(\\n          tool_name=\\'weather_forecast\\',\\n          args={\\n            \\'location\\': \\'a\\',\\n            \\'forecast_date\\': \\'2024-01-01\\', \\nTestModel isn\\'t doing anything clever to extract values from the prompt, so these values are hardcoded.\\n\\n          },\\n          tool_call_id=None,\\n        )\\n      ],\\n      model_name=\\'test\\',\\n      timestamp=IsNow(tz=timezone.utc),\\n    ),\\n    ModelRequest(\\n      parts=[\\n        ToolReturnPart(\\n          tool_name=\\'weather_forecast\\',\\n          content=\\'Sunny with a chance of rain\\',\\n          tool_call_id=None,\\n          timestamp=IsNow(tz=timezone.utc),\\n        ),\\n      ],\\n    ),\\n    ModelResponse(\\n      parts=[\\n        TextPart(\\n          content=\\'{\"weather_forecast\":\"Sunny with a chance of rain\"}\\',\\n        )\\n      ],\\n      model_name=\\'test\\',\\n      timestamp=IsNow(tz=timezone.utc),\\n    ),\\n  ]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 42, 'url': 'https://pydantic.com'}, page_content=\"importre\\nimportpytest\\nfrompydantic_aiimport models\\nfrompydantic_ai.messagesimport (\\n  ModelMessage,\\n  ModelResponse,\\n  TextPart,\\n  ToolCallPart,\\n)\\nfrompydantic_ai.models.functionimport AgentInfo, FunctionModel\\nfromfake_databaseimport DatabaseConn\\nfromweather_appimport run_weather_forecast, weather_agent\\npytestmark = pytest.mark.anyio\\nmodels.ALLOW_MODEL_REQUESTS = False\\n\\ndefcall_weather_forecast( \\nWe define a function call_weather_forecast that will be called by FunctionModel in place of the LLM, this function has access to the list of ModelMessages that make up the run, and AgentInfo which contains information about the agent and the function tools and return tools.\\n\\n  messages: list[ModelMessage], info: AgentInfo\\n) -> ModelResponse:\\n  if len(messages) == 1:\\n    # first call, call the weather forecast tool\\n    user_prompt = messages[0].parts[-1]\\n    m = re.search(r'\\\\d{4}-\\\\d{2}-\\\\d{2}', user_prompt.content)\\n    assert m is not None\\n    args = {'location': 'London', 'forecast_date': m.group()} \\nOur function is slightly intelligent in that it tries to extract a date from the prompt, but just hard codes the location.\\n\\n    return ModelResponse(parts=[ToolCallPart('weather_forecast', args)])\\n  else:\\n    # second call, return the forecast\\n    msg = messages[-1].parts[0]\\n    assert msg.part_kind == 'tool-return'\\n    return ModelResponse(parts=[TextPart(f'The forecast is: {msg.content}')])\\n\\nasync deftest_forecast_future():\\n  conn = DatabaseConn()\\n  user_id = 1\\n  with weather_agent.override(model=FunctionModel(call_weather_forecast)): \\nWe use FunctionModel to replace the agent's model with our custom function.\\n\\n    prompt = 'What will the weather be like in London on 2032-01-01?'\\n    await run_weather_forecast([(prompt, user_id)], conn)\\n  forecast = await conn.get_forecast(user_id)\\n  assert forecast == 'The forecast is: Rainy with a chance of sun'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 43, 'url': 'https://pydantic.com'}, page_content='importpytest\\nfromweather_appimport weather_agent\\nfrompydantic_ai.models.testimport TestModel\\n\\n@pytest.fixture\\ndefoverride_weather_agent():\\n  with weather_agent.override(model=TestModel()):\\n    yield\\n\\nasync deftest_forecast(override_weather_agent: None):\\n  ...\\n  # test code here'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 45, 'url': 'https://pydantic.com'}, page_content='importjson\\nfrompathlibimport Path\\nfromtypingimport Union\\nfrompydantic_aiimport Agent, RunContext\\nfromfake_databaseimport DatabaseConn\\n\\nclassSqlSystemPrompt: \\nThe SqlSystemPrompt class is used to build the system prompt, it can be customised with a list of examples and a database type. We implement this as a separate class passed as a dep to the agent so we can override both the inputs and the logic during evals via dependency injection.\\n\\n  def__init__(\\n    self, examples: Union[list[dict[str, str]], None] = None, db: str = \\'PostgreSQL\\'\\n  ):\\n    if examples is None:\\n      # if examples aren\\'t provided, load them from file, this is the default\\n      with Path(\\'examples.json\\').open(\\'rb\\') as f:\\n        self.examples = json.load(f)\\n    else:\\n      self.examples = examples\\n    self.db = db\\n  defbuild_prompt(self) -> str: \\nThe build_prompt method constructs the system prompt from the examples and the database type.\\n\\n    return f\"\"\"\\\\\\nGiven the following {self.db} table of records, your job is to\\nwrite a SQL query that suits the user\\'s request.\\nDatabase schema:\\nCREATE TABLE records (\\n ...\\n);\\n{\\'\\'.join(self.format_example(example)forexampleinself.examples)}\\n\"\"\"\\n  @staticmethod\\n  defformat_example(example: dict[str, str]) -> str: \\nSome people think that LLMs are more likely to generate good responses if examples are formatted as XML as it\\'s to identify the end of a string, see #93.\\n\\n    return f\"\"\"\\\\\\n<example>\\n <request>{example[\\'request\\']}</request>\\n <sql>{example[\\'sql\\']}</sql>\\n</example>\\n\"\"\"\\n\\nsql_agent = Agent(\\n  \\'google-gla:gemini-1.5-flash\\',\\n  deps_type=SqlSystemPrompt,\\n)\\n\\n@sql_agent.system_prompt\\nasync defsystem_prompt(ctx: RunContext[SqlSystemPrompt]) -> str:\\n  return ctx.deps.build_prompt()\\n\\nasync defuser_search(user_prompt: str) -> list[dict[str, str]]:\\n\"\"\"Search the database based on the user\\'s prompts.\"\"\"\\n  ... \\nIn reality, you would have more logic here, making it impractical to run the agent independently of the wider application.\\n\\n  result = await sql_agent.run(user_prompt, deps=SqlSystemPrompt())\\n  conn = DatabaseConn()\\n  return await conn.execute(result.data)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 45, 'url': 'https://pydantic.com'}, page_content='request: show me error records with the tag \"foobar\"\\nresponse: SELECT * FROM records WHERE level = \\'error\\' and \\'foobar\\' = ANY(tags)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 45, 'url': 'https://pydantic.com'}, page_content='{\\n\"examples\":[\\n{\\n\"request\":\"Show me all records\",\\n\"sql\":\"SELECT * FROM records;\"\\n},\\n{\\n\"request\":\"Show me all records from 2021\",\\n\"sql\":\"SELECT * FROM records WHERE date_trunc(\\'year\\', date) = \\'2021-01-01\\';\"\\n},\\n{\\n\"request\":\"show me error records with the tag \\'foobar\\'\",\\n\"sql\":\"SELECT * FROM records WHERE level = \\'error\\' and \\'foobar\\' = ANY(tags);\"\\n},\\n...\\n]\\n}'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 45, 'url': 'https://pydantic.com'}, page_content=\"importjson\\nimportstatistics\\nfrompathlibimport Path\\nfromitertoolsimport chain\\nfromfake_databaseimport DatabaseConn, QueryError\\nfromsql_appimport sql_agent, SqlSystemPrompt, user_search\\n\\nasync defmain():\\n  with Path('examples.json').open('rb') as f:\\n    examples = json.load(f)\\n  # split examples into 5 folds\\n  fold_size = len(examples) // 5\\n  folds = [examples[i : i + fold_size] for i in range(0, len(examples), fold_size)]\\n  conn = DatabaseConn()\\n  scores = []\\n  for i, fold in enumerate(folds):\\n    fold_score = 0\\n    # build all other folds into a list of examples\\n    other_folds = list(chain(*(f for j, f in enumerate(folds) if j != i)))\\n    # create a new system prompt with the other fold examples\\n    system_prompt = SqlSystemPrompt(examples=other_folds)\\n    # override the system prompt with the new one\\n    with sql_agent.override(deps=system_prompt):\\n      for case in fold:\\n        try:\\n          agent_results = await user_search(case['request'])\\n        except QueryError as e:\\n          print(f'Fold {i}{case}: {e}')\\n          fold_score -= 100\\n        else:\\n          # get the expected results using the SQL from this case\\n          expected_results = await conn.execute(case['sql'])\\n        agent_ids = [r['id'] for r in agent_results]\\n        # each returned value has a score of -1\\n        fold_score -= len(agent_ids)\\n        expected_ids = {r['id'] for r in expected_results}\\n        # each return value that matches the expected value has a score of 3\\n        fold_score += 5 * len(set(agent_ids) & expected_ids)\\n    scores.append(fold_score)\\n  overall_score = statistics.mean(scores)\\n  print(f'Overall score: {overall_score:0.2f}')\\n  #> Overall score: 12.00\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 46, 'url': 'https://pydantic.com'}, page_content='importrandom\\nfrompydantic_aiimport Agent, RunContext\\nagent = Agent(\\n  \\'google-gla:gemini-1.5-flash\\', \\nThis is a pretty simple task, so we can use the fast and cheap Gemini flash model.\\n\\n  deps_type=str, \\nWe pass the user\\'s name as the dependency, to keep things simple we use just the name as a string as the dependency.\\n\\n  system_prompt=(\\n    \"You\\'re a dice game, you should roll the die and see if the number \"\\n    \"you get back matches the user\\'s guess. If so, tell them they\\'re a winner. \"\\n    \"Use the player\\'s name in the response.\"\\n  ),\\n)\\n\\n@agent.tool_plain \\nThis tool doesn\\'t need any context, it just returns a random number. You could probably use a dynamic system prompt in this case.\\n\\ndefroll_die() -> str:\\n\"\"\"Roll a six-sided die and return the result.\"\"\"\\n  return str(random.randint(1, 6))\\n\\n@agent.tool \\nThis tool needs the player\\'s name, so it uses RunContext to access dependencies which are just the player\\'s name in this case.\\n\\ndefget_player_name(ctx: RunContext[str]) -> str:\\n\"\"\"Get the player\\'s name.\"\"\"\\n  return ctx.deps\\n\\ndice_result = agent.run_sync(\\'My guess is 4\\', deps=\\'Anne\\') \\nRun the agent, passing the player\\'s name as the dependency.\\n\\nprint(dice_result.data)\\n#> Congratulations Anne, you guessed correctly! You\\'re a winner!'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 46, 'url': 'https://pydantic.com'}, page_content='fromdice_gameimport dice_result\\nprint(dice_result.all_messages())\\n\"\"\"\\n[\\n  ModelRequest(\\n    parts=[\\n      SystemPromptPart(\\n        content=\"You\\'re a dice game, you should roll the die and see if the number you get back matches the user\\'s guess. If so, tell them they\\'re a winner. Use the player\\'s name in the response.\",\\n        dynamic_ref=None,\\n        part_kind=\\'system-prompt\\',\\n      ),\\n      UserPromptPart(\\n        content=\\'My guess is 4\\',\\n        timestamp=datetime.datetime(...),\\n        part_kind=\\'user-prompt\\',\\n      ),\\n    ],\\n    kind=\\'request\\',\\n  ),\\n  ModelResponse(\\n    parts=[\\n      ToolCallPart(\\n        tool_name=\\'roll_die\\', args={}, tool_call_id=None, part_kind=\\'tool-call\\'\\n      )\\n    ],\\n    model_name=\\'function:model_logic\\',\\n    timestamp=datetime.datetime(...),\\n    kind=\\'response\\',\\n  ),\\n  ModelRequest(\\n    parts=[\\n      ToolReturnPart(\\n        tool_name=\\'roll_die\\',\\n        content=\\'4\\',\\n        tool_call_id=None,\\n        timestamp=datetime.datetime(...),\\n        part_kind=\\'tool-return\\',\\n      )\\n    ],\\n    kind=\\'request\\',\\n  ),\\n  ModelResponse(\\n    parts=[\\n      ToolCallPart(\\n        tool_name=\\'get_player_name\\',\\n        args={},\\n        tool_call_id=None,\\n        part_kind=\\'tool-call\\',\\n      )\\n    ],\\n    model_name=\\'function:model_logic\\',\\n    timestamp=datetime.datetime(...),\\n    kind=\\'response\\',\\n  ),\\n  ModelRequest(\\n    parts=[\\n      ToolReturnPart(\\n        tool_name=\\'get_player_name\\',\\n        content=\\'Anne\\',\\n        tool_call_id=None,\\n        timestamp=datetime.datetime(...),\\n        part_kind=\\'tool-return\\',\\n      )\\n    ],\\n    kind=\\'request\\',\\n  ),\\n  ModelResponse(\\n    parts=[\\n      TextPart(\\n        content=\"Congratulations Anne, you guessed correctly! You\\'re a winner!\",\\n        part_kind=\\'text\\',\\n      )\\n    ],\\n    model_name=\\'function:model_logic\\',\\n    timestamp=datetime.datetime(...),\\n    kind=\\'response\\',\\n  ),\\n]\\n\"\"\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 46, 'url': 'https://pydantic.com'}, page_content='sequenceDiagram\\n  participant Agent\\n  participant LLM\\n  Note over Agent: Send prompts\\n  Agent ->> LLM: System: \"You\\'re a dice game...\"<br>User: \"My guess is 4\"\\n  activate LLM\\n  Note over LLM: LLM decides to use<br>a tool\\n  LLM ->> Agent: Call tool<br>roll_die()\\n  deactivate LLM\\n  activate Agent\\n  Note over Agent: Rolls a six-sided die\\n  Agent -->> LLM: ToolReturn<br>\"4\"\\n  deactivate Agent\\n  activate LLM\\n  Note over LLM: LLM decides to use<br>another tool\\n  LLM ->> Agent: Call tool<br>get_player_name()\\n  deactivate LLM\\n  activate Agent\\n  Note over Agent: Retrieves player name\\n  Agent -->> LLM: ToolReturn<br>\"Anne\"\\n  deactivate Agent\\n  activate LLM\\n  Note over LLM: LLM constructs final response\\n  LLM ->> Agent: ModelResponse<br>\"Congratulations Anne, ...\"\\n  deactivate LLM\\n  Note over Agent: Game session complete'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 47, 'url': 'https://pydantic.com'}, page_content='importrandom\\nfrompydantic_aiimport Agent, RunContext, Tool\\n\\ndefroll_die() -> str:\\n\"\"\"Roll a six-sided die and return the result.\"\"\"\\n  return str(random.randint(1, 6))\\n\\ndefget_player_name(ctx: RunContext[str]) -> str:\\n\"\"\"Get the player\\'s name.\"\"\"\\n  return ctx.deps\\n\\nagent_a = Agent(\\n  \\'google-gla:gemini-1.5-flash\\',\\n  deps_type=str,\\n  tools=[roll_die, get_player_name], \\nThe simplest way to register tools via the Agent constructor is to pass a list of functions, the function signature is inspected to determine if the tool takes RunContext.\\n\\n)\\nagent_b = Agent(\\n  \\'google-gla:gemini-1.5-flash\\',\\n  deps_type=str,\\n  tools=[ \\nagent_a and agent_b are identical — but we can use Tool to reuse tool definitions and give more fine-grained control over how tools are defined, e.g. setting their name or description, or using a custom prepare method.\\n\\n    Tool(roll_die, takes_ctx=False),\\n    Tool(get_player_name, takes_ctx=True),\\n  ],\\n)\\ndice_result = agent_b.run_sync(\\'My guess is 4\\', deps=\\'Anne\\')\\nprint(dice_result.data)\\n#> Congratulations Anne, you guessed correctly! You\\'re a winner!'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 47, 'url': 'https://pydantic.com'}, page_content='frompydantic_aiimport Agent\\nfrompydantic_ai.messagesimport ModelMessage, ModelResponse, TextPart\\nfrompydantic_ai.models.functionimport AgentInfo, FunctionModel\\nagent = Agent()\\n\\n@agent.tool_plain(docstring_format=\\'google\\', require_parameter_descriptions=True)\\ndeffoobar(a: int, b: str, c: dict[str, list[float]]) -> str:\\n\"\"\"Get me foobar.\\n  Args:\\n    a: apple pie\\n    b: banana cake\\n    c: carrot smoothie\\n  \"\"\"\\n  return f\\'{a}{b}{c}\\'\\n\\ndefprint_schema(messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:\\n  tool = info.function_tools[0]\\n  print(tool.description)\\n  #> Get me foobar.\\n  print(tool.parameters_json_schema)\\n\"\"\"\\n  {\\n    \\'properties\\': {\\n      \\'a\\': {\\'description\\': \\'apple pie\\', \\'title\\': \\'A\\', \\'type\\': \\'integer\\'},\\n      \\'b\\': {\\'description\\': \\'banana cake\\', \\'title\\': \\'B\\', \\'type\\': \\'string\\'},\\n      \\'c\\': {\\n        \\'additionalProperties\\': {\\'items\\': {\\'type\\': \\'number\\'}, \\'type\\': \\'array\\'},\\n        \\'description\\': \\'carrot smoothie\\',\\n        \\'title\\': \\'C\\',\\n        \\'type\\': \\'object\\',\\n      },\\n    },\\n    \\'required\\': [\\'a\\', \\'b\\', \\'c\\'],\\n    \\'type\\': \\'object\\',\\n    \\'additionalProperties\\': False,\\n  }\\n  \"\"\"\\n  return ModelResponse(parts=[TextPart(\\'foobar\\')])\\n\\nagent.run_sync(\\'hello\\', model=FunctionModel(print_schema))'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 48, 'url': 'https://pydantic.com'}, page_content='frompydanticimport BaseModel\\nfrompydantic_aiimport Agent\\nfrompydantic_ai.models.testimport TestModel\\nagent = Agent()\\n\\nclassFoobar(BaseModel):\\n\"\"\"This is a Foobar\"\"\"\\n  x: int\\n  y: str\\n  z: float = 3.14\\n\\n@agent.tool_plain\\ndeffoobar(f: Foobar) -> str:\\n  return str(f)\\n\\ntest_model = TestModel()\\nresult = agent.run_sync(\\'hello\\', model=test_model)\\nprint(result.data)\\n#> {\"foobar\":\"x=0 y=\\'a\\' z=3.14\"}\\nprint(test_model.last_model_request_parameters.function_tools)\\n\"\"\"\\n[\\n  ToolDefinition(\\n    name=\\'foobar\\',\\n    description=\\'This is a Foobar\\',\\n    parameters_json_schema={\\n      \\'properties\\': {\\n        \\'x\\': {\\'title\\': \\'X\\', \\'type\\': \\'integer\\'},\\n        \\'y\\': {\\'title\\': \\'Y\\', \\'type\\': \\'string\\'},\\n        \\'z\\': {\\'default\\': 3.14, \\'title\\': \\'Z\\', \\'type\\': \\'number\\'},\\n      },\\n      \\'required\\': [\\'x\\', \\'y\\'],\\n      \\'title\\': \\'Foobar\\',\\n      \\'type\\': \\'object\\',\\n    },\\n    outer_typed_dict_key=None,\\n  )\\n]\\n\"\"\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 48, 'url': 'https://pydantic.com'}, page_content='fromtypingimport Union\\nfrompydantic_aiimport Agent, RunContext\\nfrompydantic_ai.toolsimport ToolDefinition\\nagent = Agent(\\'test\\')\\n\\nasync defonly_if_42(\\n  ctx: RunContext[int], tool_def: ToolDefinition\\n) -> Union[ToolDefinition, None]:\\n  if ctx.deps == 42:\\n    return tool_def\\n\\n@agent.tool(prepare=only_if_42)\\ndefhitchhiker(ctx: RunContext[int], answer: str) -> str:\\n  return f\\'{ctx.deps}{answer}\\'\\n\\nresult = agent.run_sync(\\'testing...\\', deps=41)\\nprint(result.data)\\n#> success (no tool calls)\\nresult = agent.run_sync(\\'testing...\\', deps=42)\\nprint(result.data)\\n#> {\"hitchhiker\":\"42 a\"}'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 48, 'url': 'https://pydantic.com'}, page_content='from__future__import annotations\\nfromtypingimport Literal\\nfrompydantic_aiimport Agent, RunContext\\nfrompydantic_ai.models.testimport TestModel\\nfrompydantic_ai.toolsimport Tool, ToolDefinition\\n\\ndefgreet(name: str) -> str:\\n  return f\\'hello {name}\\'\\n\\nasync defprepare_greet(\\n  ctx: RunContext[Literal[\\'human\\', \\'machine\\']], tool_def: ToolDefinition\\n) -> ToolDefinition | None:\\n  d = f\\'Name of the {ctx.deps} to greet.\\'\\n  tool_def.parameters_json_schema[\\'properties\\'][\\'name\\'][\\'description\\'] = d\\n  return tool_def\\n\\ngreet_tool = Tool(greet, prepare=prepare_greet)\\ntest_model = TestModel()\\nagent = Agent(test_model, tools=[greet_tool], deps_type=Literal[\\'human\\', \\'machine\\'])\\nresult = agent.run_sync(\\'testing...\\', deps=\\'human\\')\\nprint(result.data)\\n#> {\"greet\":\"hello a\"}\\nprint(test_model.last_model_request_parameters.function_tools)\\n\"\"\"\\n[\\n  ToolDefinition(\\n    name=\\'greet\\',\\n    description=\\'\\',\\n    parameters_json_schema={\\n      \\'properties\\': {\\n        \\'name\\': {\\n          \\'title\\': \\'Name\\',\\n          \\'type\\': \\'string\\',\\n          \\'description\\': \\'Name of the human to greet.\\',\\n        }\\n      },\\n      \\'required\\': [\\'name\\'],\\n      \\'type\\': \\'object\\',\\n      \\'additionalProperties\\': False,\\n    },\\n    outer_typed_dict_key=None,\\n  )\\n]\\n\"\"\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 49, 'url': 'https://pydantic.com'}, page_content='importnest_asyncio\\nnest_asyncio.apply()'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 50, 'url': 'https://pydantic.com'}, page_content=\"frompydantic_aiimport Agent\\nagent = Agent('openai:gpt-4o')\\nresult = agent.run_sync('What is the capital of France?')\\nprint(result.data)\\n#> Paris\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 51, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 51, 'url': 'https://pydantic.com'}, page_content='@final\\n@dataclasses.dataclass(init=False)\\nclassAgent(Generic[AgentDepsT, ResultDataT]):\\n\"\"\"Class for defining \"agents\" - a way to have a specific type of \"conversation\" with an LLM.\\n  Agents are generic in the dependency type they take [`AgentDepsT`][pydantic_ai.tools.AgentDepsT]\\n  and the result data type they return, [`ResultDataT`][pydantic_ai.result.ResultDataT].\\n  By default, if neither generic parameter is customised, agents have type `Agent[None, str]`.\\n  Minimal usage example:'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 51, 'url': 'https://pydantic.com'}, page_content='\"\"\"\\n  # we use dataclass fields in order to conveniently know what attributes are available\\n  model: models.Model | models.KnownModelName | None\\n\"\"\"The default model configured for this agent.\"\"\"\\n  name: str | None\\n\"\"\"The name of the agent, used for logging.\\n  If `None`, we try to infer the agent name from the call frame when the agent is first run.\\n  \"\"\"\\n  end_strategy: EndStrategy\\n\"\"\"Strategy for handling tool calls when a final result is found.\"\"\"\\n  model_settings: ModelSettings | None\\n\"\"\"Optional model request settings to use for this agents\\'s runs, by default.\\n  Note, if `model_settings` is provided by `run`, `run_sync`, or `run_stream`, those settings will\\n  be merged with this value, with the runtime argument taking priority.\\n  \"\"\"\\n  result_type: type[ResultDataT] = dataclasses.field(repr=False)\\n\"\"\"\\n  The type of the result data, used to validate the result data, defaults to `str`.\\n  \"\"\"\\n  _deps_type: type[AgentDepsT] = dataclasses.field(repr=False)\\n  _result_tool_name: str = dataclasses.field(repr=False)\\n  _result_tool_description: str | None = dataclasses.field(repr=False)\\n  _result_schema: _result.ResultSchema[ResultDataT] | None = dataclasses.field(repr=False)\\n  _result_validators: list[_result.ResultValidator[AgentDepsT, ResultDataT]] = dataclasses.field(repr=False)\\n  _system_prompts: tuple[str, ...] = dataclasses.field(repr=False)\\n  _system_prompt_functions: list[_system_prompt.SystemPromptRunner[AgentDepsT]] = dataclasses.field(repr=False)\\n  _system_prompt_dynamic_functions: dict[str, _system_prompt.SystemPromptRunner[AgentDepsT]] = dataclasses.field(\\n    repr=False\\n  )\\n  _function_tools: dict[str, Tool[AgentDepsT]] = dataclasses.field(repr=False)\\n  _default_retries: int = dataclasses.field(repr=False)\\n  _max_result_retries: int = dataclasses.field(repr=False)\\n  _override_deps: _utils.Option[AgentDepsT] = dataclasses.field(default=None, repr=False)\\n  _override_model: _utils.Option[models.Model] = dataclasses.field(default=None, repr=False)\\n  def__init__(\\n    self,\\n    model: models.Model | models.KnownModelName | None = None,\\n    *,\\n    result_type: type[ResultDataT] = str,\\n    system_prompt: str | Sequence[str] = (),\\n    deps_type: type[AgentDepsT] = NoneType,\\n    name: str | None = None,\\n    model_settings: ModelSettings | None = None,\\n    retries: int = 1,\\n    result_tool_name: str = \\'final_result\\',\\n    result_tool_description: str | None = None,\\n    result_retries: int | None = None,\\n    tools: Sequence[Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]] = (),\\n    defer_model_check: bool = False,\\n    end_strategy: EndStrategy = \\'early\\',\\n  ):\\n\"\"\"Create an agent.\\n    Args:\\n      model: The default model to use for this agent, if not provide,\\n        you must provide the model when calling it.\\n      result_type: The type of the result data, used to validate the result data, defaults to `str`.\\n      system_prompt: Static system prompts to use for this agent, you can also register system\\n        prompts via a function with [`system_prompt`][pydantic_ai.Agent.system_prompt].\\n      deps_type: The type used for dependency injection, this parameter exists solely to allow you to fully\\n        parameterize the agent, and therefore get the best out of static type checking.\\n        If you\\'re not using deps, but want type checking to pass, you can set `deps=None` to satisfy Pyright\\n        or add a type hint `: Agent[None, <return type>]`.\\n      name: The name of the agent, used for logging. If `None`, we try to infer the agent name from the call frame\\n        when the agent is first run.\\n      model_settings: Optional model request settings to use for this agent\\'s runs, by default.\\n      retries: The default number of retries to allow before raising an error.\\n      result_tool_name: The name of the tool to use for the final result.\\n      result_tool_description: The description of the final result tool.\\n      result_retries: The maximum number of retries to allow for result validation, defaults to `retries`.\\n      tools: Tools to register with the agent, you can also register tools via the decorators\\n        [`@agent.tool`][pydantic_ai.Agent.tool] and [`@agent.tool_plain`][pydantic_ai.Agent.tool_plain].\\n      defer_model_check: by default, if you provide a [named][pydantic_ai.models.KnownModelName] model,\\n        it\\'s evaluated to create a [`Model`][pydantic_ai.models.Model] instance immediately,\\n        which checks for the necessary environment variables. Set this to `false`\\n        to defer the evaluation until the first run. Useful if you want to\\n        [override the model][pydantic_ai.Agent.override] for testing.\\n      end_strategy: Strategy for handling tool calls that are requested alongside a final result.\\n        See [`EndStrategy`][pydantic_ai.agent.EndStrategy] for more information.\\n    \"\"\"\\n    if model is None or defer_model_check:\\n      self.model = model\\n    else:\\n      self.model = models.infer_model(model)\\n    self.end_strategy = end_strategy\\n    self.name = name\\n    self.model_settings = model_settings\\n    self.result_type = result_type\\n    self._deps_type = deps_type\\n    self._result_tool_name = result_tool_name\\n    self._result_tool_description = result_tool_description\\n    self._result_schema: _result.ResultSchema[ResultDataT] | None = _result.ResultSchema[result_type].build(\\n      result_type, result_tool_name, result_tool_description\\n    )\\n    self._result_validators: list[_result.ResultValidator[AgentDepsT, ResultDataT]] = []\\n    self._system_prompts = (system_prompt,) if isinstance(system_prompt, str) else tuple(system_prompt)\\n    self._system_prompt_functions: list[_system_prompt.SystemPromptRunner[AgentDepsT]] = []\\n    self._system_prompt_dynamic_functions: dict[str, _system_prompt.SystemPromptRunner[AgentDepsT]] = {}\\n    self._function_tools: dict[str, Tool[AgentDepsT]] = {}\\n    self._default_retries = retries\\n    self._max_result_retries = result_retries if result_retries is not None else retries\\n    for tool in tools:\\n      if isinstance(tool, Tool):\\n        self._register_tool(tool)\\n      else:\\n        self._register_tool(Tool(tool))\\n  @overload\\n  async defrun(\\n    self,\\n    user_prompt: str,\\n    *,\\n    result_type: None = None,\\n    message_history: list[_messages.ModelMessage] | None = None,\\n    model: models.Model | models.KnownModelName | None = None,\\n    deps: AgentDepsT = None,\\n    model_settings: ModelSettings | None = None,\\n    usage_limits: _usage.UsageLimits | None = None,\\n    usage: _usage.Usage | None = None,\\n    infer_name: bool = True,\\n  ) -> AgentRunResult[ResultDataT]: ...\\n  @overload\\n  async defrun(\\n    self,\\n    user_prompt: str,\\n    *,\\n    result_type: type[RunResultDataT],\\n    message_history: list[_messages.ModelMessage] | None = None,\\n    model: models.Model | models.KnownModelName | None = None,\\n    deps: AgentDepsT = None,\\n    model_settings: ModelSettings | None = None,\\n    usage_limits: _usage.UsageLimits | None = None,\\n    usage: _usage.Usage | None = None,\\n    infer_name: bool = True,\\n  ) -> AgentRunResult[RunResultDataT]: ...\\n  async defrun(\\n    self,\\n    user_prompt: str,\\n    *,\\n    result_type: type[RunResultDataT] | None = None,\\n    message_history: list[_messages.ModelMessage] | None = None,\\n    model: models.Model | models.KnownModelName | None = None,\\n    deps: AgentDepsT = None,\\n    model_settings: ModelSettings | None = None,\\n    usage_limits: _usage.UsageLimits | None = None,\\n    usage: _usage.Usage | None = None,\\n    infer_name: bool = True,\\n  ) -> AgentRunResult[Any]:\\n\"\"\"Run the agent with a user prompt in async mode.\\n    This method builds an internal agent graph (using system prompts, tools and result schemas) and then\\n    runs the graph to completion. The result of the run is returned.\\n    Example:'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 51, 'url': 'https://pydantic.com'}, page_content='Args:\\n      user_prompt: User input to start/continue the conversation.\\n      result_type: Custom result type to use for this run, `result_type` may only be used if the agent has no\\n        result validators since result validators would expect an argument that matches the agent\\'s result type.\\n      message_history: History of the conversation so far.\\n      model: Optional model to use for this run, required if `model` was not set when creating the agent.\\n      deps: Optional dependencies to use for this run.\\n      model_settings: Optional settings to use for this model\\'s request.\\n      usage_limits: Optional limits on model request count or token usage.\\n      usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\\n      infer_name: Whether to try to infer the agent name from the call frame if it\\'s not set.\\n    Returns:\\n      The result of the run.\\n    \"\"\"\\n    if infer_name and self.name is None:\\n      self._infer_name(inspect.currentframe())\\n    with self.iter(\\n      user_prompt=user_prompt,\\n      result_type=result_type,\\n      message_history=message_history,\\n      model=model,\\n      deps=deps,\\n      model_settings=model_settings,\\n      usage_limits=usage_limits,\\n      usage=usage,\\n    ) as agent_run:\\n      async for _ in agent_run:\\n        pass\\n    assert (final_result := agent_run.result) is not None, \\'The graph run did not finish properly\\'\\n    return final_result\\n  @contextmanager\\n  defiter(\\n    self,\\n    user_prompt: str,\\n    *,\\n    result_type: type[RunResultDataT] | None = None,\\n    message_history: list[_messages.ModelMessage] | None = None,\\n    model: models.Model | models.KnownModelName | None = None,\\n    deps: AgentDepsT = None,\\n    model_settings: ModelSettings | None = None,\\n    usage_limits: _usage.UsageLimits | None = None,\\n    usage: _usage.Usage | None = None,\\n    infer_name: bool = True,\\n  ) -> Iterator[AgentRun[AgentDepsT, Any]]:\\n\"\"\"A contextmanager which can be used to iterate over the agent graph\\'s nodes as they are executed.\\n    This method builds an internal agent graph (using system prompts, tools and result schemas) and then returns an\\n    `AgentRun` object. The `AgentRun` can be used to async-iterate over the nodes of the graph as they are\\n    executed. This is the API to use if you want to consume the outputs coming from each LLM model response, or the\\n    stream of events coming from the execution of tools.\\n    The `AgentRun` also provides methods to access the full message history, new messages, and usage statistics,\\n    and the final result of the run once it has completed.\\n    For more details, see the documentation of `AgentRun`.\\n    Example:'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 51, 'url': 'https://pydantic.com'}, page_content='Args:\\n      user_prompt: User input to start/continue the conversation.\\n      result_type: Custom result type to use for this run, `result_type` may only be used if the agent has no\\n        result validators since result validators would expect an argument that matches the agent\\'s result type.\\n      message_history: History of the conversation so far.\\n      model: Optional model to use for this run, required if `model` was not set when creating the agent.\\n      deps: Optional dependencies to use for this run.\\n      model_settings: Optional settings to use for this model\\'s request.\\n      usage_limits: Optional limits on model request count or token usage.\\n      usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\\n      infer_name: Whether to try to infer the agent name from the call frame if it\\'s not set.\\n    Returns:\\n      The result of the run.\\n    \"\"\"\\n    if infer_name and self.name is None:\\n      self._infer_name(inspect.currentframe())\\n    model_used = self._get_model(model)\\n    deps = self._get_deps(deps)\\n    new_message_index = len(message_history) if message_history else 0\\n    result_schema: _result.ResultSchema[RunResultDataT] | None = self._prepare_result_schema(result_type)\\n    # Build the graph\\n    graph = self._build_graph(result_type)\\n    # Build the initial state\\n    state = _agent_graph.GraphAgentState(\\n      message_history=message_history[:] if message_history else [],\\n      usage=usage or _usage.Usage(),\\n      retries=0,\\n      run_step=0,\\n    )\\n    # We consider it a user error if a user tries to restrict the result type while having a result validator that\\n    # may change the result type from the restricted type to something else. Therefore, we consider the following\\n    # typecast reasonable, even though it is possible to violate it with otherwise-type-checked code.\\n    result_validators = cast(list[_result.ResultValidator[AgentDepsT, RunResultDataT]], self._result_validators)\\n    # TODO: Instead of this, copy the function tools to ensure they don\\'t share current_retry state between agent\\n    # runs. Requires some changes to `Tool` to make them copyable though.\\n    for v in self._function_tools.values():\\n      v.current_retry = 0\\n    model_settings = merge_model_settings(self.model_settings, model_settings)\\n    usage_limits = usage_limits or _usage.UsageLimits()\\n    # Build the deps object for the graph\\n    run_span = _logfire.span(\\n      \\'{agent_name} run {prompt=}\\',\\n      prompt=user_prompt,\\n      agent=self,\\n      model_name=model_used.model_name if model_used else \\'no-model\\',\\n      agent_name=self.name or \\'agent\\',\\n    )\\n    graph_deps = _agent_graph.GraphAgentDeps[AgentDepsT, RunResultDataT](\\n      user_deps=deps,\\n      prompt=user_prompt,\\n      new_message_index=new_message_index,\\n      model=model_used,\\n      model_settings=model_settings,\\n      usage_limits=usage_limits,\\n      max_result_retries=self._max_result_retries,\\n      end_strategy=self.end_strategy,\\n      result_schema=result_schema,\\n      result_tools=self._result_schema.tool_defs() if self._result_schema else [],\\n      result_validators=result_validators,\\n      function_tools=self._function_tools,\\n      run_span=run_span,\\n    )\\n    start_node = _agent_graph.UserPromptNode[AgentDepsT](\\n      user_prompt=user_prompt,\\n      system_prompts=self._system_prompts,\\n      system_prompt_functions=self._system_prompt_functions,\\n      system_prompt_dynamic_functions=self._system_prompt_dynamic_functions,\\n    )\\n    with graph.iter(\\n      start_node,\\n      state=state,\\n      deps=graph_deps,\\n      infer_name=False,\\n      span=run_span,\\n    ) as graph_run:\\n      yield AgentRun(graph_run)\\n  @overload\\n  defrun_sync(\\n    self,\\n    user_prompt: str,\\n    *,\\n    message_history: list[_messages.ModelMessage] | None = None,\\n    model: models.Model | models.KnownModelName | None = None,\\n    deps: AgentDepsT = None,\\n    model_settings: ModelSettings | None = None,\\n    usage_limits: _usage.UsageLimits | None = None,\\n    usage: _usage.Usage | None = None,\\n    infer_name: bool = True,\\n  ) -> AgentRunResult[ResultDataT]: ...\\n  @overload\\n  defrun_sync(\\n    self,\\n    user_prompt: str,\\n    *,\\n    result_type: type[RunResultDataT] | None,\\n    message_history: list[_messages.ModelMessage] | None = None,\\n    model: models.Model | models.KnownModelName | None = None,\\n    deps: AgentDepsT = None,\\n    model_settings: ModelSettings | None = None,\\n    usage_limits: _usage.UsageLimits | None = None,\\n    usage: _usage.Usage | None = None,\\n    infer_name: bool = True,\\n  ) -> AgentRunResult[RunResultDataT]: ...\\n  defrun_sync(\\n    self,\\n    user_prompt: str,\\n    *,\\n    result_type: type[RunResultDataT] | None = None,\\n    message_history: list[_messages.ModelMessage] | None = None,\\n    model: models.Model | models.KnownModelName | None = None,\\n    deps: AgentDepsT = None,\\n    model_settings: ModelSettings | None = None,\\n    usage_limits: _usage.UsageLimits | None = None,\\n    usage: _usage.Usage | None = None,\\n    infer_name: bool = True,\\n  ) -> AgentRunResult[Any]:\\n\"\"\"Synchronously run the agent with a user prompt.\\n    This is a convenience method that wraps [`self.run`][pydantic_ai.Agent.run] with `loop.run_until_complete(...)`.\\n    You therefore can\\'t use this method inside async code or if there\\'s an active event loop.\\n    Example:'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 51, 'url': 'https://pydantic.com'}, page_content='Args:\\n      user_prompt: User input to start/continue the conversation.\\n      result_type: Custom result type to use for this run, `result_type` may only be used if the agent has no\\n        result validators since result validators would expect an argument that matches the agent\\'s result type.\\n      message_history: History of the conversation so far.\\n      model: Optional model to use for this run, required if `model` was not set when creating the agent.\\n      deps: Optional dependencies to use for this run.\\n      model_settings: Optional settings to use for this model\\'s request.\\n      usage_limits: Optional limits on model request count or token usage.\\n      usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\\n      infer_name: Whether to try to infer the agent name from the call frame if it\\'s not set.\\n    Returns:\\n      The result of the run.\\n    \"\"\"\\n    if infer_name and self.name is None:\\n      self._infer_name(inspect.currentframe())\\n    return asyncio.get_event_loop().run_until_complete(\\n      self.run(\\n        user_prompt,\\n        result_type=result_type,\\n        message_history=message_history,\\n        model=model,\\n        deps=deps,\\n        model_settings=model_settings,\\n        usage_limits=usage_limits,\\n        usage=usage,\\n        infer_name=False,\\n      )\\n    )\\n  @overload\\n  defrun_stream(\\n    self,\\n    user_prompt: str,\\n    *,\\n    result_type: None = None,\\n    message_history: list[_messages.ModelMessage] | None = None,\\n    model: models.Model | models.KnownModelName | None = None,\\n    deps: AgentDepsT = None,\\n    model_settings: ModelSettings | None = None,\\n    usage_limits: _usage.UsageLimits | None = None,\\n    usage: _usage.Usage | None = None,\\n    infer_name: bool = True,\\n  ) -> AbstractAsyncContextManager[result.StreamedRunResult[AgentDepsT, ResultDataT]]: ...\\n  @overload\\n  defrun_stream(\\n    self,\\n    user_prompt: str,\\n    *,\\n    result_type: type[RunResultDataT],\\n    message_history: list[_messages.ModelMessage] | None = None,\\n    model: models.Model | models.KnownModelName | None = None,\\n    deps: AgentDepsT = None,\\n    model_settings: ModelSettings | None = None,\\n    usage_limits: _usage.UsageLimits | None = None,\\n    usage: _usage.Usage | None = None,\\n    infer_name: bool = True,\\n  ) -> AbstractAsyncContextManager[result.StreamedRunResult[AgentDepsT, RunResultDataT]]: ...\\n  @asynccontextmanager\\n  async defrun_stream( # noqa C901\\n    self,\\n    user_prompt: str,\\n    *,\\n    result_type: type[RunResultDataT] | None = None,\\n    message_history: list[_messages.ModelMessage] | None = None,\\n    model: models.Model | models.KnownModelName | None = None,\\n    deps: AgentDepsT = None,\\n    model_settings: ModelSettings | None = None,\\n    usage_limits: _usage.UsageLimits | None = None,\\n    usage: _usage.Usage | None = None,\\n    infer_name: bool = True,\\n  ) -> AsyncIterator[result.StreamedRunResult[AgentDepsT, Any]]:\\n\"\"\"Run the agent with a user prompt in async mode, returning a streamed response.\\n    Example:'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 51, 'url': 'https://pydantic.com'}, page_content='Args:\\n      user_prompt: User input to start/continue the conversation.\\n      result_type: Custom result type to use for this run, `result_type` may only be used if the agent has no\\n        result validators since result validators would expect an argument that matches the agent\\'s result type.\\n      message_history: History of the conversation so far.\\n      model: Optional model to use for this run, required if `model` was not set when creating the agent.\\n      deps: Optional dependencies to use for this run.\\n      model_settings: Optional settings to use for this model\\'s request.\\n      usage_limits: Optional limits on model request count or token usage.\\n      usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\\n      infer_name: Whether to try to infer the agent name from the call frame if it\\'s not set.\\n    Returns:\\n      The result of the run.\\n    \"\"\"\\n    # TODO: We need to deprecate this now that we have the `iter` method.\\n    #  Before that, though, we should add an event for when we reach the final result of the stream.\\n    if infer_name and self.name is None:\\n      # f_back because `asynccontextmanager` adds one frame\\n      if frame := inspect.currentframe(): # pragma: no branch\\n        self._infer_name(frame.f_back)\\n    yielded = False\\n    with self.iter(\\n      user_prompt,\\n      result_type=result_type,\\n      message_history=message_history,\\n      model=model,\\n      deps=deps,\\n      model_settings=model_settings,\\n      usage_limits=usage_limits,\\n      usage=usage,\\n      infer_name=False,\\n    ) as agent_run:\\n      first_node = agent_run.next_node # start with the first node\\n      assert isinstance(first_node, _agent_graph.UserPromptNode) # the first node should be a user prompt node\\n      node: BaseNode[Any, Any, Any] = cast(BaseNode[Any, Any, Any], first_node)\\n      while True:\\n        if isinstance(node, _agent_graph.ModelRequestNode):\\n          node = cast(_agent_graph.ModelRequestNode[AgentDepsT, Any], node)\\n          graph_ctx = agent_run.ctx\\n          async with node._stream(graph_ctx) as streamed_response: # pyright: ignore[reportPrivateUsage]\\n            async defstream_to_final(\\n              s: models.StreamedResponse,\\n            ) -> FinalResult[models.StreamedResponse] | None:\\n              result_schema = graph_ctx.deps.result_schema\\n              async for maybe_part_event in streamed_response:\\n                if isinstance(maybe_part_event, _messages.PartStartEvent):\\n                  new_part = maybe_part_event.part\\n                  if isinstance(new_part, _messages.TextPart):\\n                    if _agent_graph.allow_text_result(result_schema):\\n                      return FinalResult(s, None)\\n                  elif isinstance(new_part, _messages.ToolCallPart):\\n                    if result_schema is not None and (match := result_schema.find_tool([new_part])):\\n                      call, _ = match\\n                      return FinalResult(s, call.tool_name)\\n              return None\\n            final_result_details = await stream_to_final(streamed_response)\\n            if final_result_details is not None:\\n              if yielded:\\n                raise exceptions.AgentRunError(\\'Agent run produced final results\\')\\n              yielded = True\\n              messages = graph_ctx.state.message_history.copy()\\n              async defon_complete() -> None:\\n\"\"\"Called when the stream has completed.\\n                The model response will have been added to messages by now\\n                by `StreamedRunResult._marked_completed`.\\n                \"\"\"\\n                last_message = messages[-1]\\n                assert isinstance(last_message, _messages.ModelResponse)\\n                tool_calls = [\\n                  part for part in last_message.parts if isinstance(part, _messages.ToolCallPart)\\n                ]\\n                parts: list[_messages.ModelRequestPart] = []\\n                async for _event in _agent_graph.process_function_tools(\\n                  tool_calls,\\n                  final_result_details.tool_name,\\n                  graph_ctx,\\n                  parts,\\n                ):\\n                  pass\\n                # TODO: Should we do something here related to the retry count?\\n                #  Maybe we should move the incrementing of the retry count to where we actually make a request?\\n                # if any(isinstance(part, _messages.RetryPromptPart) for part in parts):\\n                #   ctx.state.increment_retries(ctx.deps.max_result_retries)\\n                if parts:\\n                  messages.append(_messages.ModelRequest(parts))\\n              yield StreamedRunResult(\\n                messages,\\n                graph_ctx.deps.new_message_index,\\n                graph_ctx.deps.usage_limits,\\n                streamed_response,\\n                graph_ctx.deps.result_schema,\\n                _agent_graph.build_run_context(graph_ctx),\\n                graph_ctx.deps.result_validators,\\n                final_result_details.tool_name,\\n                on_complete,\\n              )\\n              break\\n        next_node = await agent_run.next(node)\\n        if not isinstance(next_node, BaseNode):\\n          raise exceptions.AgentRunError(\\'Should have produced a StreamedRunResult before getting here\\')\\n        node = cast(BaseNode[Any, Any, Any], next_node)\\n    if not yielded:\\n      raise exceptions.AgentRunError(\\'Agent run finished without producing a final result\\')\\n  @contextmanager\\n  defoverride(\\n    self,\\n    *,\\n    deps: AgentDepsT | _utils.Unset = _utils.UNSET,\\n    model: models.Model | models.KnownModelName | _utils.Unset = _utils.UNSET,\\n  ) -> Iterator[None]:\\n\"\"\"Context manager to temporarily override agent dependencies and model.\\n    This is particularly useful when testing.\\n    You can find an example of this [here](../testing-evals.md#overriding-model-via-pytest-fixtures).\\n    Args:\\n      deps: The dependencies to use instead of the dependencies passed to the agent run.\\n      model: The model to use instead of the model passed to the agent run.\\n    \"\"\"\\n    if _utils.is_set(deps):\\n      override_deps_before = self._override_deps\\n      self._override_deps = _utils.Some(deps)\\n    else:\\n      override_deps_before = _utils.UNSET\\n    # noinspection PyTypeChecker\\n    if _utils.is_set(model):\\n      override_model_before = self._override_model\\n      # noinspection PyTypeChecker\\n      self._override_model = _utils.Some(models.infer_model(model)) # pyright: ignore[reportArgumentType]\\n    else:\\n      override_model_before = _utils.UNSET\\n    try:\\n      yield\\n    finally:\\n      if _utils.is_set(override_deps_before):\\n        self._override_deps = override_deps_before\\n      if _utils.is_set(override_model_before):\\n        self._override_model = override_model_before\\n  @overload\\n  defsystem_prompt(\\n    self, func: Callable[[RunContext[AgentDepsT]], str], /\\n  ) -> Callable[[RunContext[AgentDepsT]], str]: ...\\n  @overload\\n  defsystem_prompt(\\n    self, func: Callable[[RunContext[AgentDepsT]], Awaitable[str]], /\\n  ) -> Callable[[RunContext[AgentDepsT]], Awaitable[str]]: ...\\n  @overload\\n  defsystem_prompt(self, func: Callable[[], str], /) -> Callable[[], str]: ...\\n  @overload\\n  defsystem_prompt(self, func: Callable[[], Awaitable[str]], /) -> Callable[[], Awaitable[str]]: ...\\n  @overload\\n  defsystem_prompt(\\n    self, /, *, dynamic: bool = False\\n  ) -> Callable[[_system_prompt.SystemPromptFunc[AgentDepsT]], _system_prompt.SystemPromptFunc[AgentDepsT]]: ...\\n  defsystem_prompt(\\n    self,\\n    func: _system_prompt.SystemPromptFunc[AgentDepsT] | None = None,\\n    /,\\n    *,\\n    dynamic: bool = False,\\n  ) -> (\\n    Callable[[_system_prompt.SystemPromptFunc[AgentDepsT]], _system_prompt.SystemPromptFunc[AgentDepsT]]\\n    | _system_prompt.SystemPromptFunc[AgentDepsT]\\n  ):\\n\"\"\"Decorator to register a system prompt function.\\n    Optionally takes [`RunContext`][pydantic_ai.tools.RunContext] as its only argument.\\n    Can decorate a sync or async functions.\\n    The decorator can be used either bare (`agent.system_prompt`) or as a function call\\n    (`agent.system_prompt(...)`), see the examples below.\\n    Overloads for every possible signature of `system_prompt` are included so the decorator doesn\\'t obscure\\n    the type of the function, see `tests/typed_agent.py` for tests.\\n    Args:\\n      func: The function to decorate\\n      dynamic: If True, the system prompt will be reevaluated even when `messages_history` is provided,\\n        see [`SystemPromptPart.dynamic_ref`][pydantic_ai.messages.SystemPromptPart.dynamic_ref]\\n    Example:'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 52, 'url': 'https://pydantic.com'}, page_content='\"\"\"\\n    if func is None:\\n      defdecorator(\\n        func_: _system_prompt.SystemPromptFunc[AgentDepsT],\\n      ) -> _system_prompt.SystemPromptFunc[AgentDepsT]:\\n        runner = _system_prompt.SystemPromptRunner[AgentDepsT](func_, dynamic=dynamic)\\n        self._system_prompt_functions.append(runner)\\n        if dynamic:\\n          self._system_prompt_dynamic_functions[func_.__qualname__] = runner\\n        return func_\\n      return decorator\\n    else:\\n      assert not dynamic, \"dynamic can\\'t be True in this case\"\\n      self._system_prompt_functions.append(_system_prompt.SystemPromptRunner[AgentDepsT](func, dynamic=dynamic))\\n      return func\\n  @overload\\n  defresult_validator(\\n    self, func: Callable[[RunContext[AgentDepsT], ResultDataT], ResultDataT], /\\n  ) -> Callable[[RunContext[AgentDepsT], ResultDataT], ResultDataT]: ...\\n  @overload\\n  defresult_validator(\\n    self, func: Callable[[RunContext[AgentDepsT], ResultDataT], Awaitable[ResultDataT]], /\\n  ) -> Callable[[RunContext[AgentDepsT], ResultDataT], Awaitable[ResultDataT]]: ...\\n  @overload\\n  defresult_validator(\\n    self, func: Callable[[ResultDataT], ResultDataT], /\\n  ) -> Callable[[ResultDataT], ResultDataT]: ...\\n  @overload\\n  defresult_validator(\\n    self, func: Callable[[ResultDataT], Awaitable[ResultDataT]], /\\n  ) -> Callable[[ResultDataT], Awaitable[ResultDataT]]: ...\\n  defresult_validator(\\n    self, func: _result.ResultValidatorFunc[AgentDepsT, ResultDataT], /\\n  ) -> _result.ResultValidatorFunc[AgentDepsT, ResultDataT]:\\n\"\"\"Decorator to register a result validator function.\\n    Optionally takes [`RunContext`][pydantic_ai.tools.RunContext] as its first argument.\\n    Can decorate a sync or async functions.\\n    Overloads for every possible signature of `result_validator` are included so the decorator doesn\\'t obscure\\n    the type of the function, see `tests/typed_agent.py` for tests.\\n    Example:'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 52, 'url': 'https://pydantic.com'}, page_content='\"\"\"\\n    self._result_validators.append(_result.ResultValidator[AgentDepsT, Any](func))\\n    return func\\n  @overload\\n  deftool(self, func: ToolFuncContext[AgentDepsT, ToolParams], /) -> ToolFuncContext[AgentDepsT, ToolParams]: ...\\n  @overload\\n  deftool(\\n    self,\\n    /,\\n    *,\\n    retries: int | None = None,\\n    prepare: ToolPrepareFunc[AgentDepsT] | None = None,\\n    docstring_format: DocstringFormat = \\'auto\\',\\n    require_parameter_descriptions: bool = False,\\n  ) -> Callable[[ToolFuncContext[AgentDepsT, ToolParams]], ToolFuncContext[AgentDepsT, ToolParams]]: ...\\n  deftool(\\n    self,\\n    func: ToolFuncContext[AgentDepsT, ToolParams] | None = None,\\n    /,\\n    *,\\n    retries: int | None = None,\\n    prepare: ToolPrepareFunc[AgentDepsT] | None = None,\\n    docstring_format: DocstringFormat = \\'auto\\',\\n    require_parameter_descriptions: bool = False,\\n  ) -> Any:\\n\"\"\"Decorator to register a tool function which takes [`RunContext`][pydantic_ai.tools.RunContext] as its first argument.\\n    Can decorate a sync or async functions.\\n    The docstring is inspected to extract both the tool description and description of each parameter,\\n    [learn more](../tools.md#function-tools-and-schema).\\n    We can\\'t add overloads for every possible signature of tool, since the return type is a recursive union\\n    so the signature of functions decorated with `@agent.tool` is obscured.\\n    Example:'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 52, 'url': 'https://pydantic.com'}, page_content='Args:\\n      func: The tool function to register.\\n      retries: The number of retries to allow for this tool, defaults to the agent\\'s default retries,\\n        which defaults to 1.\\n      prepare: custom method to prepare the tool definition for each step, return `None` to omit this\\n        tool from a given step. This is useful if you want to customise a tool at call time,\\n        or omit it completely from a step. See [`ToolPrepareFunc`][pydantic_ai.tools.ToolPrepareFunc].\\n      docstring_format: The format of the docstring, see [`DocstringFormat`][pydantic_ai.tools.DocstringFormat].\\n        Defaults to `\\'auto\\'`, such that the format is inferred from the structure of the docstring.\\n      require_parameter_descriptions: If True, raise an error if a parameter description is missing. Defaults to False.\\n    \"\"\"\\n    if func is None:\\n      deftool_decorator(\\n        func_: ToolFuncContext[AgentDepsT, ToolParams],\\n      ) -> ToolFuncContext[AgentDepsT, ToolParams]:\\n        # noinspection PyTypeChecker\\n        self._register_function(func_, True, retries, prepare, docstring_format, require_parameter_descriptions)\\n        return func_\\n      return tool_decorator\\n    else:\\n      # noinspection PyTypeChecker\\n      self._register_function(func, True, retries, prepare, docstring_format, require_parameter_descriptions)\\n      return func\\n  @overload\\n  deftool_plain(self, func: ToolFuncPlain[ToolParams], /) -> ToolFuncPlain[ToolParams]: ...\\n  @overload\\n  deftool_plain(\\n    self,\\n    /,\\n    *,\\n    retries: int | None = None,\\n    prepare: ToolPrepareFunc[AgentDepsT] | None = None,\\n    docstring_format: DocstringFormat = \\'auto\\',\\n    require_parameter_descriptions: bool = False,\\n  ) -> Callable[[ToolFuncPlain[ToolParams]], ToolFuncPlain[ToolParams]]: ...\\n  deftool_plain(\\n    self,\\n    func: ToolFuncPlain[ToolParams] | None = None,\\n    /,\\n    *,\\n    retries: int | None = None,\\n    prepare: ToolPrepareFunc[AgentDepsT] | None = None,\\n    docstring_format: DocstringFormat = \\'auto\\',\\n    require_parameter_descriptions: bool = False,\\n  ) -> Any:\\n\"\"\"Decorator to register a tool function which DOES NOT take `RunContext` as an argument.\\n    Can decorate a sync or async functions.\\n    The docstring is inspected to extract both the tool description and description of each parameter,\\n    [learn more](../tools.md#function-tools-and-schema).\\n    We can\\'t add overloads for every possible signature of tool, since the return type is a recursive union\\n    so the signature of functions decorated with `@agent.tool` is obscured.\\n    Example:'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 52, 'url': 'https://pydantic.com'}, page_content='Args:\\n      func: The tool function to register.\\n      retries: The number of retries to allow for this tool, defaults to the agent\\'s default retries,\\n        which defaults to 1.\\n      prepare: custom method to prepare the tool definition for each step, return `None` to omit this\\n        tool from a given step. This is useful if you want to customise a tool at call time,\\n        or omit it completely from a step. See [`ToolPrepareFunc`][pydantic_ai.tools.ToolPrepareFunc].\\n      docstring_format: The format of the docstring, see [`DocstringFormat`][pydantic_ai.tools.DocstringFormat].\\n        Defaults to `\\'auto\\'`, such that the format is inferred from the structure of the docstring.\\n      require_parameter_descriptions: If True, raise an error if a parameter description is missing. Defaults to False.\\n    \"\"\"\\n    if func is None:\\n      deftool_decorator(func_: ToolFuncPlain[ToolParams]) -> ToolFuncPlain[ToolParams]:\\n        # noinspection PyTypeChecker\\n        self._register_function(\\n          func_, False, retries, prepare, docstring_format, require_parameter_descriptions\\n        )\\n        return func_\\n      return tool_decorator\\n    else:\\n      self._register_function(func, False, retries, prepare, docstring_format, require_parameter_descriptions)\\n      return func\\n  def_register_function(\\n    self,\\n    func: ToolFuncEither[AgentDepsT, ToolParams],\\n    takes_ctx: bool,\\n    retries: int | None,\\n    prepare: ToolPrepareFunc[AgentDepsT] | None,\\n    docstring_format: DocstringFormat,\\n    require_parameter_descriptions: bool,\\n  ) -> None:\\n\"\"\"Private utility to register a function as a tool.\"\"\"\\n    retries_ = retries if retries is not None else self._default_retries\\n    tool = Tool[AgentDepsT](\\n      func,\\n      takes_ctx=takes_ctx,\\n      max_retries=retries_,\\n      prepare=prepare,\\n      docstring_format=docstring_format,\\n      require_parameter_descriptions=require_parameter_descriptions,\\n    )\\n    self._register_tool(tool)\\n  def_register_tool(self, tool: Tool[AgentDepsT]) -> None:\\n\"\"\"Private utility to register a tool instance.\"\"\"\\n    if tool.max_retries is None:\\n      # noinspection PyTypeChecker\\n      tool = dataclasses.replace(tool, max_retries=self._default_retries)\\n    if tool.name in self._function_tools:\\n      raise exceptions.UserError(f\\'Tool name conflicts with existing tool: {tool.name!r}\\')\\n    if self._result_schema and tool.name in self._result_schema.tools:\\n      raise exceptions.UserError(f\\'Tool name conflicts with result schema name: {tool.name!r}\\')\\n    self._function_tools[tool.name] = tool\\n  def_get_model(self, model: models.Model | models.KnownModelName | None) -> models.Model:\\n\"\"\"Create a model configured for this agent.\\n    Args:\\n      model: model to use for this run, required if `model` was not set when creating the agent.\\n    Returns:\\n      The model used\\n    \"\"\"\\n    model_: models.Model\\n    if some_model := self._override_model:\\n      # we don\\'t want `override()` to cover up errors from the model not being defined, hence this check\\n      if model is None and self.model is None:\\n        raise exceptions.UserError(\\n          \\'`model` must be set either when creating the agent or when calling it. \\'\\n          \\'(Even when `override(model=...)` is customizing the model that will actually be called)\\'\\n        )\\n      model_ = some_model.value\\n    elif model is not None:\\n      model_ = models.infer_model(model)\\n    elif self.model is not None:\\n      # noinspection PyTypeChecker\\n      model_ = self.model = models.infer_model(self.model)\\n    else:\\n      raise exceptions.UserError(\\'`model` must be set either when creating the agent or when calling it.\\')\\n    return model_\\n  def_get_deps(self: Agent[T, ResultDataT], deps: T) -> T:\\n\"\"\"Get deps for a run.\\n    If we\\'ve overridden deps via `_override_deps`, use that, otherwise use the deps passed to the call.\\n    We could do runtime type checking of deps against `self._deps_type`, but that\\'s a slippery slope.\\n    \"\"\"\\n    if some_deps := self._override_deps:\\n      return some_deps.value\\n    else:\\n      return deps\\n  def_infer_name(self, function_frame: FrameType | None) -> None:\\n\"\"\"Infer the agent name from the call frame.\\n    Usage should be `self._infer_name(inspect.currentframe())`.\\n    \"\"\"\\n    assert self.name is None, \\'Name already set\\'\\n    if function_frame is not None: # pragma: no branch\\n      if parent_frame := function_frame.f_back: # pragma: no branch\\n        for name, item in parent_frame.f_locals.items():\\n          if item is self:\\n            self.name = name\\n            return\\n        if parent_frame.f_locals != parent_frame.f_globals:\\n          # if we couldn\\'t find the agent in locals and globals are a different dict, try globals\\n          for name, item in parent_frame.f_globals.items():\\n            if item is self:\\n              self.name = name\\n              return\\n  @property\\n  @deprecated(\\n    \\'The `last_run_messages` attribute has been removed, use `capture_run_messages` instead.\\', category=None\\n  )\\n  deflast_run_messages(self) -> list[_messages.ModelMessage]:\\n    raise AttributeError(\\'The `last_run_messages` attribute has been removed, use `capture_run_messages` instead.\\')\\n  def_build_graph(\\n    self, result_type: type[RunResultDataT] | None\\n  ) -> Graph[_agent_graph.GraphAgentState, _agent_graph.GraphAgentDeps[AgentDepsT, Any], FinalResult[Any]]:\\n    return _agent_graph.build_agent_graph(self.name, self._deps_type, result_type or self.result_type)\\n  def_prepare_result_schema(\\n    self, result_type: type[RunResultDataT] | None\\n  ) -> _result.ResultSchema[RunResultDataT] | None:\\n    if result_type is not None:\\n      if self._result_validators:\\n        raise exceptions.UserError(\\'Cannot set a custom run `result_type` when the agent has result validators\\')\\n      return _result.ResultSchema[result_type].build(\\n        result_type, self._result_tool_name, self._result_tool_description\\n      )\\n    else:\\n      return self._result_schema # pyright: ignore[reportReturnType]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 52, 'url': 'https://pydantic.com'}, page_content='model: Model | KnownModelName | None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 52, 'url': 'https://pydantic.com'}, page_content='__init__(\\n  model: Model | KnownModelName | None = None,\\n  *,\\n  result_type: typeResultDataT[] = str,\\n  system_prompt: str | Sequencestr[] = (),\\n  deps_type: typeAgentDepsT[] = NoneType,\\n  name: str | None = None,\\n  model_settings: ModelSettings | None = None,\\n  retries: int = 1,\\n  result_tool_name: str = \"final_result\",\\n  result_tool_description: str | None = None,\\n  result_retries: int | None = None,\\n  tools: Sequence[\\n    ToolAgentDepsT[] | ToolFuncEitherAgentDepsT[, ...]\\n  ] = (),\\n  defer_model_check: bool = False,\\n  end_strategy: EndStrategy = \"early\"\\n)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 54, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 54, 'url': 'https://pydantic.com'}, page_content='def__init__(\\n  self,\\n  model: models.Model | models.KnownModelName | None = None,\\n  *,\\n  result_type: type[ResultDataT] = str,\\n  system_prompt: str | Sequence[str] = (),\\n  deps_type: type[AgentDepsT] = NoneType,\\n  name: str | None = None,\\n  model_settings: ModelSettings | None = None,\\n  retries: int = 1,\\n  result_tool_name: str = \\'final_result\\',\\n  result_tool_description: str | None = None,\\n  result_retries: int | None = None,\\n  tools: Sequence[Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]] = (),\\n  defer_model_check: bool = False,\\n  end_strategy: EndStrategy = \\'early\\',\\n):\\n\"\"\"Create an agent.\\n  Args:\\n    model: The default model to use for this agent, if not provide,\\n      you must provide the model when calling it.\\n    result_type: The type of the result data, used to validate the result data, defaults to `str`.\\n    system_prompt: Static system prompts to use for this agent, you can also register system\\n      prompts via a function with [`system_prompt`][pydantic_ai.Agent.system_prompt].\\n    deps_type: The type used for dependency injection, this parameter exists solely to allow you to fully\\n      parameterize the agent, and therefore get the best out of static type checking.\\n      If you\\'re not using deps, but want type checking to pass, you can set `deps=None` to satisfy Pyright\\n      or add a type hint `: Agent[None, <return type>]`.\\n    name: The name of the agent, used for logging. If `None`, we try to infer the agent name from the call frame\\n      when the agent is first run.\\n    model_settings: Optional model request settings to use for this agent\\'s runs, by default.\\n    retries: The default number of retries to allow before raising an error.\\n    result_tool_name: The name of the tool to use for the final result.\\n    result_tool_description: The description of the final result tool.\\n    result_retries: The maximum number of retries to allow for result validation, defaults to `retries`.\\n    tools: Tools to register with the agent, you can also register tools via the decorators\\n      [`@agent.tool`][pydantic_ai.Agent.tool] and [`@agent.tool_plain`][pydantic_ai.Agent.tool_plain].\\n    defer_model_check: by default, if you provide a [named][pydantic_ai.models.KnownModelName] model,\\n      it\\'s evaluated to create a [`Model`][pydantic_ai.models.Model] instance immediately,\\n      which checks for the necessary environment variables. Set this to `false`\\n      to defer the evaluation until the first run. Useful if you want to\\n      [override the model][pydantic_ai.Agent.override] for testing.\\n    end_strategy: Strategy for handling tool calls that are requested alongside a final result.\\n      See [`EndStrategy`][pydantic_ai.agent.EndStrategy] for more information.\\n  \"\"\"\\n  if model is None or defer_model_check:\\n    self.model = model\\n  else:\\n    self.model = models.infer_model(model)\\n  self.end_strategy = end_strategy\\n  self.name = name\\n  self.model_settings = model_settings\\n  self.result_type = result_type\\n  self._deps_type = deps_type\\n  self._result_tool_name = result_tool_name\\n  self._result_tool_description = result_tool_description\\n  self._result_schema: _result.ResultSchema[ResultDataT] | None = _result.ResultSchema[result_type].build(\\n    result_type, result_tool_name, result_tool_description\\n  )\\n  self._result_validators: list[_result.ResultValidator[AgentDepsT, ResultDataT]] = []\\n  self._system_prompts = (system_prompt,) if isinstance(system_prompt, str) else tuple(system_prompt)\\n  self._system_prompt_functions: list[_system_prompt.SystemPromptRunner[AgentDepsT]] = []\\n  self._system_prompt_dynamic_functions: dict[str, _system_prompt.SystemPromptRunner[AgentDepsT]] = {}\\n  self._function_tools: dict[str, Tool[AgentDepsT]] = {}\\n  self._default_retries = retries\\n  self._max_result_retries = result_retries if result_retries is not None else retries\\n  for tool in tools:\\n    if isinstance(tool, Tool):\\n      self._register_tool(tool)\\n    else:\\n      self._register_tool(Tool(tool))'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 54, 'url': 'https://pydantic.com'}, page_content='end_strategy: EndStrategy = end_strategy'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 54, 'url': 'https://pydantic.com'}, page_content='name: str | None = name'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 54, 'url': 'https://pydantic.com'}, page_content='model_settings: ModelSettings | None = model_settings'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 54, 'url': 'https://pydantic.com'}, page_content='result_type: typeResultDataT[] = result_type'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 54, 'url': 'https://pydantic.com'}, page_content='run(\\n  user_prompt: str,\\n  *,\\n  result_type: None = None,\\n  message_history: listModelMessage[] | None = None,\\n  model: Model | KnownModelName | None = None,\\n  deps: AgentDepsT = None,\\n  model_settings: ModelSettings | None = None,\\n  usage_limits: UsageLimits | None = None,\\n  usage: Usage | None = None,\\n  infer_name: bool = True\\n) -> AgentRunResultResultDataT[]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 54, 'url': 'https://pydantic.com'}, page_content='run(\\n  user_prompt: str,\\n  *,\\n  result_type: typeRunResultDataT[],\\n  message_history: listModelMessage[] | None = None,\\n  model: Model | KnownModelName | None = None,\\n  deps: AgentDepsT = None,\\n  model_settings: ModelSettings | None = None,\\n  usage_limits: UsageLimits | None = None,\\n  usage: Usage | None = None,\\n  infer_name: bool = True\\n) -> AgentRunResultRunResultDataT[]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 54, 'url': 'https://pydantic.com'}, page_content='run(\\n  user_prompt: str,\\n  *,\\n  result_type: typeRunResultDataT[] | None = None,\\n  message_history: listModelMessage[] | None = None,\\n  model: Model | KnownModelName | None = None,\\n  deps: AgentDepsT = None,\\n  model_settings: ModelSettings | None = None,\\n  usage_limits: UsageLimits | None = None,\\n  usage: Usage | None = None,\\n  infer_name: bool = True\\n) -> AgentRunResultAny[]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 54, 'url': 'https://pydantic.com'}, page_content=\"frompydantic_aiimport Agent\\nagent = Agent('openai:gpt-4o')\\nasync defmain():\\n  agent_run = await agent.run('What is the capital of France?')\\n  print(agent_run.data)\\n  #> Paris\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 55, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 55, 'url': 'https://pydantic.com'}, page_content='async defrun(\\n  self,\\n  user_prompt: str,\\n  *,\\n  result_type: type[RunResultDataT] | None = None,\\n  message_history: list[_messages.ModelMessage] | None = None,\\n  model: models.Model | models.KnownModelName | None = None,\\n  deps: AgentDepsT = None,\\n  model_settings: ModelSettings | None = None,\\n  usage_limits: _usage.UsageLimits | None = None,\\n  usage: _usage.Usage | None = None,\\n  infer_name: bool = True,\\n) -> AgentRunResult[Any]:\\n\"\"\"Run the agent with a user prompt in async mode.\\n  This method builds an internal agent graph (using system prompts, tools and result schemas) and then\\n  runs the graph to completion. The result of the run is returned.\\n  Example:'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 55, 'url': 'https://pydantic.com'}, page_content='Args:\\n    user_prompt: User input to start/continue the conversation.\\n    result_type: Custom result type to use for this run, `result_type` may only be used if the agent has no\\n      result validators since result validators would expect an argument that matches the agent\\'s result type.\\n    message_history: History of the conversation so far.\\n    model: Optional model to use for this run, required if `model` was not set when creating the agent.\\n    deps: Optional dependencies to use for this run.\\n    model_settings: Optional settings to use for this model\\'s request.\\n    usage_limits: Optional limits on model request count or token usage.\\n    usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\\n    infer_name: Whether to try to infer the agent name from the call frame if it\\'s not set.\\n  Returns:\\n    The result of the run.\\n  \"\"\"\\n  if infer_name and self.name is None:\\n    self._infer_name(inspect.currentframe())\\n  with self.iter(\\n    user_prompt=user_prompt,\\n    result_type=result_type,\\n    message_history=message_history,\\n    model=model,\\n    deps=deps,\\n    model_settings=model_settings,\\n    usage_limits=usage_limits,\\n    usage=usage,\\n  ) as agent_run:\\n    async for _ in agent_run:\\n      pass\\n  assert (final_result := agent_run.result) is not None, \\'The graph run did not finish properly\\'\\n  return final_result'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 55, 'url': 'https://pydantic.com'}, page_content='iter(\\n  user_prompt: str,\\n  *,\\n  result_type: typeRunResultDataT[] | None = None,\\n  message_history: listModelMessage[] | None = None,\\n  model: Model | KnownModelName | None = None,\\n  deps: AgentDepsT = None,\\n  model_settings: ModelSettings | None = None,\\n  usage_limits: UsageLimits | None = None,\\n  usage: Usage | None = None,\\n  infer_name: bool = True\\n) -> IteratorAgentRun[AgentDepsT[, Any]]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 56, 'url': 'https://pydantic.com'}, page_content=\"frompydantic_aiimport Agent\\nagent = Agent('openai:gpt-4o')\\nasync defmain():\\n  nodes = []\\n  with agent.iter('What is the capital of France?') as agent_run:\\n    async for node in agent_run:\\n      nodes.append(node)\\n  print(nodes)\\n'''\\n  [\\n    ModelRequestNode(\\n      request=ModelRequest(\\n        parts=[\\n          UserPromptPart(\\n            content='What is the capital of France?',\\n            timestamp=datetime.datetime(...),\\n            part_kind='user-prompt',\\n          )\\n        ],\\n        kind='request',\\n      )\\n    ),\\n    HandleResponseNode(\\n      model_response=ModelResponse(\\n        parts=[TextPart(content='Paris', part_kind='text')],\\n        model_name='function:model_logic',\\n        timestamp=datetime.datetime(...),\\n        kind='response',\\n      )\\n    ),\\n    End(data=FinalResult(data='Paris', tool_name=None)),\\n  ]\\n  '''\\n  print(agent_run.result.data)\\n  #> Paris\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 57, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 57, 'url': 'https://pydantic.com'}, page_content='@contextmanager\\ndefiter(\\n  self,\\n  user_prompt: str,\\n  *,\\n  result_type: type[RunResultDataT] | None = None,\\n  message_history: list[_messages.ModelMessage] | None = None,\\n  model: models.Model | models.KnownModelName | None = None,\\n  deps: AgentDepsT = None,\\n  model_settings: ModelSettings | None = None,\\n  usage_limits: _usage.UsageLimits | None = None,\\n  usage: _usage.Usage | None = None,\\n  infer_name: bool = True,\\n) -> Iterator[AgentRun[AgentDepsT, Any]]:\\n\"\"\"A contextmanager which can be used to iterate over the agent graph\\'s nodes as they are executed.\\n  This method builds an internal agent graph (using system prompts, tools and result schemas) and then returns an\\n  `AgentRun` object. The `AgentRun` can be used to async-iterate over the nodes of the graph as they are\\n  executed. This is the API to use if you want to consume the outputs coming from each LLM model response, or the\\n  stream of events coming from the execution of tools.\\n  The `AgentRun` also provides methods to access the full message history, new messages, and usage statistics,\\n  and the final result of the run once it has completed.\\n  For more details, see the documentation of `AgentRun`.\\n  Example:'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 57, 'url': 'https://pydantic.com'}, page_content='Args:\\n    user_prompt: User input to start/continue the conversation.\\n    result_type: Custom result type to use for this run, `result_type` may only be used if the agent has no\\n      result validators since result validators would expect an argument that matches the agent\\'s result type.\\n    message_history: History of the conversation so far.\\n    model: Optional model to use for this run, required if `model` was not set when creating the agent.\\n    deps: Optional dependencies to use for this run.\\n    model_settings: Optional settings to use for this model\\'s request.\\n    usage_limits: Optional limits on model request count or token usage.\\n    usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\\n    infer_name: Whether to try to infer the agent name from the call frame if it\\'s not set.\\n  Returns:\\n    The result of the run.\\n  \"\"\"\\n  if infer_name and self.name is None:\\n    self._infer_name(inspect.currentframe())\\n  model_used = self._get_model(model)\\n  deps = self._get_deps(deps)\\n  new_message_index = len(message_history) if message_history else 0\\n  result_schema: _result.ResultSchema[RunResultDataT] | None = self._prepare_result_schema(result_type)\\n  # Build the graph\\n  graph = self._build_graph(result_type)\\n  # Build the initial state\\n  state = _agent_graph.GraphAgentState(\\n    message_history=message_history[:] if message_history else [],\\n    usage=usage or _usage.Usage(),\\n    retries=0,\\n    run_step=0,\\n  )\\n  # We consider it a user error if a user tries to restrict the result type while having a result validator that\\n  # may change the result type from the restricted type to something else. Therefore, we consider the following\\n  # typecast reasonable, even though it is possible to violate it with otherwise-type-checked code.\\n  result_validators = cast(list[_result.ResultValidator[AgentDepsT, RunResultDataT]], self._result_validators)\\n  # TODO: Instead of this, copy the function tools to ensure they don\\'t share current_retry state between agent\\n  # runs. Requires some changes to `Tool` to make them copyable though.\\n  for v in self._function_tools.values():\\n    v.current_retry = 0\\n  model_settings = merge_model_settings(self.model_settings, model_settings)\\n  usage_limits = usage_limits or _usage.UsageLimits()\\n  # Build the deps object for the graph\\n  run_span = _logfire.span(\\n    \\'{agent_name} run {prompt=}\\',\\n    prompt=user_prompt,\\n    agent=self,\\n    model_name=model_used.model_name if model_used else \\'no-model\\',\\n    agent_name=self.name or \\'agent\\',\\n  )\\n  graph_deps = _agent_graph.GraphAgentDeps[AgentDepsT, RunResultDataT](\\n    user_deps=deps,\\n    prompt=user_prompt,\\n    new_message_index=new_message_index,\\n    model=model_used,\\n    model_settings=model_settings,\\n    usage_limits=usage_limits,\\n    max_result_retries=self._max_result_retries,\\n    end_strategy=self.end_strategy,\\n    result_schema=result_schema,\\n    result_tools=self._result_schema.tool_defs() if self._result_schema else [],\\n    result_validators=result_validators,\\n    function_tools=self._function_tools,\\n    run_span=run_span,\\n  )\\n  start_node = _agent_graph.UserPromptNode[AgentDepsT](\\n    user_prompt=user_prompt,\\n    system_prompts=self._system_prompts,\\n    system_prompt_functions=self._system_prompt_functions,\\n    system_prompt_dynamic_functions=self._system_prompt_dynamic_functions,\\n  )\\n  with graph.iter(\\n    start_node,\\n    state=state,\\n    deps=graph_deps,\\n    infer_name=False,\\n    span=run_span,\\n  ) as graph_run:\\n    yield AgentRun(graph_run)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 57, 'url': 'https://pydantic.com'}, page_content='run_sync(\\n  user_prompt: str,\\n  *,\\n  message_history: listModelMessage[] | None = None,\\n  model: Model | KnownModelName | None = None,\\n  deps: AgentDepsT = None,\\n  model_settings: ModelSettings | None = None,\\n  usage_limits: UsageLimits | None = None,\\n  usage: Usage | None = None,\\n  infer_name: bool = True\\n) -> AgentRunResultResultDataT[]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 57, 'url': 'https://pydantic.com'}, page_content='run_sync(\\n  user_prompt: str,\\n  *,\\n  result_type: typeRunResultDataT[] | None,\\n  message_history: listModelMessage[] | None = None,\\n  model: Model | KnownModelName | None = None,\\n  deps: AgentDepsT = None,\\n  model_settings: ModelSettings | None = None,\\n  usage_limits: UsageLimits | None = None,\\n  usage: Usage | None = None,\\n  infer_name: bool = True\\n) -> AgentRunResultRunResultDataT[]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 57, 'url': 'https://pydantic.com'}, page_content='run_sync(\\n  user_prompt: str,\\n  *,\\n  result_type: typeRunResultDataT[] | None = None,\\n  message_history: listModelMessage[] | None = None,\\n  model: Model | KnownModelName | None = None,\\n  deps: AgentDepsT = None,\\n  model_settings: ModelSettings | None = None,\\n  usage_limits: UsageLimits | None = None,\\n  usage: Usage | None = None,\\n  infer_name: bool = True\\n) -> AgentRunResultAny[]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 57, 'url': 'https://pydantic.com'}, page_content=\"frompydantic_aiimport Agent\\nagent = Agent('openai:gpt-4o')\\nresult_sync = agent.run_sync('What is the capital of Italy?')\\nprint(result_sync.data)\\n#> Rome\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 58, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 58, 'url': 'https://pydantic.com'}, page_content='defrun_sync(\\n  self,\\n  user_prompt: str,\\n  *,\\n  result_type: type[RunResultDataT] | None = None,\\n  message_history: list[_messages.ModelMessage] | None = None,\\n  model: models.Model | models.KnownModelName | None = None,\\n  deps: AgentDepsT = None,\\n  model_settings: ModelSettings | None = None,\\n  usage_limits: _usage.UsageLimits | None = None,\\n  usage: _usage.Usage | None = None,\\n  infer_name: bool = True,\\n) -> AgentRunResult[Any]:\\n\"\"\"Synchronously run the agent with a user prompt.\\n  This is a convenience method that wraps [`self.run`][pydantic_ai.Agent.run] with `loop.run_until_complete(...)`.\\n  You therefore can\\'t use this method inside async code or if there\\'s an active event loop.\\n  Example:'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 58, 'url': 'https://pydantic.com'}, page_content='Args:\\n    user_prompt: User input to start/continue the conversation.\\n    result_type: Custom result type to use for this run, `result_type` may only be used if the agent has no\\n      result validators since result validators would expect an argument that matches the agent\\'s result type.\\n    message_history: History of the conversation so far.\\n    model: Optional model to use for this run, required if `model` was not set when creating the agent.\\n    deps: Optional dependencies to use for this run.\\n    model_settings: Optional settings to use for this model\\'s request.\\n    usage_limits: Optional limits on model request count or token usage.\\n    usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\\n    infer_name: Whether to try to infer the agent name from the call frame if it\\'s not set.\\n  Returns:\\n    The result of the run.\\n  \"\"\"\\n  if infer_name and self.name is None:\\n    self._infer_name(inspect.currentframe())\\n  return asyncio.get_event_loop().run_until_complete(\\n    self.run(\\n      user_prompt,\\n      result_type=result_type,\\n      message_history=message_history,\\n      model=model,\\n      deps=deps,\\n      model_settings=model_settings,\\n      usage_limits=usage_limits,\\n      usage=usage,\\n      infer_name=False,\\n    )\\n  )'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 58, 'url': 'https://pydantic.com'}, page_content='run_stream(\\n  user_prompt: str,\\n  *,\\n  result_type: None = None,\\n  message_history: listModelMessage[] | None = None,\\n  model: Model | KnownModelName | None = None,\\n  deps: AgentDepsT = None,\\n  model_settings: ModelSettings | None = None,\\n  usage_limits: UsageLimits | None = None,\\n  usage: Usage | None = None,\\n  infer_name: bool = True\\n) -> AbstractAsyncContextManager[\\n  StreamedRunResultAgentDepsT[, ResultDataT]\\n]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 58, 'url': 'https://pydantic.com'}, page_content='run_stream(\\n  user_prompt: str,\\n  *,\\n  result_type: typeRunResultDataT[],\\n  message_history: listModelMessage[] | None = None,\\n  model: Model | KnownModelName | None = None,\\n  deps: AgentDepsT = None,\\n  model_settings: ModelSettings | None = None,\\n  usage_limits: UsageLimits | None = None,\\n  usage: Usage | None = None,\\n  infer_name: bool = True\\n) -> AbstractAsyncContextManager[\\n  StreamedRunResultAgentDepsT[, RunResultDataT]\\n]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 58, 'url': 'https://pydantic.com'}, page_content='run_stream(\\n  user_prompt: str,\\n  *,\\n  result_type: typeRunResultDataT[] | None = None,\\n  message_history: listModelMessage[] | None = None,\\n  model: Model | KnownModelName | None = None,\\n  deps: AgentDepsT = None,\\n  model_settings: ModelSettings | None = None,\\n  usage_limits: UsageLimits | None = None,\\n  usage: Usage | None = None,\\n  infer_name: bool = True\\n) -> AsyncIteratorStreamedRunResult[AgentDepsT[, Any]]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 58, 'url': 'https://pydantic.com'}, page_content=\"frompydantic_aiimport Agent\\nagent = Agent('openai:gpt-4o')\\nasync defmain():\\n  async with agent.run_stream('What is the capital of the UK?') as response:\\n    print(await response.get_data())\\n    #> London\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 59, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 59, 'url': 'https://pydantic.com'}, page_content='@asynccontextmanager\\nasync defrun_stream( # noqa C901\\n  self,\\n  user_prompt: str,\\n  *,\\n  result_type: type[RunResultDataT] | None = None,\\n  message_history: list[_messages.ModelMessage] | None = None,\\n  model: models.Model | models.KnownModelName | None = None,\\n  deps: AgentDepsT = None,\\n  model_settings: ModelSettings | None = None,\\n  usage_limits: _usage.UsageLimits | None = None,\\n  usage: _usage.Usage | None = None,\\n  infer_name: bool = True,\\n) -> AsyncIterator[result.StreamedRunResult[AgentDepsT, Any]]:\\n\"\"\"Run the agent with a user prompt in async mode, returning a streamed response.\\n  Example:'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 59, 'url': 'https://pydantic.com'}, page_content='Args:\\n    user_prompt: User input to start/continue the conversation.\\n    result_type: Custom result type to use for this run, `result_type` may only be used if the agent has no\\n      result validators since result validators would expect an argument that matches the agent\\'s result type.\\n    message_history: History of the conversation so far.\\n    model: Optional model to use for this run, required if `model` was not set when creating the agent.\\n    deps: Optional dependencies to use for this run.\\n    model_settings: Optional settings to use for this model\\'s request.\\n    usage_limits: Optional limits on model request count or token usage.\\n    usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\\n    infer_name: Whether to try to infer the agent name from the call frame if it\\'s not set.\\n  Returns:\\n    The result of the run.\\n  \"\"\"\\n  # TODO: We need to deprecate this now that we have the `iter` method.\\n  #  Before that, though, we should add an event for when we reach the final result of the stream.\\n  if infer_name and self.name is None:\\n    # f_back because `asynccontextmanager` adds one frame\\n    if frame := inspect.currentframe(): # pragma: no branch\\n      self._infer_name(frame.f_back)\\n  yielded = False\\n  with self.iter(\\n    user_prompt,\\n    result_type=result_type,\\n    message_history=message_history,\\n    model=model,\\n    deps=deps,\\n    model_settings=model_settings,\\n    usage_limits=usage_limits,\\n    usage=usage,\\n    infer_name=False,\\n  ) as agent_run:\\n    first_node = agent_run.next_node # start with the first node\\n    assert isinstance(first_node, _agent_graph.UserPromptNode) # the first node should be a user prompt node\\n    node: BaseNode[Any, Any, Any] = cast(BaseNode[Any, Any, Any], first_node)\\n    while True:\\n      if isinstance(node, _agent_graph.ModelRequestNode):\\n        node = cast(_agent_graph.ModelRequestNode[AgentDepsT, Any], node)\\n        graph_ctx = agent_run.ctx\\n        async with node._stream(graph_ctx) as streamed_response: # pyright: ignore[reportPrivateUsage]\\n          async defstream_to_final(\\n            s: models.StreamedResponse,\\n          ) -> FinalResult[models.StreamedResponse] | None:\\n            result_schema = graph_ctx.deps.result_schema\\n            async for maybe_part_event in streamed_response:\\n              if isinstance(maybe_part_event, _messages.PartStartEvent):\\n                new_part = maybe_part_event.part\\n                if isinstance(new_part, _messages.TextPart):\\n                  if _agent_graph.allow_text_result(result_schema):\\n                    return FinalResult(s, None)\\n                elif isinstance(new_part, _messages.ToolCallPart):\\n                  if result_schema is not None and (match := result_schema.find_tool([new_part])):\\n                    call, _ = match\\n                    return FinalResult(s, call.tool_name)\\n            return None\\n          final_result_details = await stream_to_final(streamed_response)\\n          if final_result_details is not None:\\n            if yielded:\\n              raise exceptions.AgentRunError(\\'Agent run produced final results\\')\\n            yielded = True\\n            messages = graph_ctx.state.message_history.copy()\\n            async defon_complete() -> None:\\n\"\"\"Called when the stream has completed.\\n              The model response will have been added to messages by now\\n              by `StreamedRunResult._marked_completed`.\\n              \"\"\"\\n              last_message = messages[-1]\\n              assert isinstance(last_message, _messages.ModelResponse)\\n              tool_calls = [\\n                part for part in last_message.parts if isinstance(part, _messages.ToolCallPart)\\n              ]\\n              parts: list[_messages.ModelRequestPart] = []\\n              async for _event in _agent_graph.process_function_tools(\\n                tool_calls,\\n                final_result_details.tool_name,\\n                graph_ctx,\\n                parts,\\n              ):\\n                pass\\n              # TODO: Should we do something here related to the retry count?\\n              #  Maybe we should move the incrementing of the retry count to where we actually make a request?\\n              # if any(isinstance(part, _messages.RetryPromptPart) for part in parts):\\n              #   ctx.state.increment_retries(ctx.deps.max_result_retries)\\n              if parts:\\n                messages.append(_messages.ModelRequest(parts))\\n            yield StreamedRunResult(\\n              messages,\\n              graph_ctx.deps.new_message_index,\\n              graph_ctx.deps.usage_limits,\\n              streamed_response,\\n              graph_ctx.deps.result_schema,\\n              _agent_graph.build_run_context(graph_ctx),\\n              graph_ctx.deps.result_validators,\\n              final_result_details.tool_name,\\n              on_complete,\\n            )\\n            break\\n      next_node = await agent_run.next(node)\\n      if not isinstance(next_node, BaseNode):\\n        raise exceptions.AgentRunError(\\'Should have produced a StreamedRunResult before getting here\\')\\n      node = cast(BaseNode[Any, Any, Any], next_node)\\n  if not yielded:\\n    raise exceptions.AgentRunError(\\'Agent run finished without producing a final result\\')'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 59, 'url': 'https://pydantic.com'}, page_content='override(\\n  *,\\n  deps: AgentDepsT | Unset = UNSET,\\n  model: Model | KnownModelName | Unset = UNSET\\n) -> Iterator[None]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 59, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 59, 'url': 'https://pydantic.com'}, page_content='@contextmanager\\ndefoverride(\\n  self,\\n  *,\\n  deps: AgentDepsT | _utils.Unset = _utils.UNSET,\\n  model: models.Model | models.KnownModelName | _utils.Unset = _utils.UNSET,\\n) -> Iterator[None]:\\n\"\"\"Context manager to temporarily override agent dependencies and model.\\n  This is particularly useful when testing.\\n  You can find an example of this [here](../testing-evals.md#overriding-model-via-pytest-fixtures).\\n  Args:\\n    deps: The dependencies to use instead of the dependencies passed to the agent run.\\n    model: The model to use instead of the model passed to the agent run.\\n  \"\"\"\\n  if _utils.is_set(deps):\\n    override_deps_before = self._override_deps\\n    self._override_deps = _utils.Some(deps)\\n  else:\\n    override_deps_before = _utils.UNSET\\n  # noinspection PyTypeChecker\\n  if _utils.is_set(model):\\n    override_model_before = self._override_model\\n    # noinspection PyTypeChecker\\n    self._override_model = _utils.Some(models.infer_model(model)) # pyright: ignore[reportArgumentType]\\n  else:\\n    override_model_before = _utils.UNSET\\n  try:\\n    yield\\n  finally:\\n    if _utils.is_set(override_deps_before):\\n      self._override_deps = override_deps_before\\n    if _utils.is_set(override_model_before):\\n      self._override_model = override_model_before'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 59, 'url': 'https://pydantic.com'}, page_content='system_prompt(\\n  func: Callable[RunContext[AgentDepsT[]], str],\\n) -> Callable[RunContext[AgentDepsT[]], str]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 59, 'url': 'https://pydantic.com'}, page_content='system_prompt(\\n  func: Callable[\\n    RunContext[AgentDepsT[]], Awaitablestr[]\\n  ],\\n) -> Callable[RunContext[AgentDepsT[]], Awaitablestr[]]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 59, 'url': 'https://pydantic.com'}, page_content='system_prompt(func: Callable[], str[]) -> Callable[], str[]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 59, 'url': 'https://pydantic.com'}, page_content='system_prompt(\\n  func: Callable[], Awaitable[str[]],\\n) -> Callable[], Awaitable[str[]]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 59, 'url': 'https://pydantic.com'}, page_content='system_prompt(*, dynamic: bool = False) -> Callable[\\n  SystemPromptFunc[AgentDepsT[]],\\n  SystemPromptFuncAgentDepsT[],\\n]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 59, 'url': 'https://pydantic.com'}, page_content='system_prompt(\\n  func: SystemPromptFuncAgentDepsT[] | None = None,\\n  /,\\n  *,\\n  dynamic: bool = False,\\n) -> (\\n  Callable[\\n    SystemPromptFunc[AgentDepsT[]],\\n    SystemPromptFuncAgentDepsT[],\\n  ]\\n  | SystemPromptFuncAgentDepsT[]\\n)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 60, 'url': 'https://pydantic.com'}, page_content=\"frompydantic_aiimport Agent, RunContext\\nagent = Agent('test', deps_type=str)\\n@agent.system_prompt\\ndefsimple_system_prompt() -> str:\\n  return 'foobar'\\n@agent.system_prompt(dynamic=True)\\nasync defasync_system_prompt(ctx: RunContext[str]) -> str:\\n  return f'{ctx.deps} is the best'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 60, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 60, 'url': 'https://pydantic.com'}, page_content='defsystem_prompt(\\n  self,\\n  func: _system_prompt.SystemPromptFunc[AgentDepsT] | None = None,\\n  /,\\n  *,\\n  dynamic: bool = False,\\n) -> (\\n  Callable[[_system_prompt.SystemPromptFunc[AgentDepsT]], _system_prompt.SystemPromptFunc[AgentDepsT]]\\n  | _system_prompt.SystemPromptFunc[AgentDepsT]\\n):\\n\"\"\"Decorator to register a system prompt function.\\n  Optionally takes [`RunContext`][pydantic_ai.tools.RunContext] as its only argument.\\n  Can decorate a sync or async functions.\\n  The decorator can be used either bare (`agent.system_prompt`) or as a function call\\n  (`agent.system_prompt(...)`), see the examples below.\\n  Overloads for every possible signature of `system_prompt` are included so the decorator doesn\\'t obscure\\n  the type of the function, see `tests/typed_agent.py` for tests.\\n  Args:\\n    func: The function to decorate\\n    dynamic: If True, the system prompt will be reevaluated even when `messages_history` is provided,\\n      see [`SystemPromptPart.dynamic_ref`][pydantic_ai.messages.SystemPromptPart.dynamic_ref]\\n  Example:'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 60, 'url': 'https://pydantic.com'}, page_content='\"\"\"\\n  if func is None:\\n    defdecorator(\\n      func_: _system_prompt.SystemPromptFunc[AgentDepsT],\\n    ) -> _system_prompt.SystemPromptFunc[AgentDepsT]:\\n      runner = _system_prompt.SystemPromptRunner[AgentDepsT](func_, dynamic=dynamic)\\n      self._system_prompt_functions.append(runner)\\n      if dynamic:\\n        self._system_prompt_dynamic_functions[func_.__qualname__] = runner\\n      return func_\\n    return decorator\\n  else:\\n    assert not dynamic, \"dynamic can\\'t be True in this case\"\\n    self._system_prompt_functions.append(_system_prompt.SystemPromptRunner[AgentDepsT](func, dynamic=dynamic))\\n    return func'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 60, 'url': 'https://pydantic.com'}, page_content='result_validator(\\n  func: Callable[\\n    RunContext[AgentDepsT[], ResultDataT], ResultDataT\\n  ],\\n) -> Callable[\\n  RunContext[AgentDepsT[], ResultDataT], ResultDataT\\n]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 60, 'url': 'https://pydantic.com'}, page_content='result_validator(\\n  func: Callable[\\n    RunContext[AgentDepsT[], ResultDataT],\\n    AwaitableResultDataT[],\\n  ],\\n) -> Callable[\\n  RunContext[AgentDepsT[], ResultDataT],\\n  AwaitableResultDataT[],\\n]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 60, 'url': 'https://pydantic.com'}, page_content='result_validator(\\n  func: Callable[ResultDataT[], ResultDataT],\\n) -> Callable[ResultDataT[], ResultDataT]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 60, 'url': 'https://pydantic.com'}, page_content='result_validator(\\n  func: Callable[ResultDataT[], AwaitableResultDataT[]],\\n) -> Callable[ResultDataT[], AwaitableResultDataT[]]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 60, 'url': 'https://pydantic.com'}, page_content='result_validator(\\n  func: ResultValidatorFuncAgentDepsT[, ResultDataT],\\n) -> ResultValidatorFuncAgentDepsT[, ResultDataT]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 60, 'url': 'https://pydantic.com'}, page_content=\"frompydantic_aiimport Agent, ModelRetry, RunContext\\nagent = Agent('test', deps_type=str)\\n@agent.result_validator\\ndefresult_validator_simple(data: str) -> str:\\n  if 'wrong' in data:\\n    raise ModelRetry('wrong response')\\n  return data\\n@agent.result_validator\\nasync defresult_validator_deps(ctx: RunContext[str], data: str) -> str:\\n  if ctx.deps in data:\\n    raise ModelRetry('wrong response')\\n  return data\\nresult = agent.run_sync('foobar', deps='spam')\\nprint(result.data)\\n#> success (no tool calls)\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 61, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 61, 'url': 'https://pydantic.com'}, page_content='defresult_validator(\\n  self, func: _result.ResultValidatorFunc[AgentDepsT, ResultDataT], /\\n) -> _result.ResultValidatorFunc[AgentDepsT, ResultDataT]:\\n\"\"\"Decorator to register a result validator function.\\n  Optionally takes [`RunContext`][pydantic_ai.tools.RunContext] as its first argument.\\n  Can decorate a sync or async functions.\\n  Overloads for every possible signature of `result_validator` are included so the decorator doesn\\'t obscure\\n  the type of the function, see `tests/typed_agent.py` for tests.\\n  Example:'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 61, 'url': 'https://pydantic.com'}, page_content='\"\"\"\\n  self._result_validators.append(_result.ResultValidator[AgentDepsT, Any](func))\\n  return func'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 61, 'url': 'https://pydantic.com'}, page_content='tool(\\n  func: ToolFuncContextAgentDepsT[, ToolParams],\\n) -> ToolFuncContextAgentDepsT[, ToolParams]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 61, 'url': 'https://pydantic.com'}, page_content='tool(\\n  *,\\n  retries: int | None = None,\\n  prepare: ToolPrepareFuncAgentDepsT[] | None = None,\\n  docstring_format: DocstringFormat = \"auto\",\\n  require_parameter_descriptions: bool = False\\n) -> Callable[\\n  ToolFuncContext[AgentDepsT[, ToolParams]],\\n  ToolFuncContextAgentDepsT[, ToolParams],\\n]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 61, 'url': 'https://pydantic.com'}, page_content='tool(\\n  func: (\\n    ToolFuncContextAgentDepsT[, ToolParams] | None\\n  ) = None,\\n  /,\\n  *,\\n  retries: int | None = None,\\n  prepare: ToolPrepareFuncAgentDepsT[] | None = None,\\n  docstring_format: DocstringFormat = \"auto\",\\n  require_parameter_descriptions: bool = False,\\n) -> Any'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 61, 'url': 'https://pydantic.com'}, page_content='frompydantic_aiimport Agent, RunContext\\nagent = Agent(\\'test\\', deps_type=int)\\n@agent.tool\\ndeffoobar(ctx: RunContext[int], x: int) -> int:\\n  return ctx.deps + x\\n@agent.tool(retries=2)\\nasync defspam(ctx: RunContext[str], y: float) -> float:\\n  return ctx.deps + y\\nresult = agent.run_sync(\\'foobar\\', deps=1)\\nprint(result.data)\\n#> {\"foobar\":1,\"spam\":1.0}'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 62, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 62, 'url': 'https://pydantic.com'}, page_content='deftool(\\n  self,\\n  func: ToolFuncContext[AgentDepsT, ToolParams] | None = None,\\n  /,\\n  *,\\n  retries: int | None = None,\\n  prepare: ToolPrepareFunc[AgentDepsT] | None = None,\\n  docstring_format: DocstringFormat = \\'auto\\',\\n  require_parameter_descriptions: bool = False,\\n) -> Any:\\n\"\"\"Decorator to register a tool function which takes [`RunContext`][pydantic_ai.tools.RunContext] as its first argument.\\n  Can decorate a sync or async functions.\\n  The docstring is inspected to extract both the tool description and description of each parameter,\\n  [learn more](../tools.md#function-tools-and-schema).\\n  We can\\'t add overloads for every possible signature of tool, since the return type is a recursive union\\n  so the signature of functions decorated with `@agent.tool` is obscured.\\n  Example:'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 62, 'url': 'https://pydantic.com'}, page_content='Args:\\n    func: The tool function to register.\\n    retries: The number of retries to allow for this tool, defaults to the agent\\'s default retries,\\n      which defaults to 1.\\n    prepare: custom method to prepare the tool definition for each step, return `None` to omit this\\n      tool from a given step. This is useful if you want to customise a tool at call time,\\n      or omit it completely from a step. See [`ToolPrepareFunc`][pydantic_ai.tools.ToolPrepareFunc].\\n    docstring_format: The format of the docstring, see [`DocstringFormat`][pydantic_ai.tools.DocstringFormat].\\n      Defaults to `\\'auto\\'`, such that the format is inferred from the structure of the docstring.\\n    require_parameter_descriptions: If True, raise an error if a parameter description is missing. Defaults to False.\\n  \"\"\"\\n  if func is None:\\n    deftool_decorator(\\n      func_: ToolFuncContext[AgentDepsT, ToolParams],\\n    ) -> ToolFuncContext[AgentDepsT, ToolParams]:\\n      # noinspection PyTypeChecker\\n      self._register_function(func_, True, retries, prepare, docstring_format, require_parameter_descriptions)\\n      return func_\\n    return tool_decorator\\n  else:\\n    # noinspection PyTypeChecker\\n    self._register_function(func, True, retries, prepare, docstring_format, require_parameter_descriptions)\\n    return func'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 62, 'url': 'https://pydantic.com'}, page_content='tool_plain(\\n  func: ToolFuncPlainToolParams[],\\n) -> ToolFuncPlainToolParams[]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 62, 'url': 'https://pydantic.com'}, page_content='tool_plain(\\n  *,\\n  retries: int | None = None,\\n  prepare: ToolPrepareFuncAgentDepsT[] | None = None,\\n  docstring_format: DocstringFormat = \"auto\",\\n  require_parameter_descriptions: bool = False\\n) -> Callable[\\n  ToolFuncPlain[ToolParams[]], ToolFuncPlainToolParams[]\\n]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 62, 'url': 'https://pydantic.com'}, page_content='tool_plain(\\n  func: ToolFuncPlainToolParams[] | None = None,\\n  /,\\n  *,\\n  retries: int | None = None,\\n  prepare: ToolPrepareFuncAgentDepsT[] | None = None,\\n  docstring_format: DocstringFormat = \"auto\",\\n  require_parameter_descriptions: bool = False,\\n) -> Any'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 62, 'url': 'https://pydantic.com'}, page_content='frompydantic_aiimport Agent, RunContext\\nagent = Agent(\\'test\\')\\n@agent.tool\\ndeffoobar(ctx: RunContext[int]) -> int:\\n  return 123\\n@agent.tool(retries=2)\\nasync defspam(ctx: RunContext[str]) -> float:\\n  return 3.14\\nresult = agent.run_sync(\\'foobar\\', deps=1)\\nprint(result.data)\\n#> {\"foobar\":123,\"spam\":3.14}'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 63, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 63, 'url': 'https://pydantic.com'}, page_content='deftool_plain(\\n  self,\\n  func: ToolFuncPlain[ToolParams] | None = None,\\n  /,\\n  *,\\n  retries: int | None = None,\\n  prepare: ToolPrepareFunc[AgentDepsT] | None = None,\\n  docstring_format: DocstringFormat = \\'auto\\',\\n  require_parameter_descriptions: bool = False,\\n) -> Any:\\n\"\"\"Decorator to register a tool function which DOES NOT take `RunContext` as an argument.\\n  Can decorate a sync or async functions.\\n  The docstring is inspected to extract both the tool description and description of each parameter,\\n  [learn more](../tools.md#function-tools-and-schema).\\n  We can\\'t add overloads for every possible signature of tool, since the return type is a recursive union\\n  so the signature of functions decorated with `@agent.tool` is obscured.\\n  Example:'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 63, 'url': 'https://pydantic.com'}, page_content='Args:\\n    func: The tool function to register.\\n    retries: The number of retries to allow for this tool, defaults to the agent\\'s default retries,\\n      which defaults to 1.\\n    prepare: custom method to prepare the tool definition for each step, return `None` to omit this\\n      tool from a given step. This is useful if you want to customise a tool at call time,\\n      or omit it completely from a step. See [`ToolPrepareFunc`][pydantic_ai.tools.ToolPrepareFunc].\\n    docstring_format: The format of the docstring, see [`DocstringFormat`][pydantic_ai.tools.DocstringFormat].\\n      Defaults to `\\'auto\\'`, such that the format is inferred from the structure of the docstring.\\n    require_parameter_descriptions: If True, raise an error if a parameter description is missing. Defaults to False.\\n  \"\"\"\\n  if func is None:\\n    deftool_decorator(func_: ToolFuncPlain[ToolParams]) -> ToolFuncPlain[ToolParams]:\\n      # noinspection PyTypeChecker\\n      self._register_function(\\n        func_, False, retries, prepare, docstring_format, require_parameter_descriptions\\n      )\\n      return func_\\n    return tool_decorator\\n  else:\\n    self._register_function(func, False, retries, prepare, docstring_format, require_parameter_descriptions)\\n    return func'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 63, 'url': 'https://pydantic.com'}, page_content=\"frompydantic_aiimport Agent\\nagent = Agent('openai:gpt-4o')\\nasync defmain():\\n  nodes = []\\n  # Iterate through the run, recording each node along the way:\\n  with agent.iter('What is the capital of France?') as agent_run:\\n    async for node in agent_run:\\n      nodes.append(node)\\n  print(nodes)\\n'''\\n  [\\n    ModelRequestNode(\\n      request=ModelRequest(\\n        parts=[\\n          UserPromptPart(\\n            content='What is the capital of France?',\\n            timestamp=datetime.datetime(...),\\n            part_kind='user-prompt',\\n          )\\n        ],\\n        kind='request',\\n      )\\n    ),\\n    HandleResponseNode(\\n      model_response=ModelResponse(\\n        parts=[TextPart(content='Paris', part_kind='text')],\\n        model_name='function:model_logic',\\n        timestamp=datetime.datetime(...),\\n        kind='response',\\n      )\\n    ),\\n    End(data=FinalResult(data='Paris', tool_name=None)),\\n  ]\\n  '''\\n  print(agent_run.result.data)\\n  #> Paris\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 63, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 63, 'url': 'https://pydantic.com'}, page_content='@dataclasses.dataclass(repr=False)\\nclassAgentRun(Generic[AgentDepsT, ResultDataT]):\\n\"\"\"A stateful, async-iterable run of an [`Agent`][pydantic_ai.agent.Agent].\\n  You generally obtain an `AgentRun` instance by calling `with my_agent.iter(...) as agent_run:`.\\n  Once you have an instance, you can use it to iterate through the run\\'s nodes as they execute. When an\\n  [`End`][pydantic_graph.nodes.End] is reached, the run finishes and [`result`][pydantic_ai.agent.AgentRun.result]\\n  becomes available.\\n  Example:'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 64, 'url': 'https://pydantic.com'}, page_content='You can also manually drive the iteration using the [`next`][pydantic_ai.agent.AgentRun.next] method for\\n  more granular control.\\n  \"\"\"\\n  _graph_run: GraphRun[\\n    _agent_graph.GraphAgentState, _agent_graph.GraphAgentDeps[AgentDepsT, Any], FinalResult[ResultDataT]\\n  ]\\n  @property\\n  defctx(self) -> GraphRunContext[_agent_graph.GraphAgentState, _agent_graph.GraphAgentDeps[AgentDepsT, Any]]:\\n\"\"\"The current context of the agent run.\"\"\"\\n    return GraphRunContext[_agent_graph.GraphAgentState, _agent_graph.GraphAgentDeps[AgentDepsT, Any]](\\n      self._graph_run.state, self._graph_run.deps\\n    )\\n  @property\\n  defnext_node(\\n    self,\\n  ) -> (\\n    BaseNode[_agent_graph.GraphAgentState, _agent_graph.GraphAgentDeps[AgentDepsT, Any], FinalResult[ResultDataT]]\\n    | End[FinalResult[ResultDataT]]\\n  ):\\n\"\"\"The next node that will be run in the agent graph.\\n    This is the next node that will be used during async iteration, or if a node is not passed to `self.next(...)`.\\n    \"\"\"\\n    return self._graph_run.next_node\\n  @property\\n  defresult(self) -> AgentRunResult[ResultDataT] | None:\\n\"\"\"The final result of the run if it has ended, otherwise `None`.\\n    Once the run returns an [`End`][pydantic_graph.nodes.End] node, `result` is populated\\n    with an [`AgentRunResult`][pydantic_ai.agent.AgentRunResult].\\n    \"\"\"\\n    graph_run_result = self._graph_run.result\\n    if graph_run_result is None:\\n      return None\\n    return AgentRunResult(\\n      graph_run_result.output.data,\\n      graph_run_result.output.tool_name,\\n      graph_run_result.state,\\n      self._graph_run.deps.new_message_index,\\n    )\\n  def__aiter__(\\n    self,\\n  ) -> AsyncIterator[\\n    BaseNode[\\n      _agent_graph.GraphAgentState,\\n      _agent_graph.GraphAgentDeps[AgentDepsT, Any],\\n      FinalResult[ResultDataT],\\n    ]\\n    | End[FinalResult[ResultDataT]]\\n  ]:\\n\"\"\"Provide async-iteration over the nodes in the agent run.\"\"\"\\n    return self\\n  async def__anext__(\\n    self,\\n  ) -> (\\n    BaseNode[\\n      _agent_graph.GraphAgentState,\\n      _agent_graph.GraphAgentDeps[AgentDepsT, Any],\\n      FinalResult[ResultDataT],\\n    ]\\n    | End[FinalResult[ResultDataT]]\\n  ):\\n\"\"\"Advance to the next node automatically based on the last returned node.\"\"\"\\n    return await self._graph_run.__anext__()\\n  async defnext(\\n    self,\\n    node: BaseNode[\\n      _agent_graph.GraphAgentState,\\n      _agent_graph.GraphAgentDeps[AgentDepsT, Any],\\n      FinalResult[ResultDataT],\\n    ],\\n  ) -> (\\n    BaseNode[\\n      _agent_graph.GraphAgentState,\\n      _agent_graph.GraphAgentDeps[AgentDepsT, Any],\\n      FinalResult[ResultDataT],\\n    ]\\n    | End[FinalResult[ResultDataT]]\\n  ):\\n\"\"\"Manually drive the agent run by passing in the node you want to run next.\\n    This lets you inspect or mutate the node before continuing execution, or skip certain nodes\\n    under dynamic conditions. The agent run should be stopped when you return an [`End`][pydantic_graph.nodes.End]\\n    node.\\n    Example:'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 64, 'url': 'https://pydantic.com'}, page_content='Args:\\n      node: The node to run next in the graph.\\n    Returns:\\n      The next node returned by the graph logic, or an [`End`][pydantic_graph.nodes.End] node if\\n      the run has completed.\\n    \"\"\"\\n    # Note: It might be nice to expose a synchronous interface for iteration, but we shouldn\\'t do it\\n    # on this class, or else IDEs won\\'t warn you if you accidentally use `for` instead of `async for` to iterate.\\n    return await self._graph_run.next(node)\\n  defusage(self) -> _usage.Usage:\\n\"\"\"Get usage statistics for the run so far, including token usage, model requests, and so on.\"\"\"\\n    return self._graph_run.state.usage\\n  def__repr__(self) -> str:\\n    result = self._graph_run.result\\n    result_repr = \\'<run not finished>\\' if result is None else repr(result.output)\\n    return f\\'<{type(self).__name__} result={result_repr} usage={self.usage()}>\\''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 64, 'url': 'https://pydantic.com'}, page_content='ctx: GraphRunContext[\\n  GraphAgentState, GraphAgentDepsAgentDepsT[, Any]\\n]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 64, 'url': 'https://pydantic.com'}, page_content='next_node: (\\n  BaseNode[\\n    GraphAgentState,\\n    GraphAgentDepsAgentDepsT[, Any],\\n    FinalResultResultDataT[],\\n  ]\\n  | EndFinalResult[ResultDataT[]]\\n)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 65, 'url': 'https://pydantic.com'}, page_content='result: AgentRunResultResultDataT[] | None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 65, 'url': 'https://pydantic.com'}, page_content='__aiter__() -> AsyncIterator[\\n  BaseNode[\\n    GraphAgentState,\\n    GraphAgentDepsAgentDepsT[, Any],\\n    FinalResultResultDataT[],\\n  ]\\n  | EndFinalResult[ResultDataT[]]\\n]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 65, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 65, 'url': 'https://pydantic.com'}, page_content='def__aiter__(\\n  self,\\n) -> AsyncIterator[\\n  BaseNode[\\n    _agent_graph.GraphAgentState,\\n    _agent_graph.GraphAgentDeps[AgentDepsT, Any],\\n    FinalResult[ResultDataT],\\n  ]\\n  | End[FinalResult[ResultDataT]]\\n]:\\n\"\"\"Provide async-iteration over the nodes in the agent run.\"\"\"\\n  return self'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 65, 'url': 'https://pydantic.com'}, page_content='__anext__() -> (\\n  BaseNode[\\n    GraphAgentState,\\n    GraphAgentDepsAgentDepsT[, Any],\\n    FinalResultResultDataT[],\\n  ]\\n  | EndFinalResult[ResultDataT[]]\\n)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 65, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 65, 'url': 'https://pydantic.com'}, page_content='async def__anext__(\\n  self,\\n) -> (\\n  BaseNode[\\n    _agent_graph.GraphAgentState,\\n    _agent_graph.GraphAgentDeps[AgentDepsT, Any],\\n    FinalResult[ResultDataT],\\n  ]\\n  | End[FinalResult[ResultDataT]]\\n):\\n\"\"\"Advance to the next node automatically based on the last returned node.\"\"\"\\n  return await self._graph_run.__anext__()'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 65, 'url': 'https://pydantic.com'}, page_content='next(\\n  node: BaseNode[\\n    GraphAgentState,\\n    GraphAgentDepsAgentDepsT[, Any],\\n    FinalResultResultDataT[],\\n  ],\\n) -> (\\n  BaseNode[\\n    GraphAgentState,\\n    GraphAgentDepsAgentDepsT[, Any],\\n    FinalResultResultDataT[],\\n  ]\\n  | EndFinalResult[ResultDataT[]]\\n)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 65, 'url': 'https://pydantic.com'}, page_content=\"frompydantic_aiimport Agent\\nfrompydantic_graphimport End\\nagent = Agent('openai:gpt-4o')\\nasync defmain():\\n  with agent.iter('What is the capital of France?') as agent_run:\\n    next_node = agent_run.next_node # start with the first node\\n    nodes = [next_node]\\n    while not isinstance(next_node, End):\\n      next_node = await agent_run.next(next_node)\\n      nodes.append(next_node)\\n    # Once `next_node` is an End, we've finished:\\n    print(nodes)\\n'''\\n    [\\n      UserPromptNode(\\n        user_prompt='What is the capital of France?',\\n        system_prompts=(),\\n        system_prompt_functions=[],\\n        system_prompt_dynamic_functions={},\\n      ),\\n      ModelRequestNode(\\n        request=ModelRequest(\\n          parts=[\\n            UserPromptPart(\\n              content='What is the capital of France?',\\n              timestamp=datetime.datetime(...),\\n              part_kind='user-prompt',\\n            )\\n          ],\\n          kind='request',\\n        )\\n      ),\\n      HandleResponseNode(\\n        model_response=ModelResponse(\\n          parts=[TextPart(content='Paris', part_kind='text')],\\n          model_name='function:model_logic',\\n          timestamp=datetime.datetime(...),\\n          kind='response',\\n        )\\n      ),\\n      End(data=FinalResult(data='Paris', tool_name=None)),\\n    ]\\n    '''\\n    print('Final result:', agent_run.result.data)\\n    #> Final result: Paris\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 66, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 66, 'url': 'https://pydantic.com'}, page_content='async defnext(\\n  self,\\n  node: BaseNode[\\n    _agent_graph.GraphAgentState,\\n    _agent_graph.GraphAgentDeps[AgentDepsT, Any],\\n    FinalResult[ResultDataT],\\n  ],\\n) -> (\\n  BaseNode[\\n    _agent_graph.GraphAgentState,\\n    _agent_graph.GraphAgentDeps[AgentDepsT, Any],\\n    FinalResult[ResultDataT],\\n  ]\\n  | End[FinalResult[ResultDataT]]\\n):\\n\"\"\"Manually drive the agent run by passing in the node you want to run next.\\n  This lets you inspect or mutate the node before continuing execution, or skip certain nodes\\n  under dynamic conditions. The agent run should be stopped when you return an [`End`][pydantic_graph.nodes.End]\\n  node.\\n  Example:'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 66, 'url': 'https://pydantic.com'}, page_content='Args:\\n    node: The node to run next in the graph.\\n  Returns:\\n    The next node returned by the graph logic, or an [`End`][pydantic_graph.nodes.End] node if\\n    the run has completed.\\n  \"\"\"\\n  # Note: It might be nice to expose a synchronous interface for iteration, but we shouldn\\'t do it\\n  # on this class, or else IDEs won\\'t warn you if you accidentally use `for` instead of `async for` to iterate.\\n  return await self._graph_run.next(node)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 66, 'url': 'https://pydantic.com'}, page_content='usage() -> Usage'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 66, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 66, 'url': 'https://pydantic.com'}, page_content='defusage(self) -> _usage.Usage:\\n\"\"\"Get usage statistics for the run so far, including token usage, model requests, and so on.\"\"\"\\n  return self._graph_run.state.usage'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 66, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 66, 'url': 'https://pydantic.com'}, page_content='@dataclasses.dataclass\\nclassAgentRunResult(Generic[ResultDataT]):\\n\"\"\"The final result of an agent run.\"\"\"\\n  data: ResultDataT # TODO: rename this to output. I\\'m putting this off for now mostly to reduce the size of the diff\\n  _result_tool_name: str | None = dataclasses.field(repr=False)\\n  _state: _agent_graph.GraphAgentState = dataclasses.field(repr=False)\\n  _new_message_index: int = dataclasses.field(repr=False)\\n  def_set_result_tool_return(self, return_content: str) -> list[_messages.ModelMessage]:\\n\"\"\"Set return content for the result tool.\\n    Useful if you want to continue the conversation and want to set the response to the result tool call.\\n    \"\"\"\\n    if not self._result_tool_name:\\n      raise ValueError(\\'Cannot set result tool return content when the return type is `str`.\\')\\n    messages = deepcopy(self._state.message_history)\\n    last_message = messages[-1]\\n    for part in last_message.parts:\\n      if isinstance(part, _messages.ToolReturnPart) and part.tool_name == self._result_tool_name:\\n        part.content = return_content\\n        return messages\\n    raise LookupError(f\\'No tool call found with tool name {self._result_tool_name!r}.\\')\\n  defall_messages(self, *, result_tool_return_content: str | None = None) -> list[_messages.ModelMessage]:\\n\"\"\"Return the history of _messages.\\n    Args:\\n      result_tool_return_content: The return content of the tool call to set in the last message.\\n        This provides a convenient way to modify the content of the result tool call if you want to continue\\n        the conversation and want to set the response to the result tool call. If `None`, the last message will\\n        not be modified.\\n    Returns:\\n      List of messages.\\n    \"\"\"\\n    if result_tool_return_content is not None:\\n      return self._set_result_tool_return(result_tool_return_content)\\n    else:\\n      return self._state.message_history\\n  defall_messages_json(self, *, result_tool_return_content: str | None = None) -> bytes:\\n\"\"\"Return all messages from [`all_messages`][pydantic_ai.agent.AgentRunResult.all_messages] as JSON bytes.\\n    Args:\\n      result_tool_return_content: The return content of the tool call to set in the last message.\\n        This provides a convenient way to modify the content of the result tool call if you want to continue\\n        the conversation and want to set the response to the result tool call. If `None`, the last message will\\n        not be modified.\\n    Returns:\\n      JSON bytes representing the messages.\\n    \"\"\"\\n    return _messages.ModelMessagesTypeAdapter.dump_json(\\n      self.all_messages(result_tool_return_content=result_tool_return_content)\\n    )\\n  defnew_messages(self, *, result_tool_return_content: str | None = None) -> list[_messages.ModelMessage]:\\n\"\"\"Return new messages associated with this run.\\n    Messages from older runs are excluded.\\n    Args:\\n      result_tool_return_content: The return content of the tool call to set in the last message.\\n        This provides a convenient way to modify the content of the result tool call if you want to continue\\n        the conversation and want to set the response to the result tool call. If `None`, the last message will\\n        not be modified.\\n    Returns:\\n      List of new messages.\\n    \"\"\"\\n    return self.all_messages(result_tool_return_content=result_tool_return_content)[self._new_message_index :]\\n  defnew_messages_json(self, *, result_tool_return_content: str | None = None) -> bytes:\\n\"\"\"Return new messages from [`new_messages`][pydantic_ai.agent.AgentRunResult.new_messages] as JSON bytes.\\n    Args:\\n      result_tool_return_content: The return content of the tool call to set in the last message.\\n        This provides a convenient way to modify the content of the result tool call if you want to continue\\n        the conversation and want to set the response to the result tool call. If `None`, the last message will\\n        not be modified.\\n    Returns:\\n      JSON bytes representing the new messages.\\n    \"\"\"\\n    return _messages.ModelMessagesTypeAdapter.dump_json(\\n      self.new_messages(result_tool_return_content=result_tool_return_content)\\n    )\\n  defusage(self) -> _usage.Usage:\\n\"\"\"Return the usage of the whole run.\"\"\"\\n    return self._state.usage'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 67, 'url': 'https://pydantic.com'}, page_content='all_messages(\\n  *, result_tool_return_content: str | None = None\\n) -> listModelMessage[]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 67, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 67, 'url': 'https://pydantic.com'}, page_content='defall_messages(self, *, result_tool_return_content: str | None = None) -> list[_messages.ModelMessage]:\\n\"\"\"Return the history of _messages.\\n  Args:\\n    result_tool_return_content: The return content of the tool call to set in the last message.\\n      This provides a convenient way to modify the content of the result tool call if you want to continue\\n      the conversation and want to set the response to the result tool call. If `None`, the last message will\\n      not be modified.\\n  Returns:\\n    List of messages.\\n  \"\"\"\\n  if result_tool_return_content is not None:\\n    return self._set_result_tool_return(result_tool_return_content)\\n  else:\\n    return self._state.message_history'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 67, 'url': 'https://pydantic.com'}, page_content='all_messages_json(\\n  *, result_tool_return_content: str | None = None\\n) -> bytes'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 67, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 67, 'url': 'https://pydantic.com'}, page_content='defall_messages_json(self, *, result_tool_return_content: str | None = None) -> bytes:\\n\"\"\"Return all messages from [`all_messages`][pydantic_ai.agent.AgentRunResult.all_messages] as JSON bytes.\\n  Args:\\n    result_tool_return_content: The return content of the tool call to set in the last message.\\n      This provides a convenient way to modify the content of the result tool call if you want to continue\\n      the conversation and want to set the response to the result tool call. If `None`, the last message will\\n      not be modified.\\n  Returns:\\n    JSON bytes representing the messages.\\n  \"\"\"\\n  return _messages.ModelMessagesTypeAdapter.dump_json(\\n    self.all_messages(result_tool_return_content=result_tool_return_content)\\n  )'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 67, 'url': 'https://pydantic.com'}, page_content='new_messages(\\n  *, result_tool_return_content: str | None = None\\n) -> listModelMessage[]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 68, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 68, 'url': 'https://pydantic.com'}, page_content='defnew_messages(self, *, result_tool_return_content: str | None = None) -> list[_messages.ModelMessage]:\\n\"\"\"Return new messages associated with this run.\\n  Messages from older runs are excluded.\\n  Args:\\n    result_tool_return_content: The return content of the tool call to set in the last message.\\n      This provides a convenient way to modify the content of the result tool call if you want to continue\\n      the conversation and want to set the response to the result tool call. If `None`, the last message will\\n      not be modified.\\n  Returns:\\n    List of new messages.\\n  \"\"\"\\n  return self.all_messages(result_tool_return_content=result_tool_return_content)[self._new_message_index :]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 68, 'url': 'https://pydantic.com'}, page_content='new_messages_json(\\n  *, result_tool_return_content: str | None = None\\n) -> bytes'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 68, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 68, 'url': 'https://pydantic.com'}, page_content='defnew_messages_json(self, *, result_tool_return_content: str | None = None) -> bytes:\\n\"\"\"Return new messages from [`new_messages`][pydantic_ai.agent.AgentRunResult.new_messages] as JSON bytes.\\n  Args:\\n    result_tool_return_content: The return content of the tool call to set in the last message.\\n      This provides a convenient way to modify the content of the result tool call if you want to continue\\n      the conversation and want to set the response to the result tool call. If `None`, the last message will\\n      not be modified.\\n  Returns:\\n    JSON bytes representing the new messages.\\n  \"\"\"\\n  return _messages.ModelMessagesTypeAdapter.dump_json(\\n    self.new_messages(result_tool_return_content=result_tool_return_content)\\n  )'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 68, 'url': 'https://pydantic.com'}, page_content='usage() -> Usage'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 68, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 68, 'url': 'https://pydantic.com'}, page_content='defusage(self) -> _usage.Usage:\\n\"\"\"Return the usage of the whole run.\"\"\"\\n  return self._state.usage'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 68, 'url': 'https://pydantic.com'}, page_content='EndStrategy = EndStrategy'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 68, 'url': 'https://pydantic.com'}, page_content=\"RunResultDataT = TypeVar('RunResultDataT')\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 68, 'url': 'https://pydantic.com'}, page_content='capture_run_messages = capture_run_messages'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 69, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 69, 'url': 'https://pydantic.com'}, page_content='classModelRetry(Exception):\\n\"\"\"Exception raised when a tool function should be retried.\\n  The agent will return the message to the model and ask it to try calling the function/tool again.\\n  \"\"\"\\n  message: str\\n\"\"\"The message to return to the model.\"\"\"\\n  def__init__(self, message: str):\\n    self.message = message\\n    super().__init__(message)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 69, 'url': 'https://pydantic.com'}, page_content='message: str = message'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 69, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 69, 'url': 'https://pydantic.com'}, page_content='classUserError(RuntimeError):\\n\"\"\"Error caused by a usage mistake by the application developer — You!\"\"\"\\n  message: str\\n\"\"\"Description of the mistake.\"\"\"\\n  def__init__(self, message: str):\\n    self.message = message\\n    super().__init__(message)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 69, 'url': 'https://pydantic.com'}, page_content='message: str = message'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 69, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 69, 'url': 'https://pydantic.com'}, page_content='classAgentRunError(RuntimeError):\\n\"\"\"Base class for errors occurring during an agent run.\"\"\"\\n  message: str\\n\"\"\"The error message.\"\"\"\\n  def__init__(self, message: str):\\n    self.message = message\\n    super().__init__(message)\\n  def__str__(self) -> str:\\n    return self.message'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 69, 'url': 'https://pydantic.com'}, page_content='message: str = message'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 69, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 69, 'url': 'https://pydantic.com'}, page_content='classUsageLimitExceeded(AgentRunError):\\n\"\"\"Error raised when a Model\\'s usage exceeds the specified limits.\"\"\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 69, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 69, 'url': 'https://pydantic.com'}, page_content='classUnexpectedModelBehavior(AgentRunError):\\n\"\"\"Error caused by unexpected Model behavior, e.g. an unexpected response code.\"\"\"\\n  message: str\\n\"\"\"Description of the unexpected behavior.\"\"\"\\n  body: str | None\\n\"\"\"The body of the response, if available.\"\"\"\\n  def__init__(self, message: str, body: str | None = None):\\n    self.message = message\\n    if body is None:\\n      self.body: str | None = None\\n    else:\\n      try:\\n        self.body = json.dumps(json.loads(body), indent=2)\\n      except ValueError:\\n        self.body = body\\n    super().__init__(message)\\n  def__str__(self) -> str:\\n    if self.body:\\n      return f\\'{self.message}, body:\\\\n{self.body}\\'\\n    else:\\n      return self.message'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 69, 'url': 'https://pydantic.com'}, page_content='message: str = message'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 69, 'url': 'https://pydantic.com'}, page_content='body: str | None = dumps(loads(body), indent=2)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 71, 'url': 'https://pydantic.com'}, page_content='format_as_xml(\\n  obj: Any,\\n  root_tag: str = \"examples\",\\n  item_tag: str = \"example\",\\n  include_root_tag: bool = True,\\n  none_str: str = \"null\",\\n  indent: str | None = \" \",\\n) -> str'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 71, 'url': 'https://pydantic.com'}, page_content=\"frompydantic_ai.format_as_xmlimport format_as_xml\\nprint(format_as_xml({'name': 'John', 'height': 6, 'weight': 200}, root_tag='user'))\\n'''\\n<user>\\n <name>John</name>\\n <height>6</height>\\n <weight>200</weight>\\n</user>\\n'''\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 71, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 71, 'url': 'https://pydantic.com'}, page_content='defformat_as_xml(\\n  obj: Any,\\n  root_tag: str = \\'examples\\',\\n  item_tag: str = \\'example\\',\\n  include_root_tag: bool = True,\\n  none_str: str = \\'null\\',\\n  indent: str | None = \\' \\',\\n) -> str:\\n\"\"\"Format a Python object as XML.\\n  This is useful since LLMs often find it easier to read semi-structured data (e.g. examples) as XML,\\n  rather than JSON etc.\\n  Supports: `str`, `bytes`, `bytearray`, `bool`, `int`, `float`, `date`, `datetime`, `Mapping`,\\n  `Iterable`, `dataclass`, and `BaseModel`.\\n  Args:\\n    obj: Python Object to serialize to XML.\\n    root_tag: Outer tag to wrap the XML in, use `None` to omit the outer tag.\\n    item_tag: Tag to use for each item in an iterable (e.g. list), this is overridden by the class name\\n      for dataclasses and Pydantic models.\\n    include_root_tag: Whether to include the root tag in the output\\n      (The root tag is always included if it includes a body - e.g. when the input is a simple value).\\n    none_str: String to use for `None` values.\\n    indent: Indentation string to use for pretty printing.\\n  Returns:\\n    XML representation of the object.\\n  Example:'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 71, 'url': 'https://pydantic.com'}, page_content='\"\"\"\\n  el = _ToXml(item_tag=item_tag, none_str=none_str).to_xml(obj, root_tag)\\n  if not include_root_tag and el.text is None:\\n    join = \\'\\' if indent is None else \\'\\\\n\\'\\n    return join.join(_rootless_xml_elements(el, indent))\\n  else:\\n    if indent is not None:\\n      ElementTree.indent(el, space=indent)\\n    return ElementTree.tostring(el, encoding=\\'unicode\\')'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 72, 'url': 'https://pydantic.com'}, page_content='graph RL\\n  SystemPromptPart(SystemPromptPart) --- ModelRequestPart\\n  UserPromptPart(UserPromptPart) --- ModelRequestPart\\n  ToolReturnPart(ToolReturnPart) --- ModelRequestPart\\n  RetryPromptPart(RetryPromptPart) --- ModelRequestPart\\n  TextPart(TextPart) --- ModelResponsePart\\n  ToolCallPart(ToolCallPart) --- ModelResponsePart\\n  ModelRequestPart(\"ModelRequestPart<br>(Union)\") --- ModelRequest\\n  ModelRequest(\"ModelRequest(parts=list[...])\") --- ModelMessage\\n  ModelResponsePart(\"ModelResponsePart<br>(Union)\") --- ModelResponse\\n  ModelResponse(\"ModelResponse(parts=list[...])\") --- ModelMessage(\"ModelMessage<br>(Union)\")'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 73, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 73, 'url': 'https://pydantic.com'}, page_content='@dataclass\\nclassSystemPromptPart:\\n\"\"\"A system prompt, generally written by the application developer.\\n  This gives the model context and guidance on how to respond.\\n  \"\"\"\\n  content: str\\n\"\"\"The content of the prompt.\"\"\"\\n  dynamic_ref: str | None = None\\n\"\"\"The ref of the dynamic system prompt function that generated this part.\\n  Only set if system prompt is dynamic, see [`system_prompt`][pydantic_ai.Agent.system_prompt] for more information.\\n  \"\"\"\\n  part_kind: Literal[\\'system-prompt\\'] = \\'system-prompt\\'\\n\"\"\"Part type identifier, this is available on all parts as a discriminator.\"\"\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 73, 'url': 'https://pydantic.com'}, page_content='content: str'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 73, 'url': 'https://pydantic.com'}, page_content='dynamic_ref: str | None = None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 73, 'url': 'https://pydantic.com'}, page_content=\"part_kind: Literal['system-prompt'] = 'system-prompt'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 73, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 73, 'url': 'https://pydantic.com'}, page_content='@dataclass\\nclassUserPromptPart:\\n\"\"\"A user prompt, generally written by the end user.\\n  Content comes from the `user_prompt` parameter of [`Agent.run`][pydantic_ai.Agent.run],\\n  [`Agent.run_sync`][pydantic_ai.Agent.run_sync], and [`Agent.run_stream`][pydantic_ai.Agent.run_stream].\\n  \"\"\"\\n  content: str\\n\"\"\"The content of the prompt.\"\"\"\\n  timestamp: datetime = field(default_factory=_now_utc)\\n\"\"\"The timestamp of the prompt.\"\"\"\\n  part_kind: Literal[\\'user-prompt\\'] = \\'user-prompt\\'\\n\"\"\"Part type identifier, this is available on all parts as a discriminator.\"\"\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 73, 'url': 'https://pydantic.com'}, page_content='content: str'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 73, 'url': 'https://pydantic.com'}, page_content='timestamp: datetime = field(default_factory=now_utc)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 73, 'url': 'https://pydantic.com'}, page_content=\"part_kind: Literal['user-prompt'] = 'user-prompt'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 73, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 73, 'url': 'https://pydantic.com'}, page_content='@dataclass\\nclassToolReturnPart:\\n\"\"\"A tool return message, this encodes the result of running a tool.\"\"\"\\n  tool_name: str\\n\"\"\"The name of the \"tool\" was called.\"\"\"\\n  content: Any\\n\"\"\"The return value.\"\"\"\\n  tool_call_id: str | None = None\\n\"\"\"Optional tool call identifier, this is used by some models including OpenAI.\"\"\"\\n  timestamp: datetime = field(default_factory=_now_utc)\\n\"\"\"The timestamp, when the tool returned.\"\"\"\\n  part_kind: Literal[\\'tool-return\\'] = \\'tool-return\\'\\n\"\"\"Part type identifier, this is available on all parts as a discriminator.\"\"\"\\n  defmodel_response_str(self) -> str:\\n\"\"\"Return a string representation of the content for the model.\"\"\"\\n    if isinstance(self.content, str):\\n      return self.content\\n    else:\\n      return tool_return_ta.dump_json(self.content).decode()\\n  defmodel_response_object(self) -> dict[str, Any]:\\n\"\"\"Return a dictionary representation of the content, wrapping non-dict types appropriately.\"\"\"\\n    # gemini supports JSON dict return values, but no other JSON types, hence we wrap anything else in a dict\\n    if isinstance(self.content, dict):\\n      return tool_return_ta.dump_python(self.content, mode=\\'json\\') # pyright: ignore[reportUnknownMemberType]\\n    else:\\n      return {\\'return_value\\': tool_return_ta.dump_python(self.content, mode=\\'json\\')}'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 73, 'url': 'https://pydantic.com'}, page_content='tool_name: str'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 73, 'url': 'https://pydantic.com'}, page_content='content: Any'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 73, 'url': 'https://pydantic.com'}, page_content='tool_call_id: str | None = None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 73, 'url': 'https://pydantic.com'}, page_content='timestamp: datetime = field(default_factory=now_utc)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 73, 'url': 'https://pydantic.com'}, page_content=\"part_kind: Literal['tool-return'] = 'tool-return'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 74, 'url': 'https://pydantic.com'}, page_content='model_response_str() -> str'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 74, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 74, 'url': 'https://pydantic.com'}, page_content='defmodel_response_str(self) -> str:\\n\"\"\"Return a string representation of the content for the model.\"\"\"\\n  if isinstance(self.content, str):\\n    return self.content\\n  else:\\n    return tool_return_ta.dump_json(self.content).decode()'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 74, 'url': 'https://pydantic.com'}, page_content='model_response_object() -> dictstr[, Any]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 74, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 74, 'url': 'https://pydantic.com'}, page_content='defmodel_response_object(self) -> dict[str, Any]:\\n\"\"\"Return a dictionary representation of the content, wrapping non-dict types appropriately.\"\"\"\\n  # gemini supports JSON dict return values, but no other JSON types, hence we wrap anything else in a dict\\n  if isinstance(self.content, dict):\\n    return tool_return_ta.dump_python(self.content, mode=\\'json\\') # pyright: ignore[reportUnknownMemberType]\\n  else:\\n    return {\\'return_value\\': tool_return_ta.dump_python(self.content, mode=\\'json\\')}'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 74, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 74, 'url': 'https://pydantic.com'}, page_content='@dataclass\\nclassRetryPromptPart:\\n\"\"\"A message back to a model asking it to try again.\\n  This can be sent for a number of reasons:\\n  * Pydantic validation of tool arguments failed, here content is derived from a Pydantic\\n   [`ValidationError`][pydantic_core.ValidationError]\\n  * a tool raised a [`ModelRetry`][pydantic_ai.exceptions.ModelRetry] exception\\n  * no tool was found for the tool name\\n  * the model returned plain text when a structured response was expected\\n  * Pydantic validation of a structured response failed, here content is derived from a Pydantic\\n   [`ValidationError`][pydantic_core.ValidationError]\\n  * a result validator raised a [`ModelRetry`][pydantic_ai.exceptions.ModelRetry] exception\\n  \"\"\"\\n  content: list[pydantic_core.ErrorDetails] | str\\n\"\"\"Details of why and how the model should retry.\\n  If the retry was triggered by a [`ValidationError`][pydantic_core.ValidationError], this will be a list of\\n  error details.\\n  \"\"\"\\n  tool_name: str | None = None\\n\"\"\"The name of the tool that was called, if any.\"\"\"\\n  tool_call_id: str | None = None\\n\"\"\"Optional tool call identifier, this is used by some models including OpenAI.\"\"\"\\n  timestamp: datetime = field(default_factory=_now_utc)\\n\"\"\"The timestamp, when the retry was triggered.\"\"\"\\n  part_kind: Literal[\\'retry-prompt\\'] = \\'retry-prompt\\'\\n\"\"\"Part type identifier, this is available on all parts as a discriminator.\"\"\"\\n  defmodel_response(self) -> str:\\n\"\"\"Return a string message describing why the retry is requested.\"\"\"\\n    if isinstance(self.content, str):\\n      description = self.content\\n    else:\\n      json_errors = error_details_ta.dump_json(self.content, exclude={\\'__all__\\': {\\'ctx\\'}}, indent=2)\\n      description = f\\'{len(self.content)} validation errors: {json_errors.decode()}\\'\\n    return f\\'{description}\\\\n\\\\nFix the errors and try again.\\''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 74, 'url': 'https://pydantic.com'}, page_content='content: listErrorDetails[] | str'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 74, 'url': 'https://pydantic.com'}, page_content='tool_name: str | None = None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 74, 'url': 'https://pydantic.com'}, page_content='tool_call_id: str | None = None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 74, 'url': 'https://pydantic.com'}, page_content='timestamp: datetime = field(default_factory=now_utc)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 74, 'url': 'https://pydantic.com'}, page_content=\"part_kind: Literal['retry-prompt'] = 'retry-prompt'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 74, 'url': 'https://pydantic.com'}, page_content='model_response() -> str'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 75, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 75, 'url': 'https://pydantic.com'}, page_content='defmodel_response(self) -> str:\\n\"\"\"Return a string message describing why the retry is requested.\"\"\"\\n  if isinstance(self.content, str):\\n    description = self.content\\n  else:\\n    json_errors = error_details_ta.dump_json(self.content, exclude={\\'__all__\\': {\\'ctx\\'}}, indent=2)\\n    description = f\\'{len(self.content)} validation errors: {json_errors.decode()}\\'\\n  return f\\'{description}\\\\n\\\\nFix the errors and try again.\\''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 75, 'url': 'https://pydantic.com'}, page_content='ModelRequestPart = Annotated[\\n  Union[\\n    SystemPromptPart,\\n    UserPromptPart,\\n    ToolReturnPart,\\n    RetryPromptPart,\\n  ],\\n  Discriminator(\"part_kind\"),\\n]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 75, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 75, 'url': 'https://pydantic.com'}, page_content='@dataclass\\nclassModelRequest:\\n\"\"\"A request generated by PydanticAI and sent to a model, e.g. a message from the PydanticAI app to the model.\"\"\"\\n  parts: list[ModelRequestPart]\\n\"\"\"The parts of the user message.\"\"\"\\n  kind: Literal[\\'request\\'] = \\'request\\'\\n\"\"\"Message type identifier, this is available on all parts as a discriminator.\"\"\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 75, 'url': 'https://pydantic.com'}, page_content='parts: listModelRequestPart[]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 75, 'url': 'https://pydantic.com'}, page_content=\"kind: Literal['request'] = 'request'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 75, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 75, 'url': 'https://pydantic.com'}, page_content='@dataclass\\nclassTextPart:\\n\"\"\"A plain text response from a model.\"\"\"\\n  content: str\\n\"\"\"The text content of the response.\"\"\"\\n  part_kind: Literal[\\'text\\'] = \\'text\\'\\n\"\"\"Part type identifier, this is available on all parts as a discriminator.\"\"\"\\n  defhas_content(self) -> bool:\\n\"\"\"Return `True` if the text content is non-empty.\"\"\"\\n    return bool(self.content)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 75, 'url': 'https://pydantic.com'}, page_content='content: str'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 75, 'url': 'https://pydantic.com'}, page_content=\"part_kind: Literal['text'] = 'text'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 75, 'url': 'https://pydantic.com'}, page_content='has_content() -> bool'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 75, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 75, 'url': 'https://pydantic.com'}, page_content='defhas_content(self) -> bool:\\n\"\"\"Return `True` if the text content is non-empty.\"\"\"\\n  return bool(self.content)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 75, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 75, 'url': 'https://pydantic.com'}, page_content='@dataclass\\nclassToolCallPart:\\n\"\"\"A tool call from a model.\"\"\"\\n  tool_name: str\\n\"\"\"The name of the tool to call.\"\"\"\\n  args: str | dict[str, Any]\\n\"\"\"The arguments to pass to the tool.\\n  This is stored either as a JSON string or a Python dictionary depending on how data was received.\\n  \"\"\"\\n  tool_call_id: str | None = None\\n\"\"\"Optional tool call identifier, this is used by some models including OpenAI.\"\"\"\\n  part_kind: Literal[\\'tool-call\\'] = \\'tool-call\\'\\n\"\"\"Part type identifier, this is available on all parts as a discriminator.\"\"\"\\n  defargs_as_dict(self) -> dict[str, Any]:\\n\"\"\"Return the arguments as a Python dictionary.\\n    This is just for convenience with models that require dicts as input.\\n    \"\"\"\\n    if isinstance(self.args, dict):\\n      return self.args\\n    args = pydantic_core.from_json(self.args)\\n    assert isinstance(args, dict), \\'args should be a dict\\'\\n    return cast(dict[str, Any], args)\\n  defargs_as_json_str(self) -> str:\\n\"\"\"Return the arguments as a JSON string.\\n    This is just for convenience with models that require JSON strings as input.\\n    \"\"\"\\n    if isinstance(self.args, str):\\n      return self.args\\n    return pydantic_core.to_json(self.args).decode()\\n  defhas_content(self) -> bool:\\n\"\"\"Return `True` if the arguments contain any data.\"\"\"\\n    if isinstance(self.args, dict):\\n      # TODO: This should probably return True if you have the value False, or 0, etc.\\n      #  It makes sense to me to ignore empty strings, but not sure about empty lists or dicts\\n      return any(self.args.values())\\n    else:\\n      return bool(self.args)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 75, 'url': 'https://pydantic.com'}, page_content='tool_name: str'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 75, 'url': 'https://pydantic.com'}, page_content='args: str | dictstr[, Any]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 75, 'url': 'https://pydantic.com'}, page_content='tool_call_id: str | None = None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 75, 'url': 'https://pydantic.com'}, page_content=\"part_kind: Literal['tool-call'] = 'tool-call'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 76, 'url': 'https://pydantic.com'}, page_content='args_as_dict() -> dictstr[, Any]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 76, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 76, 'url': 'https://pydantic.com'}, page_content='defargs_as_dict(self) -> dict[str, Any]:\\n\"\"\"Return the arguments as a Python dictionary.\\n  This is just for convenience with models that require dicts as input.\\n  \"\"\"\\n  if isinstance(self.args, dict):\\n    return self.args\\n  args = pydantic_core.from_json(self.args)\\n  assert isinstance(args, dict), \\'args should be a dict\\'\\n  return cast(dict[str, Any], args)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 76, 'url': 'https://pydantic.com'}, page_content='args_as_json_str() -> str'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 76, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 76, 'url': 'https://pydantic.com'}, page_content='defargs_as_json_str(self) -> str:\\n\"\"\"Return the arguments as a JSON string.\\n  This is just for convenience with models that require JSON strings as input.\\n  \"\"\"\\n  if isinstance(self.args, str):\\n    return self.args\\n  return pydantic_core.to_json(self.args).decode()'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 76, 'url': 'https://pydantic.com'}, page_content='has_content() -> bool'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 76, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 76, 'url': 'https://pydantic.com'}, page_content='defhas_content(self) -> bool:\\n\"\"\"Return `True` if the arguments contain any data.\"\"\"\\n  if isinstance(self.args, dict):\\n    # TODO: This should probably return True if you have the value False, or 0, etc.\\n    #  It makes sense to me to ignore empty strings, but not sure about empty lists or dicts\\n    return any(self.args.values())\\n  else:\\n    return bool(self.args)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 76, 'url': 'https://pydantic.com'}, page_content='ModelResponsePart = Annotated[\\n  UnionTextPart[, ToolCallPart],\\n  Discriminator(\"part_kind\"),\\n]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 76, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 76, 'url': 'https://pydantic.com'}, page_content='@dataclass\\nclassModelResponse:\\n\"\"\"A response from a model, e.g. a message from the model to the PydanticAI app.\"\"\"\\n  parts: list[ModelResponsePart]\\n\"\"\"The parts of the model message.\"\"\"\\n  model_name: str | None = None\\n\"\"\"The name of the model that generated the response.\"\"\"\\n  timestamp: datetime = field(default_factory=_now_utc)\\n\"\"\"The timestamp of the response.\\n  If the model provides a timestamp in the response (as OpenAI does) that will be used.\\n  \"\"\"\\n  kind: Literal[\\'response\\'] = \\'response\\'\\n\"\"\"Message type identifier, this is available on all parts as a discriminator.\"\"\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 76, 'url': 'https://pydantic.com'}, page_content='parts: listModelResponsePart[]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 76, 'url': 'https://pydantic.com'}, page_content='model_name: str | None = None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 76, 'url': 'https://pydantic.com'}, page_content='timestamp: datetime = field(default_factory=now_utc)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 76, 'url': 'https://pydantic.com'}, page_content=\"kind: Literal['response'] = 'response'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 76, 'url': 'https://pydantic.com'}, page_content='ModelMessage = Annotated[\\n  UnionModelRequest[, ModelResponse],\\n  Discriminator(\"kind\"),\\n]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 76, 'url': 'https://pydantic.com'}, page_content='ModelMessagesTypeAdapter = TypeAdapter(\\n  listModelMessage[], config=ConfigDict(defer_build=True)\\n)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 76, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 76, 'url': 'https://pydantic.com'}, page_content='@dataclass\\nclassTextPartDelta:\\n\"\"\"A partial update (delta) for a `TextPart` to append new text content.\"\"\"\\n  content_delta: str\\n\"\"\"The incremental text content to add to the existing `TextPart` content.\"\"\"\\n  part_delta_kind: Literal[\\'text\\'] = \\'text\\'\\n\"\"\"Part delta type identifier, used as a discriminator.\"\"\"\\n  defapply(self, part: ModelResponsePart) -> TextPart:\\n\"\"\"Apply this text delta to an existing `TextPart`.\\n    Args:\\n      part: The existing model response part, which must be a `TextPart`.\\n    Returns:\\n      A new `TextPart` with updated text content.\\n    Raises:\\n      ValueError: If `part` is not a `TextPart`.\\n    \"\"\"\\n    if not isinstance(part, TextPart):\\n      raise ValueError(\\'Cannot apply TextPartDeltas to non-TextParts\\')\\n    return replace(part, content=part.content + self.content_delta)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 77, 'url': 'https://pydantic.com'}, page_content='content_delta: str'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 77, 'url': 'https://pydantic.com'}, page_content=\"part_delta_kind: Literal['text'] = 'text'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 77, 'url': 'https://pydantic.com'}, page_content='apply(part: ModelResponsePart) -> TextPart'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 77, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 77, 'url': 'https://pydantic.com'}, page_content='defapply(self, part: ModelResponsePart) -> TextPart:\\n\"\"\"Apply this text delta to an existing `TextPart`.\\n  Args:\\n    part: The existing model response part, which must be a `TextPart`.\\n  Returns:\\n    A new `TextPart` with updated text content.\\n  Raises:\\n    ValueError: If `part` is not a `TextPart`.\\n  \"\"\"\\n  if not isinstance(part, TextPart):\\n    raise ValueError(\\'Cannot apply TextPartDeltas to non-TextParts\\')\\n  return replace(part, content=part.content + self.content_delta)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 77, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 77, 'url': 'https://pydantic.com'}, page_content='@dataclass\\nclassToolCallPartDelta:\\n\"\"\"A partial update (delta) for a `ToolCallPart` to modify tool name, arguments, or tool call ID.\"\"\"\\n  tool_name_delta: str | None = None\\n\"\"\"Incremental text to add to the existing tool name, if any.\"\"\"\\n  args_delta: str | dict[str, Any] | None = None\\n\"\"\"Incremental data to add to the tool arguments.\\n  If this is a string, it will be appended to existing JSON arguments.\\n  If this is a dict, it will be merged with existing dict arguments.\\n  \"\"\"\\n  tool_call_id: str | None = None\\n\"\"\"Optional tool call identifier, this is used by some models including OpenAI.\\n  Note this is never treated as a delta — it can replace None, but otherwise if a\\n  non-matching value is provided an error will be raised.\"\"\"\\n  part_delta_kind: Literal[\\'tool_call\\'] = \\'tool_call\\'\\n\"\"\"Part delta type identifier, used as a discriminator.\"\"\"\\n  defas_part(self) -> ToolCallPart | None:\\n\"\"\"Convert this delta to a fully formed `ToolCallPart` if possible, otherwise return `None`.\\n    Returns:\\n      A `ToolCallPart` if both `tool_name_delta` and `args_delta` are set, otherwise `None`.\\n    \"\"\"\\n    if self.tool_name_delta is None or self.args_delta is None:\\n      return None\\n    return ToolCallPart(\\n      self.tool_name_delta,\\n      self.args_delta,\\n      self.tool_call_id,\\n    )\\n  @overload\\n  defapply(self, part: ModelResponsePart) -> ToolCallPart: ...\\n  @overload\\n  defapply(self, part: ModelResponsePart | ToolCallPartDelta) -> ToolCallPart | ToolCallPartDelta: ...\\n  defapply(self, part: ModelResponsePart | ToolCallPartDelta) -> ToolCallPart | ToolCallPartDelta:\\n\"\"\"Apply this delta to a part or delta, returning a new part or delta with the changes applied.\\n    Args:\\n      part: The existing model response part or delta to update.\\n    Returns:\\n      Either a new `ToolCallPart` or an updated `ToolCallPartDelta`.\\n    Raises:\\n      ValueError: If `part` is neither a `ToolCallPart` nor a `ToolCallPartDelta`.\\n      UnexpectedModelBehavior: If applying JSON deltas to dict arguments or vice versa.\\n    \"\"\"\\n    if isinstance(part, ToolCallPart):\\n      return self._apply_to_part(part)\\n    if isinstance(part, ToolCallPartDelta):\\n      return self._apply_to_delta(part)\\n    raise ValueError(f\\'Can only apply ToolCallPartDeltas to ToolCallParts or ToolCallPartDeltas, not {part}\\')\\n  def_apply_to_delta(self, delta: ToolCallPartDelta) -> ToolCallPart | ToolCallPartDelta:\\n\"\"\"Internal helper to apply this delta to another delta.\"\"\"\\n    if self.tool_name_delta:\\n      # Append incremental text to the existing tool_name_delta\\n      updated_tool_name_delta = (delta.tool_name_delta or \\'\\') + self.tool_name_delta\\n      delta = replace(delta, tool_name_delta=updated_tool_name_delta)\\n    if isinstance(self.args_delta, str):\\n      if isinstance(delta.args_delta, dict):\\n        raise UnexpectedModelBehavior(\\n          f\\'Cannot apply JSON deltas to non-JSON tool arguments ({delta=}, {self=})\\'\\n        )\\n      updated_args_delta = (delta.args_delta or \\'\\') + self.args_delta\\n      delta = replace(delta, args_delta=updated_args_delta)\\n    elif isinstance(self.args_delta, dict):\\n      if isinstance(delta.args_delta, str):\\n        raise UnexpectedModelBehavior(\\n          f\\'Cannot apply dict deltas to non-dict tool arguments ({delta=}, {self=})\\'\\n        )\\n      updated_args_delta = {**(delta.args_delta or {}), **self.args_delta}\\n      delta = replace(delta, args_delta=updated_args_delta)\\n    if self.tool_call_id:\\n      # Set the tool_call_id if it wasn\\'t present, otherwise error if it has changed\\n      if delta.tool_call_id is not None and delta.tool_call_id != self.tool_call_id:\\n        raise UnexpectedModelBehavior(\\n          f\\'Cannot apply a new tool_call_id to a ToolCallPartDelta that already has one ({delta=}, {self=})\\'\\n        )\\n      delta = replace(delta, tool_call_id=self.tool_call_id)\\n    # If we now have enough data to create a full ToolCallPart, do so\\n    if delta.tool_name_delta is not None and delta.args_delta is not None:\\n      return ToolCallPart(\\n        delta.tool_name_delta,\\n        delta.args_delta,\\n        delta.tool_call_id,\\n      )\\n    return delta\\n  def_apply_to_part(self, part: ToolCallPart) -> ToolCallPart:\\n\"\"\"Internal helper to apply this delta directly to a `ToolCallPart`.\"\"\"\\n    if self.tool_name_delta:\\n      # Append incremental text to the existing tool_name\\n      tool_name = part.tool_name + self.tool_name_delta\\n      part = replace(part, tool_name=tool_name)\\n    if isinstance(self.args_delta, str):\\n      if not isinstance(part.args, str):\\n        raise UnexpectedModelBehavior(f\\'Cannot apply JSON deltas to non-JSON tool arguments ({part=}, {self=})\\')\\n      updated_json = part.args + self.args_delta\\n      part = replace(part, args=updated_json)\\n    elif isinstance(self.args_delta, dict):\\n      if not isinstance(part.args, dict):\\n        raise UnexpectedModelBehavior(f\\'Cannot apply dict deltas to non-dict tool arguments ({part=}, {self=})\\')\\n      updated_dict = {**(part.args or {}), **self.args_delta}\\n      part = replace(part, args=updated_dict)\\n    if self.tool_call_id:\\n      # Replace the tool_call_id entirely if given\\n      if part.tool_call_id is not None and part.tool_call_id != self.tool_call_id:\\n        raise UnexpectedModelBehavior(\\n          f\\'Cannot apply a new tool_call_id to a ToolCallPartDelta that already has one ({part=}, {self=})\\'\\n        )\\n      part = replace(part, tool_call_id=self.tool_call_id)\\n    return part'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 77, 'url': 'https://pydantic.com'}, page_content='tool_name_delta: str | None = None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 77, 'url': 'https://pydantic.com'}, page_content='args_delta: str | dictstr[, Any] | None = None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 77, 'url': 'https://pydantic.com'}, page_content='tool_call_id: str | None = None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 77, 'url': 'https://pydantic.com'}, page_content=\"part_delta_kind: Literal['tool_call'] = 'tool_call'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 77, 'url': 'https://pydantic.com'}, page_content='as_part() -> ToolCallPart | None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 78, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 78, 'url': 'https://pydantic.com'}, page_content='defas_part(self) -> ToolCallPart | None:\\n\"\"\"Convert this delta to a fully formed `ToolCallPart` if possible, otherwise return `None`.\\n  Returns:\\n    A `ToolCallPart` if both `tool_name_delta` and `args_delta` are set, otherwise `None`.\\n  \"\"\"\\n  if self.tool_name_delta is None or self.args_delta is None:\\n    return None\\n  return ToolCallPart(\\n    self.tool_name_delta,\\n    self.args_delta,\\n    self.tool_call_id,\\n  )'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 78, 'url': 'https://pydantic.com'}, page_content='apply(part: ModelResponsePart) -> ToolCallPart'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 78, 'url': 'https://pydantic.com'}, page_content='apply(\\n  part: ModelResponsePart | ToolCallPartDelta,\\n) -> ToolCallPart | ToolCallPartDelta'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 78, 'url': 'https://pydantic.com'}, page_content='apply(\\n  part: ModelResponsePart | ToolCallPartDelta,\\n) -> ToolCallPart | ToolCallPartDelta'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 78, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 78, 'url': 'https://pydantic.com'}, page_content='defapply(self, part: ModelResponsePart | ToolCallPartDelta) -> ToolCallPart | ToolCallPartDelta:\\n\"\"\"Apply this delta to a part or delta, returning a new part or delta with the changes applied.\\n  Args:\\n    part: The existing model response part or delta to update.\\n  Returns:\\n    Either a new `ToolCallPart` or an updated `ToolCallPartDelta`.\\n  Raises:\\n    ValueError: If `part` is neither a `ToolCallPart` nor a `ToolCallPartDelta`.\\n    UnexpectedModelBehavior: If applying JSON deltas to dict arguments or vice versa.\\n  \"\"\"\\n  if isinstance(part, ToolCallPart):\\n    return self._apply_to_part(part)\\n  if isinstance(part, ToolCallPartDelta):\\n    return self._apply_to_delta(part)\\n  raise ValueError(f\\'Can only apply ToolCallPartDeltas to ToolCallParts or ToolCallPartDeltas, not {part}\\')'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 78, 'url': 'https://pydantic.com'}, page_content='ModelResponsePartDelta = Annotated[\\n  UnionTextPartDelta[, ToolCallPartDelta],\\n  Discriminator(\"part_delta_kind\"),\\n]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 78, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 78, 'url': 'https://pydantic.com'}, page_content='@dataclass\\nclassPartStartEvent:\\n\"\"\"An event indicating that a new part has started.\\n  If multiple `PartStartEvent`s are received with the same index,\\n  the new one should fully replace the old one.\\n  \"\"\"\\n  index: int\\n\"\"\"The index of the part within the overall response parts list.\"\"\"\\n  part: ModelResponsePart\\n\"\"\"The newly started `ModelResponsePart`.\"\"\"\\n  event_kind: Literal[\\'part_start\\'] = \\'part_start\\'\\n\"\"\"Event type identifier, used as a discriminator.\"\"\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 78, 'url': 'https://pydantic.com'}, page_content='index: int'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 78, 'url': 'https://pydantic.com'}, page_content='part: ModelResponsePart'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 78, 'url': 'https://pydantic.com'}, page_content=\"event_kind: Literal['part_start'] = 'part_start'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 79, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 79, 'url': 'https://pydantic.com'}, page_content='@dataclass\\nclassPartDeltaEvent:\\n\"\"\"An event indicating a delta update for an existing part.\"\"\"\\n  index: int\\n\"\"\"The index of the part within the overall response parts list.\"\"\"\\n  delta: ModelResponsePartDelta\\n\"\"\"The delta to apply to the specified part.\"\"\"\\n  event_kind: Literal[\\'part_delta\\'] = \\'part_delta\\'\\n\"\"\"Event type identifier, used as a discriminator.\"\"\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 79, 'url': 'https://pydantic.com'}, page_content='index: int'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 79, 'url': 'https://pydantic.com'}, page_content='delta: ModelResponsePartDelta'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 79, 'url': 'https://pydantic.com'}, page_content=\"event_kind: Literal['part_delta'] = 'part_delta'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 79, 'url': 'https://pydantic.com'}, page_content='ModelResponseStreamEvent = Annotated[\\n  UnionPartStartEvent[, PartDeltaEvent],\\n  Discriminator(\"event_kind\"),\\n]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 79, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 79, 'url': 'https://pydantic.com'}, page_content='@dataclass\\nclassFunctionToolCallEvent:\\n\"\"\"An event indicating the start to a call to a function tool.\"\"\"\\n  part: ToolCallPart\\n\"\"\"The (function) tool call to make.\"\"\"\\n  call_id: str = field(init=False)\\n\"\"\"An ID used for matching details about the call to its result. If present, defaults to the part\\'s tool_call_id.\"\"\"\\n  event_kind: Literal[\\'function_tool_call\\'] = \\'function_tool_call\\'\\n\"\"\"Event type identifier, used as a discriminator.\"\"\"\\n  def__post_init__(self):\\n    self.call_id = self.part.tool_call_id or str(uuid.uuid4())'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 79, 'url': 'https://pydantic.com'}, page_content='part: ToolCallPart'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 79, 'url': 'https://pydantic.com'}, page_content='call_id: str = field(init=False)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 79, 'url': 'https://pydantic.com'}, page_content='event_kind: Literal[\"function_tool_call\"] = (\\n  \"function_tool_call\"\\n)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 79, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 79, 'url': 'https://pydantic.com'}, page_content='@dataclass\\nclassFunctionToolResultEvent:\\n\"\"\"An event indicating the result of a function tool call.\"\"\"\\n  result: ToolReturnPart | RetryPromptPart\\n\"\"\"The result of the call to the function tool.\"\"\"\\n  call_id: str\\n\"\"\"An ID used to match the result to its original call.\"\"\"\\n  event_kind: Literal[\\'function_tool_result\\'] = \\'function_tool_result\\'\\n\"\"\"Event type identifier, used as a discriminator.\"\"\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 79, 'url': 'https://pydantic.com'}, page_content='result: ToolReturnPart | RetryPromptPart'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 79, 'url': 'https://pydantic.com'}, page_content='call_id: str'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 79, 'url': 'https://pydantic.com'}, page_content='event_kind: Literal[\"function_tool_result\"] = (\\n  \"function_tool_result\"\\n)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 80, 'url': 'https://pydantic.com'}, page_content='ResultDataT = TypeVar(\\n  \"ResultDataT\", default=str, covariant=True\\n)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 80, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 80, 'url': 'https://pydantic.com'}, page_content='@dataclass\\nclassStreamedRunResult(Generic[AgentDepsT, ResultDataT]):\\n\"\"\"Result of a streamed run that returns structured data via a tool call.\"\"\"\\n  _all_messages: list[_messages.ModelMessage]\\n  _new_message_index: int\\n  _usage_limits: UsageLimits | None\\n  _stream_response: models.StreamedResponse\\n  _result_schema: _result.ResultSchema[ResultDataT] | None\\n  _run_ctx: RunContext[AgentDepsT]\\n  _result_validators: list[_result.ResultValidator[AgentDepsT, ResultDataT]]\\n  _result_tool_name: str | None\\n  _on_complete: Callable[[], Awaitable[None]]\\n  _initial_run_ctx_usage: Usage = field(init=False)\\n  is_complete: bool = field(default=False, init=False)\\n\"\"\"Whether the stream has all been received.\\n  This is set to `True` when one of\\n  [`stream`][pydantic_ai.result.StreamedRunResult.stream],\\n  [`stream_text`][pydantic_ai.result.StreamedRunResult.stream_text],\\n  [`stream_structured`][pydantic_ai.result.StreamedRunResult.stream_structured] or\\n  [`get_data`][pydantic_ai.result.StreamedRunResult.get_data] completes.\\n  \"\"\"\\n  def__post_init__(self):\\n    self._initial_run_ctx_usage = copy(self._run_ctx.usage)\\n  defall_messages(self, *, result_tool_return_content: str | None = None) -> list[_messages.ModelMessage]:\\n\"\"\"Return the history of _messages.\\n    Args:\\n      result_tool_return_content: The return content of the tool call to set in the last message.\\n        This provides a convenient way to modify the content of the result tool call if you want to continue\\n        the conversation and want to set the response to the result tool call. If `None`, the last message will\\n        not be modified.\\n    Returns:\\n      List of messages.\\n    \"\"\"\\n    # this is a method to be consistent with the other methods\\n    if result_tool_return_content is not None:\\n      raise NotImplementedError(\\'Setting result tool return content is not supported for this result type.\\')\\n    return self._all_messages\\n  defall_messages_json(self, *, result_tool_return_content: str | None = None) -> bytes:\\n\"\"\"Return all messages from [`all_messages`][pydantic_ai.result.StreamedRunResult.all_messages] as JSON bytes.\\n    Args:\\n      result_tool_return_content: The return content of the tool call to set in the last message.\\n        This provides a convenient way to modify the content of the result tool call if you want to continue\\n        the conversation and want to set the response to the result tool call. If `None`, the last message will\\n        not be modified.\\n    Returns:\\n      JSON bytes representing the messages.\\n    \"\"\"\\n    return _messages.ModelMessagesTypeAdapter.dump_json(\\n      self.all_messages(result_tool_return_content=result_tool_return_content)\\n    )\\n  defnew_messages(self, *, result_tool_return_content: str | None = None) -> list[_messages.ModelMessage]:\\n\"\"\"Return new messages associated with this run.\\n    Messages from older runs are excluded.\\n    Args:\\n      result_tool_return_content: The return content of the tool call to set in the last message.\\n        This provides a convenient way to modify the content of the result tool call if you want to continue\\n        the conversation and want to set the response to the result tool call. If `None`, the last message will\\n        not be modified.\\n    Returns:\\n      List of new messages.\\n    \"\"\"\\n    return self.all_messages(result_tool_return_content=result_tool_return_content)[self._new_message_index :]\\n  defnew_messages_json(self, *, result_tool_return_content: str | None = None) -> bytes:\\n\"\"\"Return new messages from [`new_messages`][pydantic_ai.result.StreamedRunResult.new_messages] as JSON bytes.\\n    Args:\\n      result_tool_return_content: The return content of the tool call to set in the last message.\\n        This provides a convenient way to modify the content of the result tool call if you want to continue\\n        the conversation and want to set the response to the result tool call. If `None`, the last message will\\n        not be modified.\\n    Returns:\\n      JSON bytes representing the new messages.\\n    \"\"\"\\n    return _messages.ModelMessagesTypeAdapter.dump_json(\\n      self.new_messages(result_tool_return_content=result_tool_return_content)\\n    )\\n  async defstream(self, *, debounce_by: float | None = 0.1) -> AsyncIterator[ResultDataT]:\\n\"\"\"Stream the response as an async iterable.\\n    The pydantic validator for structured data will be called in\\n    partial mode\\n    on each iteration.\\n    Args:\\n      debounce_by: by how much (if at all) to debounce/group the response chunks by. `None` means no debouncing.\\n        Debouncing is particularly important for long structured responses to reduce the overhead of\\n        performing validation as each token is received.\\n    Returns:\\n      An async iterable of the response data.\\n    \"\"\"\\n    async for structured_message, is_last in self.stream_structured(debounce_by=debounce_by):\\n      result = await self.validate_structured_result(structured_message, allow_partial=not is_last)\\n      yield result\\n  async defstream_text(self, *, delta: bool = False, debounce_by: float | None = 0.1) -> AsyncIterator[str]:\\n\"\"\"Stream the text result as an async iterable.\\n    !!! note\\n      Result validators will NOT be called on the text result if `delta=True`.\\n    Args:\\n      delta: if `True`, yield each chunk of text as it is received, if `False` (default), yield the full text\\n        up to the current point.\\n      debounce_by: by how much (if at all) to debounce/group the response chunks by. `None` means no debouncing.\\n        Debouncing is particularly important for long structured responses to reduce the overhead of\\n        performing validation as each token is received.\\n    \"\"\"\\n    if self._result_schema and not self._result_schema.allow_text_result:\\n      raise exceptions.UserError(\\'stream_text() can only be used with text responses\\')\\n    with _logfire.span(\\'response stream text\\') as lf_span:\\n      if delta:\\n        async for text in self._stream_response_text(delta=delta, debounce_by=debounce_by):\\n          yield text\\n      else:\\n        combined_validated_text = \\'\\'\\n        async for text in self._stream_response_text(delta=delta, debounce_by=debounce_by):\\n          combined_validated_text = await self._validate_text_result(text)\\n          yield combined_validated_text\\n        lf_span.set_attribute(\\'combined_text\\', combined_validated_text)\\n      await self._marked_completed(self._stream_response.get())\\n  async defstream_structured(\\n    self, *, debounce_by: float | None = 0.1\\n  ) -> AsyncIterator[tuple[_messages.ModelResponse, bool]]:\\n\"\"\"Stream the response as an async iterable of Structured LLM Messages.\\n    Args:\\n      debounce_by: by how much (if at all) to debounce/group the response chunks by. `None` means no debouncing.\\n        Debouncing is particularly important for long structured responses to reduce the overhead of\\n        performing validation as each token is received.\\n    Returns:\\n      An async iterable of the structured response message and whether that is the last message.\\n    \"\"\"\\n    with _logfire.span(\\'response stream structured\\') as lf_span:\\n      # if the message currently has any parts with content, yield before streaming\\n      msg = self._stream_response.get()\\n      for part in msg.parts:\\n        if part.has_content():\\n          yield msg, False\\n          break\\n      async for msg in self._stream_response_structured(debounce_by=debounce_by):\\n        yield msg, False\\n      msg = self._stream_response.get()\\n      yield msg, True\\n      lf_span.set_attribute(\\'structured_response\\', msg)\\n      await self._marked_completed(msg)\\n  async defget_data(self) -> ResultDataT:\\n\"\"\"Stream the whole response, validate and return it.\"\"\"\\n    usage_checking_stream = _get_usage_checking_stream_response(\\n      self._stream_response, self._usage_limits, self.usage\\n    )\\n    async for _ in usage_checking_stream:\\n      pass\\n    message = self._stream_response.get()\\n    await self._marked_completed(message)\\n    return await self.validate_structured_result(message)\\n  defusage(self) -> Usage:\\n\"\"\"Return the usage of the whole run.\\n    !!! note\\n      This won\\'t return the full usage until the stream is finished.\\n    \"\"\"\\n    return self._initial_run_ctx_usage + self._stream_response.usage()\\n  deftimestamp(self) -> datetime:\\n\"\"\"Get the timestamp of the response.\"\"\"\\n    return self._stream_response.timestamp\\n  async defvalidate_structured_result(\\n    self, message: _messages.ModelResponse, *, allow_partial: bool = False\\n  ) -> ResultDataT:\\n\"\"\"Validate a structured result message.\"\"\"\\n    if self._result_schema is not None and self._result_tool_name is not None:\\n      match = self._result_schema.find_named_tool(message.parts, self._result_tool_name)\\n      if match is None:\\n        raise exceptions.UnexpectedModelBehavior(\\n          f\\'Invalid response, unable to find tool: {self._result_schema.tool_names()}\\'\\n        )\\n      call, result_tool = match\\n      result_data = result_tool.validate(call, allow_partial=allow_partial, wrap_validation_errors=False)\\n      for validator in self._result_validators:\\n        result_data = await validator.validate(result_data, call, self._run_ctx)\\n      return result_data\\n    else:\\n      text = \\'\\\\n\\\\n\\'.join(x.content for x in message.parts if isinstance(x, _messages.TextPart))\\n      for validator in self._result_validators:\\n        text = await validator.validate(\\n          text,\\n          None,\\n          self._run_ctx,\\n        )\\n      # Since there is no result tool, we can assume that str is compatible with ResultDataT\\n      return cast(ResultDataT, text)\\n  async def_validate_text_result(self, text: str) -> str:\\n    for validator in self._result_validators:\\n      text = await validator.validate(\\n        text,\\n        None,\\n        self._run_ctx,\\n      )\\n    return text\\n  async def_marked_completed(self, message: _messages.ModelResponse) -> None:\\n    self.is_complete = True\\n    self._all_messages.append(message)\\n    await self._on_complete()\\n  async def_stream_response_structured(\\n    self, *, debounce_by: float | None = 0.1\\n  ) -> AsyncIterator[_messages.ModelResponse]:\\n    async with _utils.group_by_temporal(self._stream_response, debounce_by) as group_iter:\\n      async for _items in group_iter:\\n        yield self._stream_response.get()\\n  async def_stream_response_text(\\n    self, *, delta: bool = False, debounce_by: float | None = 0.1\\n  ) -> AsyncIterator[str]:\\n\"\"\"Stream the response as an async iterable of text.\"\"\"\\n    # Define a \"merged\" version of the iterator that will yield items that have already been retrieved\\n    # and items that we receive while streaming. We define a dedicated async iterator for this so we can\\n    # pass the combined stream to the group_by_temporal function within `_stream_text_deltas` below.\\n    async def_stream_text_deltas_ungrouped() -> AsyncIterator[tuple[str, int]]:\\n      # yields tuples of (text_content, part_index)\\n      # we don\\'t currently make use of the part_index, but in principle this may be useful\\n      # so we retain it here for now to make possible future refactors simpler\\n      msg = self._stream_response.get()\\n      for i, part in enumerate(msg.parts):\\n        if isinstance(part, _messages.TextPart) and part.content:\\n          yield part.content, i\\n      async for event in self._stream_response:\\n        if (\\n          isinstance(event, _messages.PartStartEvent)\\n          and isinstance(event.part, _messages.TextPart)\\n          and event.part.content\\n        ):\\n          yield event.part.content, event.index\\n        elif (\\n          isinstance(event, _messages.PartDeltaEvent)\\n          and isinstance(event.delta, _messages.TextPartDelta)\\n          and event.delta.content_delta\\n        ):\\n          yield event.delta.content_delta, event.index\\n    async def_stream_text_deltas() -> AsyncIterator[str]:\\n      async with _utils.group_by_temporal(_stream_text_deltas_ungrouped(), debounce_by) as group_iter:\\n        async for items in group_iter:\\n          # Note: we are currently just dropping the part index on the group here\\n          yield \\'\\'.join([content for content, _ in items])\\n    if delta:\\n      async for text in _stream_text_deltas():\\n        yield text\\n    else:\\n      # a quick benchmark shows it\\'s faster to build up a string with concat when we\\'re\\n      # yielding at each step\\n      deltas: list[str] = []\\n      async for text in _stream_text_deltas():\\n        deltas.append(text)\\n        yield \\'\\'.join(deltas)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 80, 'url': 'https://pydantic.com'}, page_content='is_complete: bool = field(default=False, init=False)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 80, 'url': 'https://pydantic.com'}, page_content='all_messages(\\n  *, result_tool_return_content: str | None = None\\n) -> listModelMessage[]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 80, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 80, 'url': 'https://pydantic.com'}, page_content='defall_messages(self, *, result_tool_return_content: str | None = None) -> list[_messages.ModelMessage]:\\n\"\"\"Return the history of _messages.\\n  Args:\\n    result_tool_return_content: The return content of the tool call to set in the last message.\\n      This provides a convenient way to modify the content of the result tool call if you want to continue\\n      the conversation and want to set the response to the result tool call. If `None`, the last message will\\n      not be modified.\\n  Returns:\\n    List of messages.\\n  \"\"\"\\n  # this is a method to be consistent with the other methods\\n  if result_tool_return_content is not None:\\n    raise NotImplementedError(\\'Setting result tool return content is not supported for this result type.\\')\\n  return self._all_messages'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 80, 'url': 'https://pydantic.com'}, page_content='all_messages_json(\\n  *, result_tool_return_content: str | None = None\\n) -> bytes'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 81, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 81, 'url': 'https://pydantic.com'}, page_content='defall_messages_json(self, *, result_tool_return_content: str | None = None) -> bytes:\\n\"\"\"Return all messages from [`all_messages`][pydantic_ai.result.StreamedRunResult.all_messages] as JSON bytes.\\n  Args:\\n    result_tool_return_content: The return content of the tool call to set in the last message.\\n      This provides a convenient way to modify the content of the result tool call if you want to continue\\n      the conversation and want to set the response to the result tool call. If `None`, the last message will\\n      not be modified.\\n  Returns:\\n    JSON bytes representing the messages.\\n  \"\"\"\\n  return _messages.ModelMessagesTypeAdapter.dump_json(\\n    self.all_messages(result_tool_return_content=result_tool_return_content)\\n  )'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 81, 'url': 'https://pydantic.com'}, page_content='new_messages(\\n  *, result_tool_return_content: str | None = None\\n) -> listModelMessage[]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 81, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 81, 'url': 'https://pydantic.com'}, page_content='defnew_messages(self, *, result_tool_return_content: str | None = None) -> list[_messages.ModelMessage]:\\n\"\"\"Return new messages associated with this run.\\n  Messages from older runs are excluded.\\n  Args:\\n    result_tool_return_content: The return content of the tool call to set in the last message.\\n      This provides a convenient way to modify the content of the result tool call if you want to continue\\n      the conversation and want to set the response to the result tool call. If `None`, the last message will\\n      not be modified.\\n  Returns:\\n    List of new messages.\\n  \"\"\"\\n  return self.all_messages(result_tool_return_content=result_tool_return_content)[self._new_message_index :]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 81, 'url': 'https://pydantic.com'}, page_content='new_messages_json(\\n  *, result_tool_return_content: str | None = None\\n) -> bytes'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 82, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 82, 'url': 'https://pydantic.com'}, page_content='defnew_messages_json(self, *, result_tool_return_content: str | None = None) -> bytes:\\n\"\"\"Return new messages from [`new_messages`][pydantic_ai.result.StreamedRunResult.new_messages] as JSON bytes.\\n  Args:\\n    result_tool_return_content: The return content of the tool call to set in the last message.\\n      This provides a convenient way to modify the content of the result tool call if you want to continue\\n      the conversation and want to set the response to the result tool call. If `None`, the last message will\\n      not be modified.\\n  Returns:\\n    JSON bytes representing the new messages.\\n  \"\"\"\\n  return _messages.ModelMessagesTypeAdapter.dump_json(\\n    self.new_messages(result_tool_return_content=result_tool_return_content)\\n  )'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 82, 'url': 'https://pydantic.com'}, page_content='stream(\\n  *, debounce_by: float | None = 0.1\\n) -> AsyncIteratorResultDataT[]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 82, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 82, 'url': 'https://pydantic.com'}, page_content='async defstream(self, *, debounce_by: float | None = 0.1) -> AsyncIterator[ResultDataT]:\\n\"\"\"Stream the response as an async iterable.\\n  The pydantic validator for structured data will be called in\\n  partial mode\\n  on each iteration.\\n  Args:\\n    debounce_by: by how much (if at all) to debounce/group the response chunks by. `None` means no debouncing.\\n      Debouncing is particularly important for long structured responses to reduce the overhead of\\n      performing validation as each token is received.\\n  Returns:\\n    An async iterable of the response data.\\n  \"\"\"\\n  async for structured_message, is_last in self.stream_structured(debounce_by=debounce_by):\\n    result = await self.validate_structured_result(structured_message, allow_partial=not is_last)\\n    yield result'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 82, 'url': 'https://pydantic.com'}, page_content='stream_text(\\n  *, delta: bool = False, debounce_by: float | None = 0.1\\n) -> AsyncIteratorstr[]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 82, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 82, 'url': 'https://pydantic.com'}, page_content='async defstream_text(self, *, delta: bool = False, debounce_by: float | None = 0.1) -> AsyncIterator[str]:\\n\"\"\"Stream the text result as an async iterable.\\n  !!! note\\n    Result validators will NOT be called on the text result if `delta=True`.\\n  Args:\\n    delta: if `True`, yield each chunk of text as it is received, if `False` (default), yield the full text\\n      up to the current point.\\n    debounce_by: by how much (if at all) to debounce/group the response chunks by. `None` means no debouncing.\\n      Debouncing is particularly important for long structured responses to reduce the overhead of\\n      performing validation as each token is received.\\n  \"\"\"\\n  if self._result_schema and not self._result_schema.allow_text_result:\\n    raise exceptions.UserError(\\'stream_text() can only be used with text responses\\')\\n  with _logfire.span(\\'response stream text\\') as lf_span:\\n    if delta:\\n      async for text in self._stream_response_text(delta=delta, debounce_by=debounce_by):\\n        yield text\\n    else:\\n      combined_validated_text = \\'\\'\\n      async for text in self._stream_response_text(delta=delta, debounce_by=debounce_by):\\n        combined_validated_text = await self._validate_text_result(text)\\n        yield combined_validated_text\\n      lf_span.set_attribute(\\'combined_text\\', combined_validated_text)\\n    await self._marked_completed(self._stream_response.get())'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 82, 'url': 'https://pydantic.com'}, page_content='stream_structured(\\n  *, debounce_by: float | None = 0.1\\n) -> AsyncIteratortuple[ModelResponse[, bool]]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 83, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 83, 'url': 'https://pydantic.com'}, page_content='async defstream_structured(\\n  self, *, debounce_by: float | None = 0.1\\n) -> AsyncIterator[tuple[_messages.ModelResponse, bool]]:\\n\"\"\"Stream the response as an async iterable of Structured LLM Messages.\\n  Args:\\n    debounce_by: by how much (if at all) to debounce/group the response chunks by. `None` means no debouncing.\\n      Debouncing is particularly important for long structured responses to reduce the overhead of\\n      performing validation as each token is received.\\n  Returns:\\n    An async iterable of the structured response message and whether that is the last message.\\n  \"\"\"\\n  with _logfire.span(\\'response stream structured\\') as lf_span:\\n    # if the message currently has any parts with content, yield before streaming\\n    msg = self._stream_response.get()\\n    for part in msg.parts:\\n      if part.has_content():\\n        yield msg, False\\n        break\\n    async for msg in self._stream_response_structured(debounce_by=debounce_by):\\n      yield msg, False\\n    msg = self._stream_response.get()\\n    yield msg, True\\n    lf_span.set_attribute(\\'structured_response\\', msg)\\n    await self._marked_completed(msg)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 83, 'url': 'https://pydantic.com'}, page_content='get_data() -> ResultDataT'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 83, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 83, 'url': 'https://pydantic.com'}, page_content='async defget_data(self) -> ResultDataT:\\n\"\"\"Stream the whole response, validate and return it.\"\"\"\\n  usage_checking_stream = _get_usage_checking_stream_response(\\n    self._stream_response, self._usage_limits, self.usage\\n  )\\n  async for _ in usage_checking_stream:\\n    pass\\n  message = self._stream_response.get()\\n  await self._marked_completed(message)\\n  return await self.validate_structured_result(message)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 83, 'url': 'https://pydantic.com'}, page_content='usage() -> Usage'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 83, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 83, 'url': 'https://pydantic.com'}, page_content='defusage(self) -> Usage:\\n\"\"\"Return the usage of the whole run.\\n  !!! note\\n    This won\\'t return the full usage until the stream is finished.\\n  \"\"\"\\n  return self._initial_run_ctx_usage + self._stream_response.usage()'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 83, 'url': 'https://pydantic.com'}, page_content='timestamp() -> datetime'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 83, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 83, 'url': 'https://pydantic.com'}, page_content='deftimestamp(self) -> datetime:\\n\"\"\"Get the timestamp of the response.\"\"\"\\n  return self._stream_response.timestamp'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 83, 'url': 'https://pydantic.com'}, page_content='validate_structured_result(\\n  message: ModelResponse, *, allow_partial: bool = False\\n) -> ResultDataT'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 83, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 83, 'url': 'https://pydantic.com'}, page_content='async defvalidate_structured_result(\\n  self, message: _messages.ModelResponse, *, allow_partial: bool = False\\n) -> ResultDataT:\\n\"\"\"Validate a structured result message.\"\"\"\\n  if self._result_schema is not None and self._result_tool_name is not None:\\n    match = self._result_schema.find_named_tool(message.parts, self._result_tool_name)\\n    if match is None:\\n      raise exceptions.UnexpectedModelBehavior(\\n        f\\'Invalid response, unable to find tool: {self._result_schema.tool_names()}\\'\\n      )\\n    call, result_tool = match\\n    result_data = result_tool.validate(call, allow_partial=allow_partial, wrap_validation_errors=False)\\n    for validator in self._result_validators:\\n      result_data = await validator.validate(result_data, call, self._run_ctx)\\n    return result_data\\n  else:\\n    text = \\'\\\\n\\\\n\\'.join(x.content for x in message.parts if isinstance(x, _messages.TextPart))\\n    for validator in self._result_validators:\\n      text = await validator.validate(\\n        text,\\n        None,\\n        self._run_ctx,\\n      )\\n    # Since there is no result tool, we can assume that str is compatible with ResultDataT\\n    return cast(ResultDataT, text)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 84, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 84, 'url': 'https://pydantic.com'}, page_content='classModelSettings(TypedDict, total=False):\\n\"\"\"Settings to configure an LLM.\\n  Here we include only settings which apply to multiple models / model providers,\\n  though not all of these settings are supported by all models.\\n  \"\"\"\\n  max_tokens: int\\n\"\"\"The maximum number of tokens to generate before stopping.\\n  Supported by:\\n  * Gemini\\n  * Anthropic\\n  * OpenAI\\n  * Groq\\n  * Cohere\\n  * Mistral\\n  \"\"\"\\n  temperature: float\\n\"\"\"Amount of randomness injected into the response.\\n  Use `temperature` closer to `0.0` for analytical / multiple choice, and closer to a model\\'s\\n  maximum `temperature` for creative and generative tasks.\\n  Note that even with `temperature` of `0.0`, the results will not be fully deterministic.\\n  Supported by:\\n  * Gemini\\n  * Anthropic\\n  * OpenAI\\n  * Groq\\n  * Cohere\\n  * Mistral\\n  \"\"\"\\n  top_p: float\\n\"\"\"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.\\n  So 0.1 means only the tokens comprising the top 10% probability mass are considered.\\n  You should either alter `temperature` or `top_p`, but not both.\\n  Supported by:\\n  * Gemini\\n  * Anthropic\\n  * OpenAI\\n  * Groq\\n  * Cohere\\n  * Mistral\\n  \"\"\"\\n  timeout: float | Timeout\\n\"\"\"Override the client-level default timeout for a request, in seconds.\\n  Supported by:\\n  * Gemini\\n  * Anthropic\\n  * OpenAI\\n  * Groq\\n  * Mistral\\n  \"\"\"\\n  parallel_tool_calls: bool\\n\"\"\"Whether to allow parallel tool calls.\\n  Supported by:\\n  * OpenAI (some models, not o1)\\n  * Groq\\n  * Anthropic\\n  \"\"\"\\n  seed: int\\n\"\"\"The random seed to use for the model, theoretically allowing for deterministic results.\\n  Supported by:\\n  * OpenAI\\n  * Groq\\n  * Cohere\\n  * Mistral\\n  \"\"\"\\n  presence_penalty: float\\n\"\"\"Penalize new tokens based on whether they have appeared in the text so far.\\n  Supported by:\\n  * OpenAI\\n  * Groq\\n  * Cohere\\n  * Gemini\\n  * Mistral\\n  \"\"\"\\n  frequency_penalty: float\\n\"\"\"Penalize new tokens based on their existing frequency in the text so far.\\n  Supported by:\\n  * OpenAI\\n  * Groq\\n  * Cohere\\n  * Gemini\\n  * Mistral\\n  \"\"\"\\n  logit_bias: dict[str, int]\\n\"\"\"Modify the likelihood of specified tokens appearing in the completion.\\n  Supported by:\\n  * OpenAI\\n  * Groq\\n  \"\"\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 84, 'url': 'https://pydantic.com'}, page_content='max_tokens: int'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 84, 'url': 'https://pydantic.com'}, page_content='temperature: float'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 84, 'url': 'https://pydantic.com'}, page_content='top_p: float'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 84, 'url': 'https://pydantic.com'}, page_content='timeout: float | Timeout'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 85, 'url': 'https://pydantic.com'}, page_content='parallel_tool_calls: bool'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 85, 'url': 'https://pydantic.com'}, page_content='seed: int'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 85, 'url': 'https://pydantic.com'}, page_content='presence_penalty: float'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 85, 'url': 'https://pydantic.com'}, page_content='frequency_penalty: float'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 85, 'url': 'https://pydantic.com'}, page_content='logit_bias: dictstr[, int]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 86, 'url': 'https://pydantic.com'}, page_content='AgentDepsT = TypeVar(\\n  \"AgentDepsT\", default=None, contravariant=True\\n)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 86, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 86, 'url': 'https://pydantic.com'}, page_content='@dataclasses.dataclass\\nclassRunContext(Generic[AgentDepsT]):\\n\"\"\"Information about the current call.\"\"\"\\n  deps: AgentDepsT\\n\"\"\"Dependencies for the agent.\"\"\"\\n  model: models.Model\\n\"\"\"The model used in this run.\"\"\"\\n  usage: Usage\\n\"\"\"LLM usage associated with the run.\"\"\"\\n  prompt: str\\n\"\"\"The original user prompt passed to the run.\"\"\"\\n  messages: list[_messages.ModelMessage] = field(default_factory=list)\\n\"\"\"Messages exchanged in the conversation so far.\"\"\"\\n  tool_name: str | None = None\\n\"\"\"Name of the tool being called.\"\"\"\\n  retry: int = 0\\n\"\"\"Number of retries so far.\"\"\"\\n  run_step: int = 0\\n\"\"\"The current step in the run.\"\"\"\\n  defreplace_with(\\n    self, retry: int | None = None, tool_name: str | None | _utils.Unset = _utils.UNSET\\n  ) -> RunContext[AgentDepsT]:\\n    # Create a new `RunContext` a new `retry` value and `tool_name`.\\n    kwargs = {}\\n    if retry is not None:\\n      kwargs[\\'retry\\'] = retry\\n    if tool_name is not _utils.UNSET:\\n      kwargs[\\'tool_name\\'] = tool_name\\n    return dataclasses.replace(self, **kwargs)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 86, 'url': 'https://pydantic.com'}, page_content='deps: AgentDepsT'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 86, 'url': 'https://pydantic.com'}, page_content='model: Model'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 86, 'url': 'https://pydantic.com'}, page_content='usage: Usage'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 86, 'url': 'https://pydantic.com'}, page_content='prompt: str'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 86, 'url': 'https://pydantic.com'}, page_content='messages: listModelMessage[] = field(default_factory=list)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 86, 'url': 'https://pydantic.com'}, page_content='tool_name: str | None = None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 86, 'url': 'https://pydantic.com'}, page_content='retry: int = 0'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 86, 'url': 'https://pydantic.com'}, page_content='run_step: int = 0'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 86, 'url': 'https://pydantic.com'}, page_content=\"ToolParams = ParamSpec('ToolParams', default=...)\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 86, 'url': 'https://pydantic.com'}, page_content='SystemPromptFunc = Union[\\n  Callable[RunContext[AgentDepsT[]], str],\\n  Callable[RunContext[AgentDepsT[]], Awaitablestr[]],\\n  Callable[], str[],\\n  Callable[], Awaitable[str[]],\\n]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 87, 'url': 'https://pydantic.com'}, page_content='ToolFuncContext = Callable[\\n  ConcatenateRunContext[AgentDepsT[], ToolParams], Any\\n]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 87, 'url': 'https://pydantic.com'}, page_content='ToolFuncPlain = CallableToolParams[, Any]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 87, 'url': 'https://pydantic.com'}, page_content='ToolFuncEither = Union[\\n  ToolFuncContextAgentDepsT[, ToolParams],\\n  ToolFuncPlainToolParams[],\\n]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 87, 'url': 'https://pydantic.com'}, page_content='ToolPrepareFunc: TypeAlias = (\\n  \"Callable[[RunContext[AgentDepsT], ToolDefinition], Awaitable[ToolDefinition | None]]\"\\n)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 87, 'url': 'https://pydantic.com'}, page_content=\"fromtypingimport Union\\nfrompydantic_aiimport RunContext, Tool\\nfrompydantic_ai.toolsimport ToolDefinition\\nasync defonly_if_42(\\n  ctx: RunContext[int], tool_def: ToolDefinition\\n) -> Union[ToolDefinition, None]:\\n  if ctx.deps == 42:\\n    return tool_def\\ndefhitchhiker(ctx: RunContext[int], answer: str) -> str:\\n  return f'{ctx.deps}{answer}'\\nhitchhiker = Tool(hitchhiker, prepare=only_if_42)\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 87, 'url': 'https://pydantic.com'}, page_content='DocstringFormat = Literal[\\n  \"google\", \"numpy\", \"sphinx\", \"auto\"\\n]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 88, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 88, 'url': 'https://pydantic.com'}, page_content='@dataclass(init=False)\\nclassTool(Generic[AgentDepsT]):\\n\"\"\"A tool function for an agent.\"\"\"\\n  function: ToolFuncEither[AgentDepsT]\\n  takes_ctx: bool\\n  max_retries: int | None\\n  name: str\\n  description: str\\n  prepare: ToolPrepareFunc[AgentDepsT] | None\\n  docstring_format: DocstringFormat\\n  require_parameter_descriptions: bool\\n  _is_async: bool = field(init=False)\\n  _single_arg_name: str | None = field(init=False)\\n  _positional_fields: list[str] = field(init=False)\\n  _var_positional_field: str | None = field(init=False)\\n  _validator: SchemaValidator = field(init=False, repr=False)\\n  _parameters_json_schema: ObjectJsonSchema = field(init=False)\\n  # TODO: Move this state off the Tool class, which is otherwise stateless.\\n  #  This should be tracked inside a specific agent run, not the tool.\\n  current_retry: int = field(default=0, init=False)\\n  def__init__(\\n    self,\\n    function: ToolFuncEither[AgentDepsT],\\n    *,\\n    takes_ctx: bool | None = None,\\n    max_retries: int | None = None,\\n    name: str | None = None,\\n    description: str | None = None,\\n    prepare: ToolPrepareFunc[AgentDepsT] | None = None,\\n    docstring_format: DocstringFormat = \\'auto\\',\\n    require_parameter_descriptions: bool = False,\\n  ):\\n\"\"\"Create a new tool instance.\\n    Example usage:'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 88, 'url': 'https://pydantic.com'}, page_content='or with a custom prepare method:'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 88, 'url': 'https://pydantic.com'}, page_content='Args:\\n      function: The Python function to call as the tool.\\n      takes_ctx: Whether the function takes a [`RunContext`][pydantic_ai.tools.RunContext] first argument,\\n        this is inferred if unset.\\n      max_retries: Maximum number of retries allowed for this tool, set to the agent default if `None`.\\n      name: Name of the tool, inferred from the function if `None`.\\n      description: Description of the tool, inferred from the function if `None`.\\n      prepare: custom method to prepare the tool definition for each step, return `None` to omit this\\n        tool from a given step. This is useful if you want to customise a tool at call time,\\n        or omit it completely from a step. See [`ToolPrepareFunc`][pydantic_ai.tools.ToolPrepareFunc].\\n      docstring_format: The format of the docstring, see [`DocstringFormat`][pydantic_ai.tools.DocstringFormat].\\n        Defaults to `\\'auto\\'`, such that the format is inferred from the structure of the docstring.\\n      require_parameter_descriptions: If True, raise an error if a parameter description is missing. Defaults to False.\\n    \"\"\"\\n    if takes_ctx is None:\\n      takes_ctx = _pydantic.takes_ctx(function)\\n    f = _pydantic.function_schema(function, takes_ctx, docstring_format, require_parameter_descriptions)\\n    self.function = function\\n    self.takes_ctx = takes_ctx\\n    self.max_retries = max_retries\\n    self.name = name or function.__name__\\n    self.description = description or f[\\'description\\']\\n    self.prepare = prepare\\n    self.docstring_format = docstring_format\\n    self.require_parameter_descriptions = require_parameter_descriptions\\n    self._is_async = inspect.iscoroutinefunction(self.function)\\n    self._single_arg_name = f[\\'single_arg_name\\']\\n    self._positional_fields = f[\\'positional_fields\\']\\n    self._var_positional_field = f[\\'var_positional_field\\']\\n    self._validator = f[\\'validator\\']\\n    self._parameters_json_schema = f[\\'json_schema\\']\\n  async defprepare_tool_def(self, ctx: RunContext[AgentDepsT]) -> ToolDefinition | None:\\n\"\"\"Get the tool definition.\\n    By default, this method creates a tool definition, then either returns it, or calls `self.prepare`\\n    if it\\'s set.\\n    Returns:\\n      return a `ToolDefinition` or `None` if the tools should not be registered for this run.\\n    \"\"\"\\n    tool_def = ToolDefinition(\\n      name=self.name,\\n      description=self.description,\\n      parameters_json_schema=self._parameters_json_schema,\\n    )\\n    if self.prepare is not None:\\n      return await self.prepare(ctx, tool_def)\\n    else:\\n      return tool_def\\n  async defrun(\\n    self, message: _messages.ToolCallPart, run_context: RunContext[AgentDepsT]\\n  ) -> _messages.ToolReturnPart | _messages.RetryPromptPart:\\n\"\"\"Run the tool function asynchronously.\"\"\"\\n    try:\\n      if isinstance(message.args, str):\\n        args_dict = self._validator.validate_json(message.args)\\n      else:\\n        args_dict = self._validator.validate_python(message.args)\\n    except ValidationError as e:\\n      return self._on_error(e, message)\\n    args, kwargs = self._call_args(args_dict, message, run_context)\\n    try:\\n      if self._is_async:\\n        function = cast(Callable[[Any], Awaitable[str]], self.function)\\n        response_content = await function(*args, **kwargs)\\n      else:\\n        function = cast(Callable[[Any], str], self.function)\\n        response_content = await _utils.run_in_executor(function, *args, **kwargs)\\n    except ModelRetry as e:\\n      return self._on_error(e, message)\\n    self.current_retry = 0\\n    return _messages.ToolReturnPart(\\n      tool_name=message.tool_name,\\n      content=response_content,\\n      tool_call_id=message.tool_call_id,\\n    )\\n  def_call_args(\\n    self,\\n    args_dict: dict[str, Any],\\n    message: _messages.ToolCallPart,\\n    run_context: RunContext[AgentDepsT],\\n  ) -> tuple[list[Any], dict[str, Any]]:\\n    if self._single_arg_name:\\n      args_dict = {self._single_arg_name: args_dict}\\n    ctx = dataclasses.replace(run_context, retry=self.current_retry, tool_name=message.tool_name)\\n    args = [ctx] if self.takes_ctx else []\\n    for positional_field in self._positional_fields:\\n      args.append(args_dict.pop(positional_field))\\n    if self._var_positional_field:\\n      args.extend(args_dict.pop(self._var_positional_field))\\n    return args, args_dict\\n  def_on_error(\\n    self, exc: ValidationError | ModelRetry, call_message: _messages.ToolCallPart\\n  ) -> _messages.RetryPromptPart:\\n    self.current_retry += 1\\n    if self.max_retries is None or self.current_retry > self.max_retries:\\n      raise UnexpectedModelBehavior(f\\'Tool exceeded max retries count of {self.max_retries}\\') fromexc\\n    else:\\n      if isinstance(exc, ValidationError):\\n        content = exc.errors(include_url=False)\\n      else:\\n        content = exc.message\\n      return _messages.RetryPromptPart(\\n        tool_name=call_message.tool_name,\\n        content=content,\\n        tool_call_id=call_message.tool_call_id,\\n      )'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 88, 'url': 'https://pydantic.com'}, page_content='__init__(\\n  function: ToolFuncEitherAgentDepsT[],\\n  *,\\n  takes_ctx: bool | None = None,\\n  max_retries: int | None = None,\\n  name: str | None = None,\\n  description: str | None = None,\\n  prepare: ToolPrepareFuncAgentDepsT[] | None = None,\\n  docstring_format: DocstringFormat = \"auto\",\\n  require_parameter_descriptions: bool = False\\n)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 88, 'url': 'https://pydantic.com'}, page_content=\"frompydantic_aiimport Agent, RunContext, Tool\\nasync defmy_tool(ctx: RunContext[int], x: int, y: int) -> str:\\n  return f'{ctx.deps}{x}{y}'\\nagent = Agent('test', tools=[Tool(my_tool)])\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 88, 'url': 'https://pydantic.com'}, page_content=\"fromtypingimport Union\\nfrompydantic_aiimport Agent, RunContext, Tool\\nfrompydantic_ai.toolsimport ToolDefinition\\nasync defmy_tool(ctx: RunContext[int], x: int, y: int) -> str:\\n  return f'{ctx.deps}{x}{y}'\\nasync defprep_my_tool(\\n  ctx: RunContext[int], tool_def: ToolDefinition\\n) -> Union[ToolDefinition, None]:\\n  # only register the tool if `deps == 42`\\n  if ctx.deps == 42:\\n    return tool_def\\nagent = Agent('test', tools=[Tool(my_tool, prepare=prep_my_tool)])\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 89, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 89, 'url': 'https://pydantic.com'}, page_content='def__init__(\\n  self,\\n  function: ToolFuncEither[AgentDepsT],\\n  *,\\n  takes_ctx: bool | None = None,\\n  max_retries: int | None = None,\\n  name: str | None = None,\\n  description: str | None = None,\\n  prepare: ToolPrepareFunc[AgentDepsT] | None = None,\\n  docstring_format: DocstringFormat = \\'auto\\',\\n  require_parameter_descriptions: bool = False,\\n):\\n\"\"\"Create a new tool instance.\\n  Example usage:'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 89, 'url': 'https://pydantic.com'}, page_content='or with a custom prepare method:'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 89, 'url': 'https://pydantic.com'}, page_content='Args:\\n    function: The Python function to call as the tool.\\n    takes_ctx: Whether the function takes a [`RunContext`][pydantic_ai.tools.RunContext] first argument,\\n      this is inferred if unset.\\n    max_retries: Maximum number of retries allowed for this tool, set to the agent default if `None`.\\n    name: Name of the tool, inferred from the function if `None`.\\n    description: Description of the tool, inferred from the function if `None`.\\n    prepare: custom method to prepare the tool definition for each step, return `None` to omit this\\n      tool from a given step. This is useful if you want to customise a tool at call time,\\n      or omit it completely from a step. See [`ToolPrepareFunc`][pydantic_ai.tools.ToolPrepareFunc].\\n    docstring_format: The format of the docstring, see [`DocstringFormat`][pydantic_ai.tools.DocstringFormat].\\n      Defaults to `\\'auto\\'`, such that the format is inferred from the structure of the docstring.\\n    require_parameter_descriptions: If True, raise an error if a parameter description is missing. Defaults to False.\\n  \"\"\"\\n  if takes_ctx is None:\\n    takes_ctx = _pydantic.takes_ctx(function)\\n  f = _pydantic.function_schema(function, takes_ctx, docstring_format, require_parameter_descriptions)\\n  self.function = function\\n  self.takes_ctx = takes_ctx\\n  self.max_retries = max_retries\\n  self.name = name or function.__name__\\n  self.description = description or f[\\'description\\']\\n  self.prepare = prepare\\n  self.docstring_format = docstring_format\\n  self.require_parameter_descriptions = require_parameter_descriptions\\n  self._is_async = inspect.iscoroutinefunction(self.function)\\n  self._single_arg_name = f[\\'single_arg_name\\']\\n  self._positional_fields = f[\\'positional_fields\\']\\n  self._var_positional_field = f[\\'var_positional_field\\']\\n  self._validator = f[\\'validator\\']\\n  self._parameters_json_schema = f[\\'json_schema\\']'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 89, 'url': 'https://pydantic.com'}, page_content='prepare_tool_def(\\n  ctx: RunContextAgentDepsT[],\\n) -> ToolDefinition | None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 89, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 89, 'url': 'https://pydantic.com'}, page_content='async defprepare_tool_def(self, ctx: RunContext[AgentDepsT]) -> ToolDefinition | None:\\n\"\"\"Get the tool definition.\\n  By default, this method creates a tool definition, then either returns it, or calls `self.prepare`\\n  if it\\'s set.\\n  Returns:\\n    return a `ToolDefinition` or `None` if the tools should not be registered for this run.\\n  \"\"\"\\n  tool_def = ToolDefinition(\\n    name=self.name,\\n    description=self.description,\\n    parameters_json_schema=self._parameters_json_schema,\\n  )\\n  if self.prepare is not None:\\n    return await self.prepare(ctx, tool_def)\\n  else:\\n    return tool_def'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 89, 'url': 'https://pydantic.com'}, page_content='run(\\n  message: ToolCallPart,\\n  run_context: RunContextAgentDepsT[],\\n) -> ToolReturnPart | RetryPromptPart'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 89, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 89, 'url': 'https://pydantic.com'}, page_content='async defrun(\\n  self, message: _messages.ToolCallPart, run_context: RunContext[AgentDepsT]\\n) -> _messages.ToolReturnPart | _messages.RetryPromptPart:\\n\"\"\"Run the tool function asynchronously.\"\"\"\\n  try:\\n    if isinstance(message.args, str):\\n      args_dict = self._validator.validate_json(message.args)\\n    else:\\n      args_dict = self._validator.validate_python(message.args)\\n  except ValidationError as e:\\n    return self._on_error(e, message)\\n  args, kwargs = self._call_args(args_dict, message, run_context)\\n  try:\\n    if self._is_async:\\n      function = cast(Callable[[Any], Awaitable[str]], self.function)\\n      response_content = await function(*args, **kwargs)\\n    else:\\n      function = cast(Callable[[Any], str], self.function)\\n      response_content = await _utils.run_in_executor(function, *args, **kwargs)\\n  except ModelRetry as e:\\n    return self._on_error(e, message)\\n  self.current_retry = 0\\n  return _messages.ToolReturnPart(\\n    tool_name=message.tool_name,\\n    content=response_content,\\n    tool_call_id=message.tool_call_id,\\n  )'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 89, 'url': 'https://pydantic.com'}, page_content='ObjectJsonSchema: TypeAlias = dictstr[, Any]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 90, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 90, 'url': 'https://pydantic.com'}, page_content='@dataclass\\nclassToolDefinition:\\n\"\"\"Definition of a tool passed to a model.\\n  This is used for both function tools result tools.\\n  \"\"\"\\n  name: str\\n\"\"\"The name of the tool.\"\"\"\\n  description: str\\n\"\"\"The description of the tool.\"\"\"\\n  parameters_json_schema: ObjectJsonSchema\\n\"\"\"The JSON schema for the tool\\'s parameters.\"\"\"\\n  outer_typed_dict_key: str | None = None\\n\"\"\"The key in the outer [TypedDict] that wraps a result tool.\\n  This will only be set for result tools which don\\'t have an `object` JSON schema.\\n  \"\"\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 90, 'url': 'https://pydantic.com'}, page_content='name: str'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 90, 'url': 'https://pydantic.com'}, page_content='description: str'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 90, 'url': 'https://pydantic.com'}, page_content='parameters_json_schema: ObjectJsonSchema'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 90, 'url': 'https://pydantic.com'}, page_content='outer_typed_dict_key: str | None = None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 91, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 91, 'url': 'https://pydantic.com'}, page_content='@dataclass\\nclassUsage:\\n\"\"\"LLM usage associated with a request or run.\\n  Responsibility for calculating usage is on the model; PydanticAI simply sums the usage information across requests.\\n  You\\'ll need to look up the documentation of the model you\\'re using to convert usage to monetary costs.\\n  \"\"\"\\n  requests: int = 0\\n\"\"\"Number of requests made to the LLM API.\"\"\"\\n  request_tokens: int | None = None\\n\"\"\"Tokens used in processing requests.\"\"\"\\n  response_tokens: int | None = None\\n\"\"\"Tokens used in generating responses.\"\"\"\\n  total_tokens: int | None = None\\n\"\"\"Total tokens used in the whole run, should generally be equal to `request_tokens + response_tokens`.\"\"\"\\n  details: dict[str, int] | None = None\\n\"\"\"Any extra details returned by the model.\"\"\"\\n  defincr(self, incr_usage: Usage, *, requests: int = 0) -> None:\\n\"\"\"Increment the usage in place.\\n    Args:\\n      incr_usage: The usage to increment by.\\n      requests: The number of requests to increment by in addition to `incr_usage.requests`.\\n    \"\"\"\\n    self.requests += requests\\n    for f in \\'requests\\', \\'request_tokens\\', \\'response_tokens\\', \\'total_tokens\\':\\n      self_value = getattr(self, f)\\n      other_value = getattr(incr_usage, f)\\n      if self_value is not None or other_value is not None:\\n        setattr(self, f, (self_value or 0) + (other_value or 0))\\n    if incr_usage.details:\\n      self.details = self.details or {}\\n      for key, value in incr_usage.details.items():\\n        self.details[key] = self.details.get(key, 0) + value\\n  def__add__(self, other: Usage) -> Usage:\\n\"\"\"Add two Usages together.\\n    This is provided so it\\'s trivial to sum usage information from multiple requests and runs.\\n    \"\"\"\\n    new_usage = copy(self)\\n    new_usage.incr(other)\\n    return new_usage'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 91, 'url': 'https://pydantic.com'}, page_content='requests: int = 0'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 91, 'url': 'https://pydantic.com'}, page_content='request_tokens: int | None = None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 91, 'url': 'https://pydantic.com'}, page_content='response_tokens: int | None = None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 91, 'url': 'https://pydantic.com'}, page_content='total_tokens: int | None = None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 91, 'url': 'https://pydantic.com'}, page_content='details: dictstr[, int] | None = None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 91, 'url': 'https://pydantic.com'}, page_content='incr(incr_usage: Usage, *, requests: int = 0) -> None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 91, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 91, 'url': 'https://pydantic.com'}, page_content='defincr(self, incr_usage: Usage, *, requests: int = 0) -> None:\\n\"\"\"Increment the usage in place.\\n  Args:\\n    incr_usage: The usage to increment by.\\n    requests: The number of requests to increment by in addition to `incr_usage.requests`.\\n  \"\"\"\\n  self.requests += requests\\n  for f in \\'requests\\', \\'request_tokens\\', \\'response_tokens\\', \\'total_tokens\\':\\n    self_value = getattr(self, f)\\n    other_value = getattr(incr_usage, f)\\n    if self_value is not None or other_value is not None:\\n      setattr(self, f, (self_value or 0) + (other_value or 0))\\n  if incr_usage.details:\\n    self.details = self.details or {}\\n    for key, value in incr_usage.details.items():\\n      self.details[key] = self.details.get(key, 0) + value'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 92, 'url': 'https://pydantic.com'}, page_content='__add__(other: Usage) -> Usage'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 92, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 92, 'url': 'https://pydantic.com'}, page_content='def__add__(self, other: Usage) -> Usage:\\n\"\"\"Add two Usages together.\\n  This is provided so it\\'s trivial to sum usage information from multiple requests and runs.\\n  \"\"\"\\n  new_usage = copy(self)\\n  new_usage.incr(other)\\n  return new_usage'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 92, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 92, 'url': 'https://pydantic.com'}, page_content='@dataclass\\nclassUsageLimits:\\n\"\"\"Limits on model usage.\\n  The request count is tracked by pydantic_ai, and the request limit is checked before each request to the model.\\n  Token counts are provided in responses from the model, and the token limits are checked after each response.\\n  Each of the limits can be set to `None` to disable that limit.\\n  \"\"\"\\n  request_limit: int | None = 50\\n\"\"\"The maximum number of requests allowed to the model.\"\"\"\\n  request_tokens_limit: int | None = None\\n\"\"\"The maximum number of tokens allowed in requests to the model.\"\"\"\\n  response_tokens_limit: int | None = None\\n\"\"\"The maximum number of tokens allowed in responses from the model.\"\"\"\\n  total_tokens_limit: int | None = None\\n\"\"\"The maximum number of tokens allowed in requests and responses combined.\"\"\"\\n  defhas_token_limits(self) -> bool:\\n\"\"\"Returns `True` if this instance places any limits on token counts.\\n    If this returns `False`, the `check_tokens` method will never raise an error.\\n    This is useful because if we have token limits, we need to check them after receiving each streamed message.\\n    If there are no limits, we can skip that processing in the streaming response iterator.\\n    \"\"\"\\n    return any(\\n      limit is not None\\n      for limit in (self.request_tokens_limit, self.response_tokens_limit, self.total_tokens_limit)\\n    )\\n  defcheck_before_request(self, usage: Usage) -> None:\\n\"\"\"Raises a `UsageLimitExceeded` exception if the next request would exceed the request_limit.\"\"\"\\n    request_limit = self.request_limit\\n    if request_limit is not None and usage.requests >= request_limit:\\n      raise UsageLimitExceeded(f\\'The next request would exceed the request_limit of {request_limit}\\')\\n  defcheck_tokens(self, usage: Usage) -> None:\\n\"\"\"Raises a `UsageLimitExceeded` exception if the usage exceeds any of the token limits.\"\"\"\\n    request_tokens = usage.request_tokens or 0\\n    if self.request_tokens_limit is not None and request_tokens > self.request_tokens_limit:\\n      raise UsageLimitExceeded(\\n        f\\'Exceeded the request_tokens_limit of {self.request_tokens_limit} ({request_tokens=})\\'\\n      )\\n    response_tokens = usage.response_tokens or 0\\n    if self.response_tokens_limit is not None and response_tokens > self.response_tokens_limit:\\n      raise UsageLimitExceeded(\\n        f\\'Exceeded the response_tokens_limit of {self.response_tokens_limit} ({response_tokens=})\\'\\n      )\\n    total_tokens = usage.total_tokens or 0\\n    if self.total_tokens_limit is not None and total_tokens > self.total_tokens_limit:\\n      raise UsageLimitExceeded(f\\'Exceeded the total_tokens_limit of {self.total_tokens_limit} ({total_tokens=})\\')'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 92, 'url': 'https://pydantic.com'}, page_content='request_limit: int | None = 50'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 92, 'url': 'https://pydantic.com'}, page_content='request_tokens_limit: int | None = None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 92, 'url': 'https://pydantic.com'}, page_content='response_tokens_limit: int | None = None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 92, 'url': 'https://pydantic.com'}, page_content='total_tokens_limit: int | None = None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 92, 'url': 'https://pydantic.com'}, page_content='has_token_limits() -> bool'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 92, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 92, 'url': 'https://pydantic.com'}, page_content='defhas_token_limits(self) -> bool:\\n\"\"\"Returns `True` if this instance places any limits on token counts.\\n  If this returns `False`, the `check_tokens` method will never raise an error.\\n  This is useful because if we have token limits, we need to check them after receiving each streamed message.\\n  If there are no limits, we can skip that processing in the streaming response iterator.\\n  \"\"\"\\n  return any(\\n    limit is not None\\n    for limit in (self.request_tokens_limit, self.response_tokens_limit, self.total_tokens_limit)\\n  )'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 92, 'url': 'https://pydantic.com'}, page_content='check_before_request(usage: Usage) -> None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 92, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 92, 'url': 'https://pydantic.com'}, page_content='defcheck_before_request(self, usage: Usage) -> None:\\n\"\"\"Raises a `UsageLimitExceeded` exception if the next request would exceed the request_limit.\"\"\"\\n  request_limit = self.request_limit\\n  if request_limit is not None and usage.requests >= request_limit:\\n    raise UsageLimitExceeded(f\\'The next request would exceed the request_limit of {request_limit}\\')'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 92, 'url': 'https://pydantic.com'}, page_content='check_tokens(usage: Usage) -> None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 93, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 93, 'url': 'https://pydantic.com'}, page_content='defcheck_tokens(self, usage: Usage) -> None:\\n\"\"\"Raises a `UsageLimitExceeded` exception if the usage exceeds any of the token limits.\"\"\"\\n  request_tokens = usage.request_tokens or 0\\n  if self.request_tokens_limit is not None and request_tokens > self.request_tokens_limit:\\n    raise UsageLimitExceeded(\\n      f\\'Exceeded the request_tokens_limit of {self.request_tokens_limit} ({request_tokens=})\\'\\n    )\\n  response_tokens = usage.response_tokens or 0\\n  if self.response_tokens_limit is not None and response_tokens > self.response_tokens_limit:\\n    raise UsageLimitExceeded(\\n      f\\'Exceeded the response_tokens_limit of {self.response_tokens_limit} ({response_tokens=})\\'\\n    )\\n  total_tokens = usage.total_tokens or 0\\n  if self.total_tokens_limit is not None and total_tokens > self.total_tokens_limit:\\n    raise UsageLimitExceeded(f\\'Exceeded the total_tokens_limit of {self.total_tokens_limit} ({total_tokens=})\\')'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 94, 'url': 'https://pydantic.com'}, page_content='LatestAnthropicModelNames = Literal[\\n  \"claude-3-5-haiku-latest\",\\n  \"claude-3-5-sonnet-latest\",\\n  \"claude-3-opus-latest\",\\n]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 94, 'url': 'https://pydantic.com'}, page_content='AnthropicModelName = Unionstr[, LatestAnthropicModelNames]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 94, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 94, 'url': 'https://pydantic.com'}, page_content='classAnthropicModelSettings(ModelSettings):\\n\"\"\"Settings used for an Anthropic model request.\"\"\"\\n  anthropic_metadata: MetadataParam\\n\"\"\"An object describing metadata about the request.\\n  Contains `user_id`, an external identifier for the user who is associated with the request.\"\"\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 94, 'url': 'https://pydantic.com'}, page_content='anthropic_metadata: MetadataParam'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 94, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 94, 'url': 'https://pydantic.com'}, page_content='@dataclass(init=False)\\nclassAnthropicModel(Model):\\n\"\"\"A model that uses the Anthropic API.\\n  Internally, this uses the Anthropic Python client to interact with the API.\\n  Apart from `__init__`, all methods are private or match those of the base class.\\n  !!! note\\n    The `AnthropicModel` class does not yet support streaming responses.\\n    We anticipate adding support for streaming responses in a near-term future release.\\n  \"\"\"\\n  client: AsyncAnthropic = field(repr=False)\\n  _model_name: AnthropicModelName = field(repr=False)\\n  _system: str | None = field(default=\\'anthropic\\', repr=False)\\n  def__init__(\\n    self,\\n    model_name: AnthropicModelName,\\n    *,\\n    api_key: str | None = None,\\n    anthropic_client: AsyncAnthropic | None = None,\\n    http_client: AsyncHTTPClient | None = None,\\n  ):\\n\"\"\"Initialize an Anthropic model.\\n    Args:\\n      model_name: The name of the Anthropic model to use. List of model names available\\n        here.\\n      api_key: The API key to use for authentication, if not provided, the `ANTHROPIC_API_KEY` environment variable\\n        will be used if available.\\n      anthropic_client: An existing\\n        `AsyncAnthropic`\\n        client to use, if provided, `api_key` and `http_client` must be `None`.\\n      http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.\\n    \"\"\"\\n    self._model_name = model_name\\n    if anthropic_client is not None:\\n      assert http_client is None, \\'Cannot provide both `anthropic_client` and `http_client`\\'\\n      assert api_key is None, \\'Cannot provide both `anthropic_client` and `api_key`\\'\\n      self.client = anthropic_client\\n    elif http_client is not None:\\n      self.client = AsyncAnthropic(api_key=api_key, http_client=http_client)\\n    else:\\n      self.client = AsyncAnthropic(api_key=api_key, http_client=cached_async_http_client())\\n  async defrequest(\\n    self,\\n    messages: list[ModelMessage],\\n    model_settings: ModelSettings | None,\\n    model_request_parameters: ModelRequestParameters,\\n  ) -> tuple[ModelResponse, usage.Usage]:\\n    check_allow_model_requests()\\n    response = await self._messages_create(\\n      messages, False, cast(AnthropicModelSettings, model_settings or {}), model_request_parameters\\n    )\\n    return self._process_response(response), _map_usage(response)\\n  @asynccontextmanager\\n  async defrequest_stream(\\n    self,\\n    messages: list[ModelMessage],\\n    model_settings: ModelSettings | None,\\n    model_request_parameters: ModelRequestParameters,\\n  ) -> AsyncIterator[StreamedResponse]:\\n    check_allow_model_requests()\\n    response = await self._messages_create(\\n      messages, True, cast(AnthropicModelSettings, model_settings or {}), model_request_parameters\\n    )\\n    async with response:\\n      yield await self._process_streamed_response(response)\\n  @property\\n  defmodel_name(self) -> AnthropicModelName:\\n\"\"\"The model name.\"\"\"\\n    return self._model_name\\n  @property\\n  defsystem(self) -> str | None:\\n\"\"\"The system / model provider.\"\"\"\\n    return self._system\\n  @overload\\n  async def_messages_create(\\n    self,\\n    messages: list[ModelMessage],\\n    stream: Literal[True],\\n    model_settings: AnthropicModelSettings,\\n    model_request_parameters: ModelRequestParameters,\\n  ) -> AsyncStream[RawMessageStreamEvent]:\\n    pass\\n  @overload\\n  async def_messages_create(\\n    self,\\n    messages: list[ModelMessage],\\n    stream: Literal[False],\\n    model_settings: AnthropicModelSettings,\\n    model_request_parameters: ModelRequestParameters,\\n  ) -> AnthropicMessage:\\n    pass\\n  async def_messages_create(\\n    self,\\n    messages: list[ModelMessage],\\n    stream: bool,\\n    model_settings: AnthropicModelSettings,\\n    model_request_parameters: ModelRequestParameters,\\n  ) -> AnthropicMessage | AsyncStream[RawMessageStreamEvent]:\\n    # standalone function to make it easier to override\\n    tools = self._get_tools(model_request_parameters)\\n    tool_choice: ToolChoiceParam | None\\n    if not tools:\\n      tool_choice = None\\n    else:\\n      if not model_request_parameters.allow_text_result:\\n        tool_choice = {\\'type\\': \\'any\\'}\\n      else:\\n        tool_choice = {\\'type\\': \\'auto\\'}\\n      if (allow_parallel_tool_calls := model_settings.get(\\'parallel_tool_calls\\')) is not None:\\n        tool_choice[\\'disable_parallel_tool_use\\'] = not allow_parallel_tool_calls\\n    system_prompt, anthropic_messages = self._map_message(messages)\\n    return await self.client.messages.create(\\n      max_tokens=model_settings.get(\\'max_tokens\\', 1024),\\n      system=system_prompt or NOT_GIVEN,\\n      messages=anthropic_messages,\\n      model=self._model_name,\\n      tools=tools or NOT_GIVEN,\\n      tool_choice=tool_choice or NOT_GIVEN,\\n      stream=stream,\\n      temperature=model_settings.get(\\'temperature\\', NOT_GIVEN),\\n      top_p=model_settings.get(\\'top_p\\', NOT_GIVEN),\\n      timeout=model_settings.get(\\'timeout\\', NOT_GIVEN),\\n      metadata=model_settings.get(\\'anthropic_metadata\\', NOT_GIVEN),\\n    )\\n  def_process_response(self, response: AnthropicMessage) -> ModelResponse:\\n\"\"\"Process a non-streamed response, and prepare a message to return.\"\"\"\\n    items: list[ModelResponsePart] = []\\n    for item in response.content:\\n      if isinstance(item, TextBlock):\\n        items.append(TextPart(content=item.text))\\n      else:\\n        assert isinstance(item, ToolUseBlock), \\'unexpected item type\\'\\n        items.append(\\n          ToolCallPart(\\n            tool_name=item.name,\\n            args=cast(dict[str, Any], item.input),\\n            tool_call_id=item.id,\\n          )\\n        )\\n    return ModelResponse(items, model_name=response.model)\\n  async def_process_streamed_response(self, response: AsyncStream[RawMessageStreamEvent]) -> StreamedResponse:\\n    peekable_response = _utils.PeekableAsyncStream(response)\\n    first_chunk = await peekable_response.peek()\\n    if isinstance(first_chunk, _utils.Unset):\\n      raise UnexpectedModelBehavior(\\'Streamed response ended without content or tool calls\\')\\n    # Since Anthropic doesn\\'t provide a timestamp in the message, we\\'ll use the current time\\n    timestamp = datetime.now(tz=timezone.utc)\\n    return AnthropicStreamedResponse(\\n      _model_name=self._model_name, _response=peekable_response, _timestamp=timestamp\\n    )\\n  def_get_tools(self, model_request_parameters: ModelRequestParameters) -> list[ToolParam]:\\n    tools = [self._map_tool_definition(r) for r in model_request_parameters.function_tools]\\n    if model_request_parameters.result_tools:\\n      tools += [self._map_tool_definition(r) for r in model_request_parameters.result_tools]\\n    return tools\\n  def_map_message(self, messages: list[ModelMessage]) -> tuple[str, list[MessageParam]]:\\n\"\"\"Just maps a `pydantic_ai.Message` to a `anthropic.types.MessageParam`.\"\"\"\\n    system_prompt: str = \\'\\'\\n    anthropic_messages: list[MessageParam] = []\\n    for m in messages:\\n      if isinstance(m, ModelRequest):\\n        user_content_params: list[ToolResultBlockParam | TextBlockParam] = []\\n        for request_part in m.parts:\\n          if isinstance(request_part, SystemPromptPart):\\n            system_prompt += request_part.content\\n          elif isinstance(request_part, UserPromptPart):\\n            text_block_param = TextBlockParam(type=\\'text\\', text=request_part.content)\\n            user_content_params.append(text_block_param)\\n          elif isinstance(request_part, ToolReturnPart):\\n            tool_result_block_param = ToolResultBlockParam(\\n              tool_use_id=_guard_tool_call_id(t=request_part, model_source=\\'Anthropic\\'),\\n              type=\\'tool_result\\',\\n              content=request_part.model_response_str(),\\n              is_error=False,\\n            )\\n            user_content_params.append(tool_result_block_param)\\n          elif isinstance(request_part, RetryPromptPart):\\n            if request_part.tool_name is None:\\n              retry_param = TextBlockParam(type=\\'text\\', text=request_part.model_response())\\n            else:\\n              retry_param = ToolResultBlockParam(\\n                tool_use_id=_guard_tool_call_id(t=request_part, model_source=\\'Anthropic\\'),\\n                type=\\'tool_result\\',\\n                content=request_part.model_response(),\\n                is_error=True,\\n              )\\n            user_content_params.append(retry_param)\\n        anthropic_messages.append(\\n          MessageParam(\\n            role=\\'user\\',\\n            content=user_content_params,\\n          )\\n        )\\n      elif isinstance(m, ModelResponse):\\n        assistant_content_params: list[TextBlockParam | ToolUseBlockParam] = []\\n        for response_part in m.parts:\\n          if isinstance(response_part, TextPart):\\n            assistant_content_params.append(TextBlockParam(text=response_part.content, type=\\'text\\'))\\n          else:\\n            tool_use_block_param = ToolUseBlockParam(\\n              id=_guard_tool_call_id(t=response_part, model_source=\\'Anthropic\\'),\\n              type=\\'tool_use\\',\\n              name=response_part.tool_name,\\n              input=response_part.args_as_dict(),\\n            )\\n            assistant_content_params.append(tool_use_block_param)\\n        anthropic_messages.append(MessageParam(role=\\'assistant\\', content=assistant_content_params))\\n      else:\\n        assert_never(m)\\n    return system_prompt, anthropic_messages\\n  @staticmethod\\n  def_map_tool_definition(f: ToolDefinition) -> ToolParam:\\n    return {\\n      \\'name\\': f.name,\\n      \\'description\\': f.description,\\n      \\'input_schema\\': f.parameters_json_schema,\\n    }'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 94, 'url': 'https://pydantic.com'}, page_content='__init__(\\n  model_name: AnthropicModelName,\\n  *,\\n  api_key: str | None = None,\\n  anthropic_client: AsyncAnthropic | None = None,\\n  http_client: AsyncClient | None = None\\n)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 95, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 95, 'url': 'https://pydantic.com'}, page_content='def__init__(\\n  self,\\n  model_name: AnthropicModelName,\\n  *,\\n  api_key: str | None = None,\\n  anthropic_client: AsyncAnthropic | None = None,\\n  http_client: AsyncHTTPClient | None = None,\\n):\\n\"\"\"Initialize an Anthropic model.\\n  Args:\\n    model_name: The name of the Anthropic model to use. List of model names available\\n      here.\\n    api_key: The API key to use for authentication, if not provided, the `ANTHROPIC_API_KEY` environment variable\\n      will be used if available.\\n    anthropic_client: An existing\\n      `AsyncAnthropic`\\n      client to use, if provided, `api_key` and `http_client` must be `None`.\\n    http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.\\n  \"\"\"\\n  self._model_name = model_name\\n  if anthropic_client is not None:\\n    assert http_client is None, \\'Cannot provide both `anthropic_client` and `http_client`\\'\\n    assert api_key is None, \\'Cannot provide both `anthropic_client` and `api_key`\\'\\n    self.client = anthropic_client\\n  elif http_client is not None:\\n    self.client = AsyncAnthropic(api_key=api_key, http_client=http_client)\\n  else:\\n    self.client = AsyncAnthropic(api_key=api_key, http_client=cached_async_http_client())'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 95, 'url': 'https://pydantic.com'}, page_content='model_name: AnthropicModelName'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 95, 'url': 'https://pydantic.com'}, page_content='system: str | None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 95, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 95, 'url': 'https://pydantic.com'}, page_content='@dataclass\\nclassAnthropicStreamedResponse(StreamedResponse):\\n\"\"\"Implementation of `StreamedResponse` for Anthropic models.\"\"\"\\n  _model_name: AnthropicModelName\\n  _response: AsyncIterable[RawMessageStreamEvent]\\n  _timestamp: datetime\\n  async def_get_event_iterator(self) -> AsyncIterator[ModelResponseStreamEvent]:\\n    current_block: TextBlock | ToolUseBlock | None = None\\n    current_json: str = \\'\\'\\n    async for event in self._response:\\n      self._usage += _map_usage(event)\\n      if isinstance(event, RawContentBlockStartEvent):\\n        current_block = event.content_block\\n        if isinstance(current_block, TextBlock) and current_block.text:\\n          yield self._parts_manager.handle_text_delta(vendor_part_id=\\'content\\', content=current_block.text)\\n        elif isinstance(current_block, ToolUseBlock):\\n          maybe_event = self._parts_manager.handle_tool_call_delta(\\n            vendor_part_id=current_block.id,\\n            tool_name=current_block.name,\\n            args=cast(dict[str, Any], current_block.input),\\n            tool_call_id=current_block.id,\\n          )\\n          if maybe_event is not None:\\n            yield maybe_event\\n      elif isinstance(event, RawContentBlockDeltaEvent):\\n        if isinstance(event.delta, TextDelta):\\n          yield self._parts_manager.handle_text_delta(vendor_part_id=\\'content\\', content=event.delta.text)\\n        elif (\\n          current_block and event.delta.type == \\'input_json_delta\\' and isinstance(current_block, ToolUseBlock)\\n        ):\\n          # Try to parse the JSON immediately, otherwise cache the value for later. This handles\\n          # cases where the JSON is not currently valid but will be valid once we stream more tokens.\\n          try:\\n            parsed_args = json_loads(current_json + event.delta.partial_json)\\n            current_json = \\'\\'\\n          except JSONDecodeError:\\n            current_json += event.delta.partial_json\\n            continue\\n          # For tool calls, we need to handle partial JSON updates\\n          maybe_event = self._parts_manager.handle_tool_call_delta(\\n            vendor_part_id=current_block.id,\\n            tool_name=\\'\\',\\n            args=parsed_args,\\n            tool_call_id=current_block.id,\\n          )\\n          if maybe_event is not None:\\n            yield maybe_event\\n      elif isinstance(event, (RawContentBlockStopEvent, RawMessageStopEvent)):\\n        current_block = None\\n  @property\\n  defmodel_name(self) -> AnthropicModelName:\\n\"\"\"Get the model name of the response.\"\"\"\\n    return self._model_name\\n  @property\\n  deftimestamp(self) -> datetime:\\n\"\"\"Get the timestamp of the response.\"\"\"\\n    return self._timestamp'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 95, 'url': 'https://pydantic.com'}, page_content='model_name: AnthropicModelName'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 95, 'url': 'https://pydantic.com'}, page_content='timestamp: datetime'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 96, 'url': 'https://pydantic.com'}, page_content='KnownModelName = Literal[\\n  \"anthropic:claude-3-5-haiku-latest\",\\n  \"anthropic:claude-3-5-sonnet-latest\",\\n  \"anthropic:claude-3-opus-latest\",\\n  \"claude-3-5-haiku-latest\",\\n  \"claude-3-5-sonnet-latest\",\\n  \"claude-3-opus-latest\",\\n  \"cohere:c4ai-aya-expanse-32b\",\\n  \"cohere:c4ai-aya-expanse-8b\",\\n  \"cohere:command\",\\n  \"cohere:command-light\",\\n  \"cohere:command-light-nightly\",\\n  \"cohere:command-nightly\",\\n  \"cohere:command-r\",\\n  \"cohere:command-r-03-2024\",\\n  \"cohere:command-r-08-2024\",\\n  \"cohere:command-r-plus\",\\n  \"cohere:command-r-plus-04-2024\",\\n  \"cohere:command-r-plus-08-2024\",\\n  \"cohere:command-r7b-12-2024\",\\n  \"google-gla:gemini-1.0-pro\",\\n  \"google-gla:gemini-1.5-flash\",\\n  \"google-gla:gemini-1.5-flash-8b\",\\n  \"google-gla:gemini-1.5-pro\",\\n  \"google-gla:gemini-2.0-flash-exp\",\\n  \"google-gla:gemini-2.0-flash-thinking-exp-01-21\",\\n  \"google-gla:gemini-exp-1206\",\\n  \"google-gla:gemini-2.0-flash\",\\n  \"google-gla:gemini-2.0-flash-lite-preview-02-05\",\\n  \"google-vertex:gemini-1.0-pro\",\\n  \"google-vertex:gemini-1.5-flash\",\\n  \"google-vertex:gemini-1.5-flash-8b\",\\n  \"google-vertex:gemini-1.5-pro\",\\n  \"google-vertex:gemini-2.0-flash-exp\",\\n  \"google-vertex:gemini-2.0-flash-thinking-exp-01-21\",\\n  \"google-vertex:gemini-exp-1206\",\\n  \"google-vertex:gemini-2.0-flash\",\\n  \"google-vertex:gemini-2.0-flash-lite-preview-02-05\",\\n  \"gpt-3.5-turbo\",\\n  \"gpt-3.5-turbo-0125\",\\n  \"gpt-3.5-turbo-0301\",\\n  \"gpt-3.5-turbo-0613\",\\n  \"gpt-3.5-turbo-1106\",\\n  \"gpt-3.5-turbo-16k\",\\n  \"gpt-3.5-turbo-16k-0613\",\\n  \"gpt-4\",\\n  \"gpt-4-0125-preview\",\\n  \"gpt-4-0314\",\\n  \"gpt-4-0613\",\\n  \"gpt-4-1106-preview\",\\n  \"gpt-4-32k\",\\n  \"gpt-4-32k-0314\",\\n  \"gpt-4-32k-0613\",\\n  \"gpt-4-turbo\",\\n  \"gpt-4-turbo-2024-04-09\",\\n  \"gpt-4-turbo-preview\",\\n  \"gpt-4-vision-preview\",\\n  \"gpt-4o\",\\n  \"gpt-4o-2024-05-13\",\\n  \"gpt-4o-2024-08-06\",\\n  \"gpt-4o-2024-11-20\",\\n  \"gpt-4o-audio-preview\",\\n  \"gpt-4o-audio-preview-2024-10-01\",\\n  \"gpt-4o-audio-preview-2024-12-17\",\\n  \"gpt-4o-mini\",\\n  \"gpt-4o-mini-2024-07-18\",\\n  \"gpt-4o-mini-audio-preview\",\\n  \"gpt-4o-mini-audio-preview-2024-12-17\",\\n  \"groq:gemma2-9b-it\",\\n  \"groq:llama-3.1-8b-instant\",\\n  \"groq:llama-3.2-11b-vision-preview\",\\n  \"groq:llama-3.2-1b-preview\",\\n  \"groq:llama-3.2-3b-preview\",\\n  \"groq:llama-3.2-90b-vision-preview\",\\n  \"groq:llama-3.3-70b-specdec\",\\n  \"groq:llama-3.3-70b-versatile\",\\n  \"groq:llama3-70b-8192\",\\n  \"groq:llama3-8b-8192\",\\n  \"groq:mixtral-8x7b-32768\",\\n  \"mistral:codestral-latest\",\\n  \"mistral:mistral-large-latest\",\\n  \"mistral:mistral-moderation-latest\",\\n  \"mistral:mistral-small-latest\",\\n  \"o1\",\\n  \"o1-2024-12-17\",\\n  \"o1-mini\",\\n  \"o1-mini-2024-09-12\",\\n  \"o1-preview\",\\n  \"o1-preview-2024-09-12\",\\n  \"o3-mini\",\\n  \"o3-mini-2025-01-31\",\\n  \"openai:chatgpt-4o-latest\",\\n  \"openai:gpt-3.5-turbo\",\\n  \"openai:gpt-3.5-turbo-0125\",\\n  \"openai:gpt-3.5-turbo-0301\",\\n  \"openai:gpt-3.5-turbo-0613\",\\n  \"openai:gpt-3.5-turbo-1106\",\\n  \"openai:gpt-3.5-turbo-16k\",\\n  \"openai:gpt-3.5-turbo-16k-0613\",\\n  \"openai:gpt-4\",\\n  \"openai:gpt-4-0125-preview\",\\n  \"openai:gpt-4-0314\",\\n  \"openai:gpt-4-0613\",\\n  \"openai:gpt-4-1106-preview\",\\n  \"openai:gpt-4-32k\",\\n  \"openai:gpt-4-32k-0314\",\\n  \"openai:gpt-4-32k-0613\",\\n  \"openai:gpt-4-turbo\",\\n  \"openai:gpt-4-turbo-2024-04-09\",\\n  \"openai:gpt-4-turbo-preview\",\\n  \"openai:gpt-4-vision-preview\",\\n  \"openai:gpt-4o\",\\n  \"openai:gpt-4o-2024-05-13\",\\n  \"openai:gpt-4o-2024-08-06\",\\n  \"openai:gpt-4o-2024-11-20\",\\n  \"openai:gpt-4o-audio-preview\",\\n  \"openai:gpt-4o-audio-preview-2024-10-01\",\\n  \"openai:gpt-4o-audio-preview-2024-12-17\",\\n  \"openai:gpt-4o-mini\",\\n  \"openai:gpt-4o-mini-2024-07-18\",\\n  \"openai:gpt-4o-mini-audio-preview\",\\n  \"openai:gpt-4o-mini-audio-preview-2024-12-17\",\\n  \"openai:o1\",\\n  \"openai:o1-2024-12-17\",\\n  \"openai:o1-mini\",\\n  \"openai:o1-mini-2024-09-12\",\\n  \"openai:o1-preview\",\\n  \"openai:o1-preview-2024-09-12\",\\n  \"openai:o3-mini\",\\n  \"openai:o3-mini-2025-01-31\",\\n  \"test\",\\n]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 96, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 96, 'url': 'https://pydantic.com'}, page_content='@dataclass\\nclassModelRequestParameters:\\n\"\"\"Configuration for an agent\\'s request to a model, specifically related to tools and result handling.\"\"\"\\n  function_tools: list[ToolDefinition]\\n  allow_text_result: bool\\n  result_tools: list[ToolDefinition]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 96, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 96, 'url': 'https://pydantic.com'}, page_content='classModel(ABC):\\n\"\"\"Abstract class for a model.\"\"\"\\n  @abstractmethod\\n  async defrequest(\\n    self,\\n    messages: list[ModelMessage],\\n    model_settings: ModelSettings | None,\\n    model_request_parameters: ModelRequestParameters,\\n  ) -> tuple[ModelResponse, Usage]:\\n\"\"\"Make a request to the model.\"\"\"\\n    raise NotImplementedError()\\n  @asynccontextmanager\\n  async defrequest_stream(\\n    self,\\n    messages: list[ModelMessage],\\n    model_settings: ModelSettings | None,\\n    model_request_parameters: ModelRequestParameters,\\n  ) -> AsyncIterator[StreamedResponse]:\\n\"\"\"Make a request to the model and return a streaming response.\"\"\"\\n    # This method is not required, but you need to implement it if you want to support streamed responses\\n    raise NotImplementedError(f\\'Streamed requests not supported by this {self.__class__.__name__}\\')\\n    # yield is required to make this a generator for type checking\\n    # noinspection PyUnreachableCode\\n    yield # pragma: no cover\\n  @property\\n  @abstractmethod\\n  defmodel_name(self) -> str:\\n\"\"\"The model name.\"\"\"\\n    raise NotImplementedError()\\n  @property\\n  @abstractmethod\\n  defsystem(self) -> str | None:\\n\"\"\"The system / model provider, ex: openai.\"\"\"\\n    raise NotImplementedError()'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 96, 'url': 'https://pydantic.com'}, page_content='request(\\n  messages: listModelMessage[],\\n  model_settings: ModelSettings | None,\\n  model_request_parameters: ModelRequestParameters,\\n) -> tupleModelResponse[, Usage]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 96, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 96, 'url': 'https://pydantic.com'}, page_content='@abstractmethod\\nasync defrequest(\\n  self,\\n  messages: list[ModelMessage],\\n  model_settings: ModelSettings | None,\\n  model_request_parameters: ModelRequestParameters,\\n) -> tuple[ModelResponse, Usage]:\\n\"\"\"Make a request to the model.\"\"\"\\n  raise NotImplementedError()'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 96, 'url': 'https://pydantic.com'}, page_content='request_stream(\\n  messages: listModelMessage[],\\n  model_settings: ModelSettings | None,\\n  model_request_parameters: ModelRequestParameters,\\n) -> AsyncIteratorStreamedResponse[]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 96, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 96, 'url': 'https://pydantic.com'}, page_content='@asynccontextmanager\\nasync defrequest_stream(\\n  self,\\n  messages: list[ModelMessage],\\n  model_settings: ModelSettings | None,\\n  model_request_parameters: ModelRequestParameters,\\n) -> AsyncIterator[StreamedResponse]:\\n\"\"\"Make a request to the model and return a streaming response.\"\"\"\\n  # This method is not required, but you need to implement it if you want to support streamed responses\\n  raise NotImplementedError(f\\'Streamed requests not supported by this {self.__class__.__name__}\\')\\n  # yield is required to make this a generator for type checking\\n  # noinspection PyUnreachableCode\\n  yield # pragma: no cover'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 96, 'url': 'https://pydantic.com'}, page_content='model_name: str'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 96, 'url': 'https://pydantic.com'}, page_content='system: str | None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 97, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 97, 'url': 'https://pydantic.com'}, page_content='@dataclass\\nclassStreamedResponse(ABC):\\n\"\"\"Streamed response from an LLM when calling a tool.\"\"\"\\n  _parts_manager: ModelResponsePartsManager = field(default_factory=ModelResponsePartsManager, init=False)\\n  _event_iterator: AsyncIterator[ModelResponseStreamEvent] | None = field(default=None, init=False)\\n  _usage: Usage = field(default_factory=Usage, init=False)\\n  def__aiter__(self) -> AsyncIterator[ModelResponseStreamEvent]:\\n\"\"\"Stream the response as an async iterable of [`ModelResponseStreamEvent`][pydantic_ai.messages.ModelResponseStreamEvent]s.\"\"\"\\n    if self._event_iterator is None:\\n      self._event_iterator = self._get_event_iterator()\\n    return self._event_iterator\\n  @abstractmethod\\n  async def_get_event_iterator(self) -> AsyncIterator[ModelResponseStreamEvent]:\\n\"\"\"Return an async iterator of [`ModelResponseStreamEvent`][pydantic_ai.messages.ModelResponseStreamEvent]s.\\n    This method should be implemented by subclasses to translate the vendor-specific stream of events into\\n    pydantic_ai-format events.\\n    It should use the `_parts_manager` to handle deltas, and should update the `_usage` attributes as it goes.\\n    \"\"\"\\n    raise NotImplementedError()\\n    # noinspection PyUnreachableCode\\n    yield\\n  defget(self) -> ModelResponse:\\n\"\"\"Build a [`ModelResponse`][pydantic_ai.messages.ModelResponse] from the data received from the stream so far.\"\"\"\\n    return ModelResponse(\\n      parts=self._parts_manager.get_parts(), model_name=self.model_name, timestamp=self.timestamp\\n    )\\n  defusage(self) -> Usage:\\n\"\"\"Get the usage of the response so far. This will not be the final usage until the stream is exhausted.\"\"\"\\n    return self._usage\\n  @property\\n  @abstractmethod\\n  defmodel_name(self) -> str:\\n\"\"\"Get the model name of the response.\"\"\"\\n    raise NotImplementedError()\\n  @property\\n  @abstractmethod\\n  deftimestamp(self) -> datetime:\\n\"\"\"Get the timestamp of the response.\"\"\"\\n    raise NotImplementedError()'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 97, 'url': 'https://pydantic.com'}, page_content='__aiter__() -> AsyncIteratorModelResponseStreamEvent[]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 97, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 97, 'url': 'https://pydantic.com'}, page_content='def__aiter__(self) -> AsyncIterator[ModelResponseStreamEvent]:\\n\"\"\"Stream the response as an async iterable of [`ModelResponseStreamEvent`][pydantic_ai.messages.ModelResponseStreamEvent]s.\"\"\"\\n  if self._event_iterator is None:\\n    self._event_iterator = self._get_event_iterator()\\n  return self._event_iterator'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 97, 'url': 'https://pydantic.com'}, page_content='get() -> ModelResponse'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 97, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 97, 'url': 'https://pydantic.com'}, page_content='defget(self) -> ModelResponse:\\n\"\"\"Build a [`ModelResponse`][pydantic_ai.messages.ModelResponse] from the data received from the stream so far.\"\"\"\\n  return ModelResponse(\\n    parts=self._parts_manager.get_parts(), model_name=self.model_name, timestamp=self.timestamp\\n  )'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 97, 'url': 'https://pydantic.com'}, page_content='usage() -> Usage'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 97, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 97, 'url': 'https://pydantic.com'}, page_content='defusage(self) -> Usage:\\n\"\"\"Get the usage of the response so far. This will not be the final usage until the stream is exhausted.\"\"\"\\n  return self._usage'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 97, 'url': 'https://pydantic.com'}, page_content='model_name: str'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 97, 'url': 'https://pydantic.com'}, page_content='timestamp: datetime'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 97, 'url': 'https://pydantic.com'}, page_content='ALLOW_MODEL_REQUESTS = True'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 97, 'url': 'https://pydantic.com'}, page_content='check_allow_model_requests() -> None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 97, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 97, 'url': 'https://pydantic.com'}, page_content='defcheck_allow_model_requests() -> None:\\n\"\"\"Check if model requests are allowed.\\n  If you\\'re defining your own models that have costs or latency associated with their use, you should call this in\\n  [`Model.request`][pydantic_ai.models.Model.request] and [`Model.request_stream`][pydantic_ai.models.Model.request_stream].\\n  Raises:\\n    RuntimeError: If model requests are not allowed.\\n  \"\"\"\\n  if not ALLOW_MODEL_REQUESTS:\\n    raise RuntimeError(\\'Model requests are not allowed, since ALLOW_MODEL_REQUESTS is False\\')'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 97, 'url': 'https://pydantic.com'}, page_content='override_allow_model_requests(\\n  allow_model_requests: bool,\\n) -> Iterator[None]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 98, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 98, 'url': 'https://pydantic.com'}, page_content='@contextmanager\\ndefoverride_allow_model_requests(allow_model_requests: bool) -> Iterator[None]:\\n\"\"\"Context manager to temporarily override [`ALLOW_MODEL_REQUESTS`][pydantic_ai.models.ALLOW_MODEL_REQUESTS].\\n  Args:\\n    allow_model_requests: Whether to allow model requests within the context.\\n  \"\"\"\\n  global ALLOW_MODEL_REQUESTS\\n  old_value = ALLOW_MODEL_REQUESTS\\n  ALLOW_MODEL_REQUESTS = allow_model_requests # pyright: ignore[reportConstantRedefinition]\\n  try:\\n    yield\\n  finally:\\n    ALLOW_MODEL_REQUESTS = old_value # pyright: ignore[reportConstantRedefinition]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 99, 'url': 'https://pydantic.com'}, page_content='LatestCohereModelNames = Literal[\\n  \"c4ai-aya-expanse-32b\",\\n  \"c4ai-aya-expanse-8b\",\\n  \"command\",\\n  \"command-light\",\\n  \"command-light-nightly\",\\n  \"command-nightly\",\\n  \"command-r\",\\n  \"command-r-03-2024\",\\n  \"command-r-08-2024\",\\n  \"command-r-plus\",\\n  \"command-r-plus-04-2024\",\\n  \"command-r-plus-08-2024\",\\n  \"command-r7b-12-2024\",\\n]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 99, 'url': 'https://pydantic.com'}, page_content='CohereModelName = Unionstr[, LatestCohereModelNames]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 99, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 99, 'url': 'https://pydantic.com'}, page_content='classCohereModelSettings(ModelSettings):\\n\"\"\"Settings used for a Cohere model request.\"\"\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 99, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 99, 'url': 'https://pydantic.com'}, page_content='@dataclass(init=False)\\nclassCohereModel(Model):\\n\"\"\"A model that uses the Cohere API.\\n  Internally, this uses the [Cohere Python client](\\n  https://github.com/cohere-ai/cohere-python) to interact with the API.\\n  Apart from `__init__`, all methods are private or match those of the base class.\\n  \"\"\"\\n  client: AsyncClientV2 = field(repr=False)\\n  _model_name: CohereModelName = field(repr=False)\\n  _system: str | None = field(default=\\'cohere\\', repr=False)\\n  def__init__(\\n    self,\\n    model_name: CohereModelName,\\n    *,\\n    api_key: str | None = None,\\n    cohere_client: AsyncClientV2 | None = None,\\n    http_client: AsyncHTTPClient | None = None,\\n  ):\\n\"\"\"Initialize an Cohere model.\\n    Args:\\n      model_name: The name of the Cohere model to use. List of model names\\n        available here.\\n      api_key: The API key to use for authentication, if not provided, the\\n        `CO_API_KEY` environment variable will be used if available.\\n      cohere_client: An existing Cohere async client to use. If provided,\\n        `api_key` and `http_client` must be `None`.\\n      http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.\\n    \"\"\"\\n    self._model_name: CohereModelName = model_name\\n    if cohere_client is not None:\\n      assert http_client is None, \\'Cannot provide both `cohere_client` and `http_client`\\'\\n      assert api_key is None, \\'Cannot provide both `cohere_client` and `api_key`\\'\\n      self.client = cohere_client\\n    else:\\n      self.client = AsyncClientV2(api_key=api_key, httpx_client=http_client)\\n  async defrequest(\\n    self,\\n    messages: list[ModelMessage],\\n    model_settings: ModelSettings | None,\\n    model_request_parameters: ModelRequestParameters,\\n  ) -> tuple[ModelResponse, result.Usage]:\\n    check_allow_model_requests()\\n    response = await self._chat(messages, cast(CohereModelSettings, model_settings or {}), model_request_parameters)\\n    return self._process_response(response), _map_usage(response)\\n  @property\\n  defmodel_name(self) -> CohereModelName:\\n\"\"\"The model name.\"\"\"\\n    return self._model_name\\n  @property\\n  defsystem(self) -> str | None:\\n\"\"\"The system / model provider.\"\"\"\\n    return self._system\\n  async def_chat(\\n    self,\\n    messages: list[ModelMessage],\\n    model_settings: CohereModelSettings,\\n    model_request_parameters: ModelRequestParameters,\\n  ) -> ChatResponse:\\n    tools = self._get_tools(model_request_parameters)\\n    cohere_messages = list(chain(*(self._map_message(m) for m in messages)))\\n    return await self.client.chat(\\n      model=self._model_name,\\n      messages=cohere_messages,\\n      tools=tools or OMIT,\\n      max_tokens=model_settings.get(\\'max_tokens\\', OMIT),\\n      temperature=model_settings.get(\\'temperature\\', OMIT),\\n      p=model_settings.get(\\'top_p\\', OMIT),\\n      seed=model_settings.get(\\'seed\\', OMIT),\\n      presence_penalty=model_settings.get(\\'presence_penalty\\', OMIT),\\n      frequency_penalty=model_settings.get(\\'frequency_penalty\\', OMIT),\\n    )\\n  def_process_response(self, response: ChatResponse) -> ModelResponse:\\n\"\"\"Process a non-streamed response, and prepare a message to return.\"\"\"\\n    parts: list[ModelResponsePart] = []\\n    if response.message.content is not None and len(response.message.content) > 0:\\n      # While Cohere\\'s API returns a list, it only does that for future proofing\\n      # and currently only one item is being returned.\\n      choice = response.message.content[0]\\n      parts.append(TextPart(choice.text))\\n    for c in response.message.tool_calls or []:\\n      if c.function and c.function.name and c.function.arguments:\\n        parts.append(\\n          ToolCallPart(\\n            tool_name=c.function.name,\\n            args=c.function.arguments,\\n            tool_call_id=c.id,\\n          )\\n        )\\n    return ModelResponse(parts=parts, model_name=self._model_name)\\n  def_map_message(self, message: ModelMessage) -> Iterable[ChatMessageV2]:\\n\"\"\"Just maps a `pydantic_ai.Message` to a `cohere.ChatMessageV2`.\"\"\"\\n    if isinstance(message, ModelRequest):\\n      yield from self._map_user_message(message)\\n    elif isinstance(message, ModelResponse):\\n      texts: list[str] = []\\n      tool_calls: list[ToolCallV2] = []\\n      for item in message.parts:\\n        if isinstance(item, TextPart):\\n          texts.append(item.content)\\n        elif isinstance(item, ToolCallPart):\\n          tool_calls.append(self._map_tool_call(item))\\n        else:\\n          assert_never(item)\\n      message_param = AssistantChatMessageV2(role=\\'assistant\\')\\n      if texts:\\n        message_param.content = [TextAssistantMessageContentItem(text=\\'\\\\n\\\\n\\'.join(texts))]\\n      if tool_calls:\\n        message_param.tool_calls = tool_calls\\n      yield message_param\\n    else:\\n      assert_never(message)\\n  def_get_tools(self, model_request_parameters: ModelRequestParameters) -> list[ToolV2]:\\n    tools = [self._map_tool_definition(r) for r in model_request_parameters.function_tools]\\n    if model_request_parameters.result_tools:\\n      tools += [self._map_tool_definition(r) for r in model_request_parameters.result_tools]\\n    return tools\\n  @staticmethod\\n  def_map_tool_call(t: ToolCallPart) -> ToolCallV2:\\n    return ToolCallV2(\\n      id=_guard_tool_call_id(t=t, model_source=\\'Cohere\\'),\\n      type=\\'function\\',\\n      function=ToolCallV2Function(\\n        name=t.tool_name,\\n        arguments=t.args_as_json_str(),\\n      ),\\n    )\\n  @staticmethod\\n  def_map_tool_definition(f: ToolDefinition) -> ToolV2:\\n    return ToolV2(\\n      type=\\'function\\',\\n      function=ToolV2Function(\\n        name=f.name,\\n        description=f.description,\\n        parameters=f.parameters_json_schema,\\n      ),\\n    )\\n  @classmethod\\n  def_map_user_message(cls, message: ModelRequest) -> Iterable[ChatMessageV2]:\\n    for part in message.parts:\\n      if isinstance(part, SystemPromptPart):\\n        yield SystemChatMessageV2(role=\\'system\\', content=part.content)\\n      elif isinstance(part, UserPromptPart):\\n        yield UserChatMessageV2(role=\\'user\\', content=part.content)\\n      elif isinstance(part, ToolReturnPart):\\n        yield ToolChatMessageV2(\\n          role=\\'tool\\',\\n          tool_call_id=_guard_tool_call_id(t=part, model_source=\\'Cohere\\'),\\n          content=part.model_response_str(),\\n        )\\n      elif isinstance(part, RetryPromptPart):\\n        if part.tool_name is None:\\n          yield UserChatMessageV2(role=\\'user\\', content=part.model_response())\\n        else:\\n          yield ToolChatMessageV2(\\n            role=\\'tool\\',\\n            tool_call_id=_guard_tool_call_id(t=part, model_source=\\'Cohere\\'),\\n            content=part.model_response(),\\n          )\\n      else:\\n        assert_never(part)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 99, 'url': 'https://pydantic.com'}, page_content='__init__(\\n  model_name: CohereModelName,\\n  *,\\n  api_key: str | None = None,\\n  cohere_client: AsyncClientV2 | None = None,\\n  http_client: AsyncClient | None = None\\n)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 100, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 100, 'url': 'https://pydantic.com'}, page_content='def__init__(\\n  self,\\n  model_name: CohereModelName,\\n  *,\\n  api_key: str | None = None,\\n  cohere_client: AsyncClientV2 | None = None,\\n  http_client: AsyncHTTPClient | None = None,\\n):\\n\"\"\"Initialize an Cohere model.\\n  Args:\\n    model_name: The name of the Cohere model to use. List of model names\\n      available here.\\n    api_key: The API key to use for authentication, if not provided, the\\n      `CO_API_KEY` environment variable will be used if available.\\n    cohere_client: An existing Cohere async client to use. If provided,\\n      `api_key` and `http_client` must be `None`.\\n    http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.\\n  \"\"\"\\n  self._model_name: CohereModelName = model_name\\n  if cohere_client is not None:\\n    assert http_client is None, \\'Cannot provide both `cohere_client` and `http_client`\\'\\n    assert api_key is None, \\'Cannot provide both `cohere_client` and `api_key`\\'\\n    self.client = cohere_client\\n  else:\\n    self.client = AsyncClientV2(api_key=api_key, httpx_client=http_client)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 100, 'url': 'https://pydantic.com'}, page_content='model_name: CohereModelName'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 100, 'url': 'https://pydantic.com'}, page_content='system: str | None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 101, 'url': 'https://pydantic.com'}, page_content='frompydantic_aiimport Agent\\nfrompydantic_ai.messagesimport ModelMessage, ModelResponse, TextPart\\nfrompydantic_ai.models.functionimport FunctionModel, AgentInfo\\nmy_agent = Agent(\\'openai:gpt-4o\\')\\n\\nasync defmodel_function(\\n  messages: list[ModelMessage], info: AgentInfo\\n) -> ModelResponse:\\n  print(messages)\\n\"\"\"\\n  [\\n    ModelRequest(\\n      parts=[\\n        UserPromptPart(\\n          content=\\'Testing my agent...\\',\\n          timestamp=datetime.datetime(...),\\n          part_kind=\\'user-prompt\\',\\n        )\\n      ],\\n      kind=\\'request\\',\\n    )\\n  ]\\n  \"\"\"\\n  print(info)\\n\"\"\"\\n  AgentInfo(\\n    function_tools=[], allow_text_result=True, result_tools=[], model_settings=None\\n  )\\n  \"\"\"\\n  return ModelResponse(parts=[TextPart(\\'hello world\\')])\\n\\nasync deftest_my_agent():\\n\"\"\"Unit test for my_agent, to be run by pytest.\"\"\"\\n  with my_agent.override(model=FunctionModel(model_function)):\\n    result = await my_agent.run(\\'Testing my agent...\\')\\n    assert result.data == \\'hello world\\''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 101, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 101, 'url': 'https://pydantic.com'}, page_content='@dataclass(init=False)\\nclassFunctionModel(Model):\\n\"\"\"A model controlled by a local function.\\n  Apart from `__init__`, all methods are private or match those of the base class.\\n  \"\"\"\\n  function: FunctionDef | None = None\\n  stream_function: StreamFunctionDef | None = None\\n  _model_name: str = field(repr=False)\\n  _system: str | None = field(default=None, repr=False)\\n  @overload\\n  def__init__(self, function: FunctionDef) -> None: ...\\n  @overload\\n  def__init__(self, *, stream_function: StreamFunctionDef) -> None: ...\\n  @overload\\n  def__init__(self, function: FunctionDef, *, stream_function: StreamFunctionDef) -> None: ...\\n  def__init__(self, function: FunctionDef | None = None, *, stream_function: StreamFunctionDef | None = None):\\n\"\"\"Initialize a `FunctionModel`.\\n    Either `function` or `stream_function` must be provided, providing both is allowed.\\n    Args:\\n      function: The function to call for non-streamed requests.\\n      stream_function: The function to call for streamed requests.\\n    \"\"\"\\n    if function is None and stream_function is None:\\n      raise TypeError(\\'Either `function` or `stream_function` must be provided\\')\\n    self.function = function\\n    self.stream_function = stream_function\\n    function_name = self.function.__name__ if self.function is not None else \\'\\'\\n    stream_function_name = self.stream_function.__name__ if self.stream_function is not None else \\'\\'\\n    self._model_name = f\\'function:{function_name}:{stream_function_name}\\'\\n  async defrequest(\\n    self,\\n    messages: list[ModelMessage],\\n    model_settings: ModelSettings | None,\\n    model_request_parameters: ModelRequestParameters,\\n  ) -> tuple[ModelResponse, usage.Usage]:\\n    agent_info = AgentInfo(\\n      model_request_parameters.function_tools,\\n      model_request_parameters.allow_text_result,\\n      model_request_parameters.result_tools,\\n      model_settings,\\n    )\\n    assert self.function is not None, \\'FunctionModel must receive a `function` to support non-streamed requests\\'\\n    if inspect.iscoroutinefunction(self.function):\\n      response = await self.function(messages, agent_info)\\n    else:\\n      response_ = await _utils.run_in_executor(self.function, messages, agent_info)\\n      assert isinstance(response_, ModelResponse), response_\\n      response = response_\\n    response.model_name = f\\'function:{self.function.__name__}\\'\\n    # TODO is `messages` right here? Should it just be new messages?\\n    return response, _estimate_usage(chain(messages, [response]))\\n  @asynccontextmanager\\n  async defrequest_stream(\\n    self,\\n    messages: list[ModelMessage],\\n    model_settings: ModelSettings | None,\\n    model_request_parameters: ModelRequestParameters,\\n  ) -> AsyncIterator[StreamedResponse]:\\n    agent_info = AgentInfo(\\n      model_request_parameters.function_tools,\\n      model_request_parameters.allow_text_result,\\n      model_request_parameters.result_tools,\\n      model_settings,\\n    )\\n    assert self.stream_function is not None, (\\n      \\'FunctionModel must receive a `stream_function` to support streamed requests\\'\\n    )\\n    response_stream = PeekableAsyncStream(self.stream_function(messages, agent_info))\\n    first = await response_stream.peek()\\n    if isinstance(first, _utils.Unset):\\n      raise ValueError(\\'Stream function must return at least one item\\')\\n    yield FunctionStreamedResponse(_model_name=f\\'function:{self.stream_function.__name__}\\', _iter=response_stream)\\n  @property\\n  defmodel_name(self) -> str:\\n\"\"\"The model name.\"\"\"\\n    return self._model_name\\n  @property\\n  defsystem(self) -> str | None:\\n\"\"\"The system / model provider.\"\"\"\\n    return self._system'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 101, 'url': 'https://pydantic.com'}, page_content='__init__(function: FunctionDef) -> None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 101, 'url': 'https://pydantic.com'}, page_content='__init__(*, stream_function: StreamFunctionDef) -> None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 101, 'url': 'https://pydantic.com'}, page_content='__init__(\\n  function: FunctionDef,\\n  *,\\n  stream_function: StreamFunctionDef\\n) -> None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 101, 'url': 'https://pydantic.com'}, page_content='__init__(\\n  function: FunctionDef | None = None,\\n  *,\\n  stream_function: StreamFunctionDef | None = None\\n)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 101, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 101, 'url': 'https://pydantic.com'}, page_content='def__init__(self, function: FunctionDef | None = None, *, stream_function: StreamFunctionDef | None = None):\\n\"\"\"Initialize a `FunctionModel`.\\n  Either `function` or `stream_function` must be provided, providing both is allowed.\\n  Args:\\n    function: The function to call for non-streamed requests.\\n    stream_function: The function to call for streamed requests.\\n  \"\"\"\\n  if function is None and stream_function is None:\\n    raise TypeError(\\'Either `function` or `stream_function` must be provided\\')\\n  self.function = function\\n  self.stream_function = stream_function\\n  function_name = self.function.__name__ if self.function is not None else \\'\\'\\n  stream_function_name = self.stream_function.__name__ if self.stream_function is not None else \\'\\'\\n  self._model_name = f\\'function:{function_name}:{stream_function_name}\\''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 101, 'url': 'https://pydantic.com'}, page_content='model_name: str'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 101, 'url': 'https://pydantic.com'}, page_content='system: str | None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 102, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 102, 'url': 'https://pydantic.com'}, page_content='@dataclass(frozen=True)\\nclassAgentInfo:\\n\"\"\"Information about an agent.\\n  This is passed as the second to functions used within [`FunctionModel`][pydantic_ai.models.function.FunctionModel].\\n  \"\"\"\\n  function_tools: list[ToolDefinition]\\n\"\"\"The function tools available on this agent.\\n  These are the tools registered via the [`tool`][pydantic_ai.Agent.tool] and\\n  [`tool_plain`][pydantic_ai.Agent.tool_plain] decorators.\\n  \"\"\"\\n  allow_text_result: bool\\n\"\"\"Whether a plain text result is allowed.\"\"\"\\n  result_tools: list[ToolDefinition]\\n\"\"\"The tools that can called as the final result of the run.\"\"\"\\n  model_settings: ModelSettings | None\\n\"\"\"The model settings passed to the run call.\"\"\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 102, 'url': 'https://pydantic.com'}, page_content='function_tools: listToolDefinition[]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 102, 'url': 'https://pydantic.com'}, page_content='allow_text_result: bool'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 102, 'url': 'https://pydantic.com'}, page_content='result_tools: listToolDefinition[]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 102, 'url': 'https://pydantic.com'}, page_content='model_settings: ModelSettings | None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 102, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 102, 'url': 'https://pydantic.com'}, page_content='@dataclass\\nclassDeltaToolCall:\\n\"\"\"Incremental change to a tool call.\\n  Used to describe a chunk when streaming structured responses.\\n  \"\"\"\\n  name: str | None = None\\n\"\"\"Incremental change to the name of the tool.\"\"\"\\n  json_args: str | None = None\\n\"\"\"Incremental change to the arguments as JSON\"\"\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 102, 'url': 'https://pydantic.com'}, page_content='name: str | None = None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 102, 'url': 'https://pydantic.com'}, page_content='json_args: str | None = None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 102, 'url': 'https://pydantic.com'}, page_content='DeltaToolCalls: TypeAlias = dictint[, DeltaToolCall]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 102, 'url': 'https://pydantic.com'}, page_content='FunctionDef: TypeAlias = Callable[\\n  list[ModelMessage[], AgentInfo],\\n  UnionModelResponse[, AwaitableModelResponse[]],\\n]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 102, 'url': 'https://pydantic.com'}, page_content='StreamFunctionDef: TypeAlias = Callable[\\n  list[ModelMessage[], AgentInfo],\\n  AsyncIteratorUnion[str[, DeltaToolCalls]],\\n]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 102, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 102, 'url': 'https://pydantic.com'}, page_content='@dataclass\\nclassFunctionStreamedResponse(StreamedResponse):\\n\"\"\"Implementation of `StreamedResponse` for [FunctionModel][pydantic_ai.models.function.FunctionModel].\"\"\"\\n  _model_name: str\\n  _iter: AsyncIterator[str | DeltaToolCalls]\\n  _timestamp: datetime = field(default_factory=_utils.now_utc)\\n  def__post_init__(self):\\n    self._usage += _estimate_usage([])\\n  async def_get_event_iterator(self) -> AsyncIterator[ModelResponseStreamEvent]:\\n    async for item in self._iter:\\n      if isinstance(item, str):\\n        response_tokens = _estimate_string_tokens(item)\\n        self._usage += usage.Usage(response_tokens=response_tokens, total_tokens=response_tokens)\\n        yield self._parts_manager.handle_text_delta(vendor_part_id=\\'content\\', content=item)\\n      else:\\n        delta_tool_calls = item\\n        for dtc_index, delta_tool_call in delta_tool_calls.items():\\n          if delta_tool_call.json_args:\\n            response_tokens = _estimate_string_tokens(delta_tool_call.json_args)\\n            self._usage += usage.Usage(response_tokens=response_tokens, total_tokens=response_tokens)\\n          maybe_event = self._parts_manager.handle_tool_call_delta(\\n            vendor_part_id=dtc_index,\\n            tool_name=delta_tool_call.name,\\n            args=delta_tool_call.json_args,\\n            tool_call_id=None,\\n          )\\n          if maybe_event is not None:\\n            yield maybe_event\\n  @property\\n  defmodel_name(self) -> str:\\n\"\"\"Get the model name of the response.\"\"\"\\n    return self._model_name\\n  @property\\n  deftimestamp(self) -> datetime:\\n\"\"\"Get the timestamp of the response.\"\"\"\\n    return self._timestamp'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 103, 'url': 'https://pydantic.com'}, page_content='model_name: str'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 103, 'url': 'https://pydantic.com'}, page_content='timestamp: datetime'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 104, 'url': 'https://pydantic.com'}, page_content='LatestGeminiModelNames = Literal[\\n  \"gemini-1.5-flash\",\\n  \"gemini-1.5-flash-8b\",\\n  \"gemini-1.5-pro\",\\n  \"gemini-1.0-pro\",\\n  \"gemini-2.0-flash-exp\",\\n  \"gemini-2.0-flash-thinking-exp-01-21\",\\n  \"gemini-exp-1206\",\\n  \"gemini-2.0-flash\",\\n  \"gemini-2.0-flash-lite-preview-02-05\",\\n]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 104, 'url': 'https://pydantic.com'}, page_content='GeminiModelName = Unionstr[, LatestGeminiModelNames]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 104, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 104, 'url': 'https://pydantic.com'}, page_content='classGeminiModelSettings(ModelSettings):\\n\"\"\"Settings used for a Gemini model request.\"\"\"\\n  gemini_safety_settings: list[GeminiSafetySettings]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 105, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 105, 'url': 'https://pydantic.com'}, page_content='@dataclass(init=False)\\nclassGeminiModel(Model):\\n\"\"\"A model that uses Gemini via `generativelanguage.googleapis.com` API.\\n  This is implemented from scratch rather than using a dedicated SDK, good API documentation is\\n  available here.\\n  Apart from `__init__`, all methods are private or match those of the base class.\\n  \"\"\"\\n  http_client: AsyncHTTPClient = field(repr=False)\\n  _model_name: GeminiModelName = field(repr=False)\\n  _auth: AuthProtocol | None = field(repr=False)\\n  _url: str | None = field(repr=False)\\n  _system: str | None = field(default=\\'google-gla\\', repr=False)\\n  def__init__(\\n    self,\\n    model_name: GeminiModelName,\\n    *,\\n    api_key: str | None = None,\\n    http_client: AsyncHTTPClient | None = None,\\n    url_template: str = \\'https://generativelanguage.googleapis.com/v1beta/models/{model}:\\',\\n  ):\\n\"\"\"Initialize a Gemini model.\\n    Args:\\n      model_name: The name of the model to use.\\n      api_key: The API key to use for authentication, if not provided, the `GEMINI_API_KEY` environment variable\\n        will be used if available.\\n      http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.\\n      url_template: The URL template to use for making requests, you shouldn\\'t need to change this,\\n        docs here,\\n        `model` is substituted with the model name, and `function` is added to the end of the URL.\\n    \"\"\"\\n    self._model_name = model_name\\n    if api_key is None:\\n      if env_api_key := os.getenv(\\'GEMINI_API_KEY\\'):\\n        api_key = env_api_key\\n      else:\\n        raise exceptions.UserError(\\'API key must be provided or set in the GEMINI_API_KEY environment variable\\')\\n    self.http_client = http_client or cached_async_http_client()\\n    self._auth = ApiKeyAuth(api_key)\\n    self._url = url_template.format(model=model_name)\\n  @property\\n  defauth(self) -> AuthProtocol:\\n    assert self._auth is not None, \\'Auth not initialized\\'\\n    return self._auth\\n  @property\\n  defurl(self) -> str:\\n    assert self._url is not None, \\'URL not initialized\\'\\n    return self._url\\n  async defrequest(\\n    self,\\n    messages: list[ModelMessage],\\n    model_settings: ModelSettings | None,\\n    model_request_parameters: ModelRequestParameters,\\n  ) -> tuple[ModelResponse, usage.Usage]:\\n    check_allow_model_requests()\\n    async with self._make_request(\\n      messages, False, cast(GeminiModelSettings, model_settings or {}), model_request_parameters\\n    ) as http_response:\\n      response = _gemini_response_ta.validate_json(await http_response.aread())\\n    return self._process_response(response), _metadata_as_usage(response)\\n  @asynccontextmanager\\n  async defrequest_stream(\\n    self,\\n    messages: list[ModelMessage],\\n    model_settings: ModelSettings | None,\\n    model_request_parameters: ModelRequestParameters,\\n  ) -> AsyncIterator[StreamedResponse]:\\n    check_allow_model_requests()\\n    async with self._make_request(\\n      messages, True, cast(GeminiModelSettings, model_settings or {}), model_request_parameters\\n    ) as http_response:\\n      yield await self._process_streamed_response(http_response)\\n  @property\\n  defmodel_name(self) -> GeminiModelName:\\n\"\"\"The model name.\"\"\"\\n    return self._model_name\\n  @property\\n  defsystem(self) -> str | None:\\n\"\"\"The system / model provider.\"\"\"\\n    return self._system\\n  def_get_tools(self, model_request_parameters: ModelRequestParameters) -> _GeminiTools | None:\\n    tools = [_function_from_abstract_tool(t) for t in model_request_parameters.function_tools]\\n    if model_request_parameters.result_tools:\\n      tools += [_function_from_abstract_tool(t) for t in model_request_parameters.result_tools]\\n    return _GeminiTools(function_declarations=tools) if tools else None\\n  def_get_tool_config(\\n    self, model_request_parameters: ModelRequestParameters, tools: _GeminiTools | None\\n  ) -> _GeminiToolConfig | None:\\n    if model_request_parameters.allow_text_result:\\n      return None\\n    elif tools:\\n      return _tool_config([t[\\'name\\'] for t in tools[\\'function_declarations\\']])\\n    else:\\n      return _tool_config([])\\n  @asynccontextmanager\\n  async def_make_request(\\n    self,\\n    messages: list[ModelMessage],\\n    streamed: bool,\\n    model_settings: GeminiModelSettings,\\n    model_request_parameters: ModelRequestParameters,\\n  ) -> AsyncIterator[HTTPResponse]:\\n    tools = self._get_tools(model_request_parameters)\\n    tool_config = self._get_tool_config(model_request_parameters, tools)\\n    sys_prompt_parts, contents = self._message_to_gemini_content(messages)\\n    request_data = _GeminiRequest(contents=contents)\\n    if sys_prompt_parts:\\n      request_data[\\'system_instruction\\'] = _GeminiTextContent(role=\\'user\\', parts=sys_prompt_parts)\\n    if tools is not None:\\n      request_data[\\'tools\\'] = tools\\n    if tool_config is not None:\\n      request_data[\\'tool_config\\'] = tool_config\\n    generation_config: _GeminiGenerationConfig = {}\\n    if model_settings:\\n      if (max_tokens := model_settings.get(\\'max_tokens\\')) is not None:\\n        generation_config[\\'max_output_tokens\\'] = max_tokens\\n      if (temperature := model_settings.get(\\'temperature\\')) is not None:\\n        generation_config[\\'temperature\\'] = temperature\\n      if (top_p := model_settings.get(\\'top_p\\')) is not None:\\n        generation_config[\\'top_p\\'] = top_p\\n      if (presence_penalty := model_settings.get(\\'presence_penalty\\')) is not None:\\n        generation_config[\\'presence_penalty\\'] = presence_penalty\\n      if (frequency_penalty := model_settings.get(\\'frequency_penalty\\')) is not None:\\n        generation_config[\\'frequency_penalty\\'] = frequency_penalty\\n      if (gemini_safety_settings := model_settings.get(\\'gemini_safety_settings\\')) != []:\\n        request_data[\\'safety_settings\\'] = gemini_safety_settings\\n    if generation_config:\\n      request_data[\\'generation_config\\'] = generation_config\\n    url = self.url + (\\'streamGenerateContent\\' if streamed else \\'generateContent\\')\\n    headers = {\\n      \\'Content-Type\\': \\'application/json\\',\\n      \\'User-Agent\\': get_user_agent(),\\n      **await self.auth.headers(),\\n    }\\n    request_json = _gemini_request_ta.dump_json(request_data, by_alias=True)\\n    async with self.http_client.stream(\\n      \\'POST\\',\\n      url,\\n      content=request_json,\\n      headers=headers,\\n      timeout=model_settings.get(\\'timeout\\', USE_CLIENT_DEFAULT),\\n    ) as r:\\n      if r.status_code != 200:\\n        await r.aread()\\n        raise exceptions.UnexpectedModelBehavior(f\\'Unexpected response from gemini {r.status_code}\\', r.text)\\n      yield r\\n  def_process_response(self, response: _GeminiResponse) -> ModelResponse:\\n    if len(response[\\'candidates\\']) != 1:\\n      raise UnexpectedModelBehavior(\\'Expected exactly one candidate in Gemini response\\')\\n    if \\'content\\' not in response[\\'candidates\\'][0]:\\n      if response[\\'candidates\\'][0].get(\\'finish_reason\\') == \\'SAFETY\\':\\n        raise UnexpectedModelBehavior(\\'Safety settings triggered\\', str(response))\\n      else:\\n        raise UnexpectedModelBehavior(\\'Content field missing from Gemini response\\', str(response))\\n    parts = response[\\'candidates\\'][0][\\'content\\'][\\'parts\\']\\n    return _process_response_from_parts(parts, model_name=response.get(\\'model_version\\', self._model_name))\\n  async def_process_streamed_response(self, http_response: HTTPResponse) -> StreamedResponse:\\n\"\"\"Process a streamed response, and prepare a streaming response to return.\"\"\"\\n    aiter_bytes = http_response.aiter_bytes()\\n    start_response: _GeminiResponse | None = None\\n    content = bytearray()\\n    async for chunk in aiter_bytes:\\n      content.extend(chunk)\\n      responses = _gemini_streamed_response_ta.validate_json(\\n        _ensure_decodeable(content),\\n        experimental_allow_partial=\\'trailing-strings\\',\\n      )\\n      if responses:\\n        last = responses[-1]\\n        if last[\\'candidates\\'] and last[\\'candidates\\'][0].get(\\'content\\', {}).get(\\'parts\\'):\\n          start_response = last\\n          break\\n    if start_response is None:\\n      raise UnexpectedModelBehavior(\\'Streamed response ended without content or tool calls\\')\\n    return GeminiStreamedResponse(_model_name=self._model_name, _content=content, _stream=aiter_bytes)\\n  @classmethod\\n  def_message_to_gemini_content(\\n    cls, messages: list[ModelMessage]\\n  ) -> tuple[list[_GeminiTextPart], list[_GeminiContent]]:\\n    sys_prompt_parts: list[_GeminiTextPart] = []\\n    contents: list[_GeminiContent] = []\\n    for m in messages:\\n      if isinstance(m, ModelRequest):\\n        message_parts: list[_GeminiPartUnion] = []\\n        for part in m.parts:\\n          if isinstance(part, SystemPromptPart):\\n            sys_prompt_parts.append(_GeminiTextPart(text=part.content))\\n          elif isinstance(part, UserPromptPart):\\n            message_parts.append(_GeminiTextPart(text=part.content))\\n          elif isinstance(part, ToolReturnPart):\\n            message_parts.append(_response_part_from_response(part.tool_name, part.model_response_object()))\\n          elif isinstance(part, RetryPromptPart):\\n            if part.tool_name is None:\\n              message_parts.append(_GeminiTextPart(text=part.model_response()))\\n            else:\\n              response = {\\'call_error\\': part.model_response()}\\n              message_parts.append(_response_part_from_response(part.tool_name, response))\\n          else:\\n            assert_never(part)\\n        if message_parts:\\n          contents.append(_GeminiContent(role=\\'user\\', parts=message_parts))\\n      elif isinstance(m, ModelResponse):\\n        contents.append(_content_model_response(m))\\n      else:\\n        assert_never(m)\\n    return sys_prompt_parts, contents'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 105, 'url': 'https://pydantic.com'}, page_content='__init__(\\n  model_name: GeminiModelName,\\n  *,\\n  api_key: str | None = None,\\n  http_client: AsyncClient | None = None,\\n  url_template: str = \"https://generativelanguage.googleapis.com/v1beta/models/{model}:\"\\n)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 105, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 105, 'url': 'https://pydantic.com'}, page_content='def__init__(\\n  self,\\n  model_name: GeminiModelName,\\n  *,\\n  api_key: str | None = None,\\n  http_client: AsyncHTTPClient | None = None,\\n  url_template: str = \\'https://generativelanguage.googleapis.com/v1beta/models/{model}:\\',\\n):\\n\"\"\"Initialize a Gemini model.\\n  Args:\\n    model_name: The name of the model to use.\\n    api_key: The API key to use for authentication, if not provided, the `GEMINI_API_KEY` environment variable\\n      will be used if available.\\n    http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.\\n    url_template: The URL template to use for making requests, you shouldn\\'t need to change this,\\n      docs here,\\n      `model` is substituted with the model name, and `function` is added to the end of the URL.\\n  \"\"\"\\n  self._model_name = model_name\\n  if api_key is None:\\n    if env_api_key := os.getenv(\\'GEMINI_API_KEY\\'):\\n      api_key = env_api_key\\n    else:\\n      raise exceptions.UserError(\\'API key must be provided or set in the GEMINI_API_KEY environment variable\\')\\n  self.http_client = http_client or cached_async_http_client()\\n  self._auth = ApiKeyAuth(api_key)\\n  self._url = url_template.format(model=model_name)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 105, 'url': 'https://pydantic.com'}, page_content='model_name: GeminiModelName'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 105, 'url': 'https://pydantic.com'}, page_content='system: str | None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 105, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 105, 'url': 'https://pydantic.com'}, page_content='classAuthProtocol(Protocol):\\n\"\"\"Abstract definition for Gemini authentication.\"\"\"\\n  async defheaders(self) -> dict[str, str]: ...'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 105, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 105, 'url': 'https://pydantic.com'}, page_content='@dataclass\\nclassApiKeyAuth:\\n\"\"\"Authentication using an API key for the `X-Goog-Api-Key` header.\"\"\"\\n  api_key: str\\n  async defheaders(self) -> dict[str, str]:\\n    # https://cloud.google.com/docs/authentication/api-keys-use#using-with-rest\\n    return {\\'X-Goog-Api-Key\\': self.api_key}'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 106, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 106, 'url': 'https://pydantic.com'}, page_content='@dataclass\\nclassGeminiStreamedResponse(StreamedResponse):\\n\"\"\"Implementation of `StreamedResponse` for the Gemini model.\"\"\"\\n  _model_name: GeminiModelName\\n  _content: bytearray\\n  _stream: AsyncIterator[bytes]\\n  _timestamp: datetime = field(default_factory=_utils.now_utc, init=False)\\n  async def_get_event_iterator(self) -> AsyncIterator[ModelResponseStreamEvent]:\\n    async for gemini_response in self._get_gemini_responses():\\n      candidate = gemini_response[\\'candidates\\'][0]\\n      if \\'content\\' not in candidate:\\n        raise UnexpectedModelBehavior(\\'Streamed response has no content field\\')\\n      gemini_part: _GeminiPartUnion\\n      for gemini_part in candidate[\\'content\\'][\\'parts\\']:\\n        if \\'text\\' in gemini_part:\\n          # Using vendor_part_id=None means we can produce multiple text parts if their deltas are sprinkled\\n          # amongst the tool call deltas\\n          yield self._parts_manager.handle_text_delta(vendor_part_id=None, content=gemini_part[\\'text\\'])\\n        elif \\'function_call\\' in gemini_part:\\n          # Here, we assume all function_call parts are complete and don\\'t have deltas.\\n          # We do this by assigning a unique randomly generated \"vendor_part_id\".\\n          # We need to confirm whether this is actually true, but if it isn\\'t, we can still handle it properly\\n          # it would just be a bit more complicated. And we\\'d need to confirm the intended semantics.\\n          maybe_event = self._parts_manager.handle_tool_call_delta(\\n            vendor_part_id=uuid4(),\\n            tool_name=gemini_part[\\'function_call\\'][\\'name\\'],\\n            args=gemini_part[\\'function_call\\'][\\'args\\'],\\n            tool_call_id=None,\\n          )\\n          if maybe_event is not None:\\n            yield maybe_event\\n        else:\\n          assert \\'function_response\\' in gemini_part, f\\'Unexpected part: {gemini_part}\\'\\n  async def_get_gemini_responses(self) -> AsyncIterator[_GeminiResponse]:\\n    # This method exists to ensure we only yield completed items, so we don\\'t need to worry about\\n    # partial gemini responses, which would make everything more complicated\\n    gemini_responses: list[_GeminiResponse] = []\\n    current_gemini_response_index = 0\\n    # Right now, there are some circumstances where we will have information that could be yielded sooner than it is\\n    # But changing that would make things a lot more complicated.\\n    async for chunk in self._stream:\\n      self._content.extend(chunk)\\n      gemini_responses = _gemini_streamed_response_ta.validate_json(\\n        _ensure_decodeable(self._content),\\n        experimental_allow_partial=\\'trailing-strings\\',\\n      )\\n      # The idea: yield only up to the latest response, which might still be partial.\\n      # Note that if the latest response is complete, we could yield it immediately, but there\\'s not a good\\n      # allow_partial API to determine if the last item in the list is complete.\\n      responses_to_yield = gemini_responses[:-1]\\n      for r in responses_to_yield[current_gemini_response_index:]:\\n        current_gemini_response_index += 1\\n        self._usage += _metadata_as_usage(r)\\n        yield r\\n    # Now yield the final response, which should be complete\\n    if gemini_responses:\\n      r = gemini_responses[-1]\\n      self._usage += _metadata_as_usage(r)\\n      yield r\\n  @property\\n  defmodel_name(self) -> GeminiModelName:\\n\"\"\"Get the model name of the response.\"\"\"\\n    return self._model_name\\n  @property\\n  deftimestamp(self) -> datetime:\\n\"\"\"Get the timestamp of the response.\"\"\"\\n    return self._timestamp'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 106, 'url': 'https://pydantic.com'}, page_content='model_name: GeminiModelName'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 106, 'url': 'https://pydantic.com'}, page_content='timestamp: datetime'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 106, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 106, 'url': 'https://pydantic.com'}, page_content='classGeminiSafetySettings(TypedDict):\\n\"\"\"Safety settings options for Gemini model request.\\n  See Gemini API docs for safety category and threshold descriptions.\\n  For an example on how to use `GeminiSafetySettings`, see [here](../../agents.md#model-specific-settings).\\n  \"\"\"\\n  category: Literal[\\n    \\'HARM_CATEGORY_UNSPECIFIED\\',\\n    \\'HARM_CATEGORY_HARASSMENT\\',\\n    \\'HARM_CATEGORY_HATE_SPEECH\\',\\n    \\'HARM_CATEGORY_SEXUALLY_EXPLICIT\\',\\n    \\'HARM_CATEGORY_DANGEROUS_CONTENT\\',\\n    \\'HARM_CATEGORY_CIVIC_INTEGRITY\\',\\n  ]\\n\"\"\"\\n  Safety settings category.\\n  \"\"\"\\n  threshold: Literal[\\n    \\'HARM_BLOCK_THRESHOLD_UNSPECIFIED\\',\\n    \\'BLOCK_LOW_AND_ABOVE\\',\\n    \\'BLOCK_MEDIUM_AND_ABOVE\\',\\n    \\'BLOCK_ONLY_HIGH\\',\\n    \\'BLOCK_NONE\\',\\n    \\'OFF\\',\\n  ]\\n\"\"\"\\n  Safety settings threshold.\\n  \"\"\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 106, 'url': 'https://pydantic.com'}, page_content='category: Literal[\\n  \"HARM_CATEGORY_UNSPECIFIED\",\\n  \"HARM_CATEGORY_HARASSMENT\",\\n  \"HARM_CATEGORY_HATE_SPEECH\",\\n  \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\\n  \"HARM_CATEGORY_DANGEROUS_CONTENT\",\\n  \"HARM_CATEGORY_CIVIC_INTEGRITY\",\\n]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 106, 'url': 'https://pydantic.com'}, page_content='threshold: Literal[\\n  \"HARM_BLOCK_THRESHOLD_UNSPECIFIED\",\\n  \"BLOCK_LOW_AND_ABOVE\",\\n  \"BLOCK_MEDIUM_AND_ABOVE\",\\n  \"BLOCK_ONLY_HIGH\",\\n  \"BLOCK_NONE\",\\n  \"OFF\",\\n]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 107, 'url': 'https://pydantic.com'}, page_content='LatestGroqModelNames = Literal[\\n  \"llama-3.3-70b-versatile\",\\n  \"llama-3.3-70b-specdec\",\\n  \"llama-3.1-8b-instant\",\\n  \"llama-3.2-1b-preview\",\\n  \"llama-3.2-3b-preview\",\\n  \"llama-3.2-11b-vision-preview\",\\n  \"llama-3.2-90b-vision-preview\",\\n  \"llama3-70b-8192\",\\n  \"llama3-8b-8192\",\\n  \"mixtral-8x7b-32768\",\\n  \"gemma2-9b-it\",\\n]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 107, 'url': 'https://pydantic.com'}, page_content='GroqModelName = Unionstr[, LatestGroqModelNames]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 107, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 107, 'url': 'https://pydantic.com'}, page_content='classGroqModelSettings(ModelSettings):\\n\"\"\"Settings used for a Groq model request.\"\"\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 107, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 107, 'url': 'https://pydantic.com'}, page_content='@dataclass(init=False)\\nclassGroqModel(Model):\\n\"\"\"A model that uses the Groq API.\\n  Internally, this uses the Groq Python client to interact with the API.\\n  Apart from `__init__`, all methods are private or match those of the base class.\\n  \"\"\"\\n  client: AsyncGroq = field(repr=False)\\n  _model_name: GroqModelName = field(repr=False)\\n  _system: str | None = field(default=\\'groq\\', repr=False)\\n  def__init__(\\n    self,\\n    model_name: GroqModelName,\\n    *,\\n    api_key: str | None = None,\\n    groq_client: AsyncGroq | None = None,\\n    http_client: AsyncHTTPClient | None = None,\\n  ):\\n\"\"\"Initialize a Groq model.\\n    Args:\\n      model_name: The name of the Groq model to use. List of model names available\\n        here.\\n      api_key: The API key to use for authentication, if not provided, the `GROQ_API_KEY` environment variable\\n        will be used if available.\\n      groq_client: An existing\\n        `AsyncGroq`\\n        client to use, if provided, `api_key` and `http_client` must be `None`.\\n      http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.\\n    \"\"\"\\n    self._model_name = model_name\\n    if groq_client is not None:\\n      assert http_client is None, \\'Cannot provide both `groq_client` and `http_client`\\'\\n      assert api_key is None, \\'Cannot provide both `groq_client` and `api_key`\\'\\n      self.client = groq_client\\n    elif http_client is not None:\\n      self.client = AsyncGroq(api_key=api_key, http_client=http_client)\\n    else:\\n      self.client = AsyncGroq(api_key=api_key, http_client=cached_async_http_client())\\n  async defrequest(\\n    self,\\n    messages: list[ModelMessage],\\n    model_settings: ModelSettings | None,\\n    model_request_parameters: ModelRequestParameters,\\n  ) -> tuple[ModelResponse, usage.Usage]:\\n    check_allow_model_requests()\\n    response = await self._completions_create(\\n      messages, False, cast(GroqModelSettings, model_settings or {}), model_request_parameters\\n    )\\n    return self._process_response(response), _map_usage(response)\\n  @asynccontextmanager\\n  async defrequest_stream(\\n    self,\\n    messages: list[ModelMessage],\\n    model_settings: ModelSettings | None,\\n    model_request_parameters: ModelRequestParameters,\\n  ) -> AsyncIterator[StreamedResponse]:\\n    check_allow_model_requests()\\n    response = await self._completions_create(\\n      messages, True, cast(GroqModelSettings, model_settings or {}), model_request_parameters\\n    )\\n    async with response:\\n      yield await self._process_streamed_response(response)\\n  @property\\n  defmodel_name(self) -> GroqModelName:\\n\"\"\"The model name.\"\"\"\\n    return self._model_name\\n  @property\\n  defsystem(self) -> str | None:\\n\"\"\"The system / model provider.\"\"\"\\n    return self._system\\n  @overload\\n  async def_completions_create(\\n    self,\\n    messages: list[ModelMessage],\\n    stream: Literal[True],\\n    model_settings: GroqModelSettings,\\n    model_request_parameters: ModelRequestParameters,\\n  ) -> AsyncStream[ChatCompletionChunk]:\\n    pass\\n  @overload\\n  async def_completions_create(\\n    self,\\n    messages: list[ModelMessage],\\n    stream: Literal[False],\\n    model_settings: GroqModelSettings,\\n    model_request_parameters: ModelRequestParameters,\\n  ) -> chat.ChatCompletion:\\n    pass\\n  async def_completions_create(\\n    self,\\n    messages: list[ModelMessage],\\n    stream: bool,\\n    model_settings: GroqModelSettings,\\n    model_request_parameters: ModelRequestParameters,\\n  ) -> chat.ChatCompletion | AsyncStream[ChatCompletionChunk]:\\n    tools = self._get_tools(model_request_parameters)\\n    # standalone function to make it easier to override\\n    if not tools:\\n      tool_choice: Literal[\\'none\\', \\'required\\', \\'auto\\'] | None = None\\n    elif not model_request_parameters.allow_text_result:\\n      tool_choice = \\'required\\'\\n    else:\\n      tool_choice = \\'auto\\'\\n    groq_messages = list(chain(*(self._map_message(m) for m in messages)))\\n    return await self.client.chat.completions.create(\\n      model=str(self._model_name),\\n      messages=groq_messages,\\n      n=1,\\n      parallel_tool_calls=model_settings.get(\\'parallel_tool_calls\\', NOT_GIVEN),\\n      tools=tools or NOT_GIVEN,\\n      tool_choice=tool_choice or NOT_GIVEN,\\n      stream=stream,\\n      max_tokens=model_settings.get(\\'max_tokens\\', NOT_GIVEN),\\n      temperature=model_settings.get(\\'temperature\\', NOT_GIVEN),\\n      top_p=model_settings.get(\\'top_p\\', NOT_GIVEN),\\n      timeout=model_settings.get(\\'timeout\\', NOT_GIVEN),\\n      seed=model_settings.get(\\'seed\\', NOT_GIVEN),\\n      presence_penalty=model_settings.get(\\'presence_penalty\\', NOT_GIVEN),\\n      frequency_penalty=model_settings.get(\\'frequency_penalty\\', NOT_GIVEN),\\n      logit_bias=model_settings.get(\\'logit_bias\\', NOT_GIVEN),\\n    )\\n  def_process_response(self, response: chat.ChatCompletion) -> ModelResponse:\\n\"\"\"Process a non-streamed response, and prepare a message to return.\"\"\"\\n    timestamp = datetime.fromtimestamp(response.created, tz=timezone.utc)\\n    choice = response.choices[0]\\n    items: list[ModelResponsePart] = []\\n    if choice.message.content is not None:\\n      items.append(TextPart(content=choice.message.content))\\n    if choice.message.tool_calls is not None:\\n      for c in choice.message.tool_calls:\\n        items.append(ToolCallPart(tool_name=c.function.name, args=c.function.arguments, tool_call_id=c.id))\\n    return ModelResponse(items, model_name=response.model, timestamp=timestamp)\\n  async def_process_streamed_response(self, response: AsyncStream[ChatCompletionChunk]) -> GroqStreamedResponse:\\n\"\"\"Process a streamed response, and prepare a streaming response to return.\"\"\"\\n    peekable_response = _utils.PeekableAsyncStream(response)\\n    first_chunk = await peekable_response.peek()\\n    if isinstance(first_chunk, _utils.Unset):\\n      raise UnexpectedModelBehavior(\\'Streamed response ended without content or tool calls\\')\\n    return GroqStreamedResponse(\\n      _response=peekable_response,\\n      _model_name=self._model_name,\\n      _timestamp=datetime.fromtimestamp(first_chunk.created, tz=timezone.utc),\\n    )\\n  def_get_tools(self, model_request_parameters: ModelRequestParameters) -> list[chat.ChatCompletionToolParam]:\\n    tools = [self._map_tool_definition(r) for r in model_request_parameters.function_tools]\\n    if model_request_parameters.result_tools:\\n      tools += [self._map_tool_definition(r) for r in model_request_parameters.result_tools]\\n    return tools\\n  def_map_message(self, message: ModelMessage) -> Iterable[chat.ChatCompletionMessageParam]:\\n\"\"\"Just maps a `pydantic_ai.Message` to a `groq.types.ChatCompletionMessageParam`.\"\"\"\\n    if isinstance(message, ModelRequest):\\n      yield from self._map_user_message(message)\\n    elif isinstance(message, ModelResponse):\\n      texts: list[str] = []\\n      tool_calls: list[chat.ChatCompletionMessageToolCallParam] = []\\n      for item in message.parts:\\n        if isinstance(item, TextPart):\\n          texts.append(item.content)\\n        elif isinstance(item, ToolCallPart):\\n          tool_calls.append(self._map_tool_call(item))\\n        else:\\n          assert_never(item)\\n      message_param = chat.ChatCompletionAssistantMessageParam(role=\\'assistant\\')\\n      if texts:\\n        # Note: model responses from this model should only have one text item, so the following\\n        # shouldn\\'t merge multiple texts into one unless you switch models between runs:\\n        message_param[\\'content\\'] = \\'\\\\n\\\\n\\'.join(texts)\\n      if tool_calls:\\n        message_param[\\'tool_calls\\'] = tool_calls\\n      yield message_param\\n    else:\\n      assert_never(message)\\n  @staticmethod\\n  def_map_tool_call(t: ToolCallPart) -> chat.ChatCompletionMessageToolCallParam:\\n    return chat.ChatCompletionMessageToolCallParam(\\n      id=_guard_tool_call_id(t=t, model_source=\\'Groq\\'),\\n      type=\\'function\\',\\n      function={\\'name\\': t.tool_name, \\'arguments\\': t.args_as_json_str()},\\n    )\\n  @staticmethod\\n  def_map_tool_definition(f: ToolDefinition) -> chat.ChatCompletionToolParam:\\n    return {\\n      \\'type\\': \\'function\\',\\n      \\'function\\': {\\n        \\'name\\': f.name,\\n        \\'description\\': f.description,\\n        \\'parameters\\': f.parameters_json_schema,\\n      },\\n    }\\n  @classmethod\\n  def_map_user_message(cls, message: ModelRequest) -> Iterable[chat.ChatCompletionMessageParam]:\\n    for part in message.parts:\\n      if isinstance(part, SystemPromptPart):\\n        yield chat.ChatCompletionSystemMessageParam(role=\\'system\\', content=part.content)\\n      elif isinstance(part, UserPromptPart):\\n        yield chat.ChatCompletionUserMessageParam(role=\\'user\\', content=part.content)\\n      elif isinstance(part, ToolReturnPart):\\n        yield chat.ChatCompletionToolMessageParam(\\n          role=\\'tool\\',\\n          tool_call_id=_guard_tool_call_id(t=part, model_source=\\'Groq\\'),\\n          content=part.model_response_str(),\\n        )\\n      elif isinstance(part, RetryPromptPart):\\n        if part.tool_name is None:\\n          yield chat.ChatCompletionUserMessageParam(role=\\'user\\', content=part.model_response())\\n        else:\\n          yield chat.ChatCompletionToolMessageParam(\\n            role=\\'tool\\',\\n            tool_call_id=_guard_tool_call_id(t=part, model_source=\\'Groq\\'),\\n            content=part.model_response(),\\n          )'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 107, 'url': 'https://pydantic.com'}, page_content='__init__(\\n  model_name: GroqModelName,\\n  *,\\n  api_key: str | None = None,\\n  groq_client: AsyncGroq | None = None,\\n  http_client: AsyncClient | None = None\\n)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 108, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 108, 'url': 'https://pydantic.com'}, page_content='def__init__(\\n  self,\\n  model_name: GroqModelName,\\n  *,\\n  api_key: str | None = None,\\n  groq_client: AsyncGroq | None = None,\\n  http_client: AsyncHTTPClient | None = None,\\n):\\n\"\"\"Initialize a Groq model.\\n  Args:\\n    model_name: The name of the Groq model to use. List of model names available\\n      here.\\n    api_key: The API key to use for authentication, if not provided, the `GROQ_API_KEY` environment variable\\n      will be used if available.\\n    groq_client: An existing\\n      `AsyncGroq`\\n      client to use, if provided, `api_key` and `http_client` must be `None`.\\n    http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.\\n  \"\"\"\\n  self._model_name = model_name\\n  if groq_client is not None:\\n    assert http_client is None, \\'Cannot provide both `groq_client` and `http_client`\\'\\n    assert api_key is None, \\'Cannot provide both `groq_client` and `api_key`\\'\\n    self.client = groq_client\\n  elif http_client is not None:\\n    self.client = AsyncGroq(api_key=api_key, http_client=http_client)\\n  else:\\n    self.client = AsyncGroq(api_key=api_key, http_client=cached_async_http_client())'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 108, 'url': 'https://pydantic.com'}, page_content='model_name: GroqModelName'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 108, 'url': 'https://pydantic.com'}, page_content='system: str | None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 108, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 108, 'url': 'https://pydantic.com'}, page_content='@dataclass\\nclassGroqStreamedResponse(StreamedResponse):\\n\"\"\"Implementation of `StreamedResponse` for Groq models.\"\"\"\\n  _model_name: GroqModelName\\n  _response: AsyncIterable[ChatCompletionChunk]\\n  _timestamp: datetime\\n  async def_get_event_iterator(self) -> AsyncIterator[ModelResponseStreamEvent]:\\n    async for chunk in self._response:\\n      self._usage += _map_usage(chunk)\\n      try:\\n        choice = chunk.choices[0]\\n      except IndexError:\\n        continue\\n      # Handle the text part of the response\\n      content = choice.delta.content\\n      if content is not None:\\n        yield self._parts_manager.handle_text_delta(vendor_part_id=\\'content\\', content=content)\\n      # Handle the tool calls\\n      for dtc in choice.delta.tool_calls or []:\\n        maybe_event = self._parts_manager.handle_tool_call_delta(\\n          vendor_part_id=dtc.index,\\n          tool_name=dtc.function and dtc.function.name,\\n          args=dtc.function and dtc.function.arguments,\\n          tool_call_id=dtc.id,\\n        )\\n        if maybe_event is not None:\\n          yield maybe_event\\n  @property\\n  defmodel_name(self) -> GroqModelName:\\n\"\"\"Get the model name of the response.\"\"\"\\n    return self._model_name\\n  @property\\n  deftimestamp(self) -> datetime:\\n\"\"\"Get the timestamp of the response.\"\"\"\\n    return self._timestamp'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 108, 'url': 'https://pydantic.com'}, page_content='model_name: GroqModelName'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 108, 'url': 'https://pydantic.com'}, page_content='timestamp: datetime'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 109, 'url': 'https://pydantic.com'}, page_content='LatestMistralModelNames = Literal[\\n  \"mistral-large-latest\",\\n  \"mistral-small-latest\",\\n  \"codestral-latest\",\\n  \"mistral-moderation-latest\",\\n]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 109, 'url': 'https://pydantic.com'}, page_content='MistralModelName = Unionstr[, LatestMistralModelNames]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 109, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 109, 'url': 'https://pydantic.com'}, page_content='classMistralModelSettings(ModelSettings):\\n\"\"\"Settings used for a Mistral model request.\"\"\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 110, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 110, 'url': 'https://pydantic.com'}, page_content='@dataclass(init=False)\\nclassMistralModel(Model):\\n\"\"\"A model that uses Mistral.\\n  Internally, this uses the Mistral Python client to interact with the API.\\n  API Documentation\\n  \"\"\"\\n  client: Mistral = field(repr=False)\\n  json_mode_schema_prompt: str = \"\"\"Answer in JSON Object, respect the format:\\\\n'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 117, 'url': 'https://pydantic.com'}, page_content='---|---  \\n####  __init__'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 117, 'url': 'https://pydantic.com'}, page_content=\"Initialize a Mistral model.\\nParameters:\\nName | Type | Description | Default  \\n---|---|---|---  \\n`model_name` |  `MistralModelName` |  The name of the model to use. |  _required_  \\n`api_key` |  `str | Callable[], str[ | None] | None` |  The API key to use for authentication, if unset uses `MISTRAL_API_KEY` environment variable. |  `None`  \\n`client` |  `Mistral | None` |  An existing `Mistral` client to use, if provided, `api_key` and `http_client` must be `None`. |  `None`  \\n`http_client` |  `AsyncClient | None` |  An existing `httpx.AsyncClient` to use for making HTTP requests. |  `None`  \\n`json_mode_schema_prompt` |  `str` |  The prompt to show when the model expects a JSON object as input. |  `'Answer in JSON Object, respect the format:\\\\n\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 117, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 117, 'url': 'https://pydantic.com'}, page_content='def__init__(\\n  self,\\n  model_name: MistralModelName,\\n  *,\\n  api_key: str | Callable[[], str | None] | None = None,\\n  client: Mistral | None = None,\\n  http_client: AsyncHTTPClient | None = None,\\n  json_mode_schema_prompt: str = \"\"\"Answer in JSON Object, respect the format:\\\\n'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 117, 'url': 'https://pydantic.com'}, page_content='---|---  \\n####  request `async`'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 117, 'url': 'https://pydantic.com'}, page_content='Make a non-streaming request to the model from Pydantic AI call.\\nSource code in `pydantic_ai_slim/pydantic_ai/models/mistral.py`'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 118, 'url': 'https://pydantic.com'}, page_content='|'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 118, 'url': 'https://pydantic.com'}, page_content='---|---  \\n####  request_stream `async`'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 118, 'url': 'https://pydantic.com'}, page_content='Make a streaming request to the model from Pydantic AI call.\\nSource code in `pydantic_ai_slim/pydantic_ai/models/mistral.py`'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 118, 'url': 'https://pydantic.com'}, page_content='|'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 118, 'url': 'https://pydantic.com'}, page_content='---|---  \\n####  model_name `property`'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 118, 'url': 'https://pydantic.com'}, page_content='The model name.\\n####  system `property`'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 118, 'url': 'https://pydantic.com'}, page_content='The system / model provider.\\n###  MistralStreamedResponse `dataclass`\\nBases: `StreamedResponse`\\nImplementation of `StreamedResponse` for Mistral models.\\nSource code in `pydantic_ai_slim/pydantic_ai/models/mistral.py`'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 119, 'url': 'https://pydantic.com'}, page_content='|'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 121, 'url': 'https://pydantic.com'}, page_content='---|---  \\n####  model_name `property`'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 121, 'url': 'https://pydantic.com'}, page_content='Get the model name of the response.\\n####  timestamp `property`'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 122, 'url': 'https://pydantic.com'}, page_content='OpenAIModelName = Unionstr[, ChatModel]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 122, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 122, 'url': 'https://pydantic.com'}, page_content='classOpenAIModelSettings(ModelSettings):\\n\"\"\"Settings used for an OpenAI model request.\"\"\"\\n  openai_reasoning_effort: chat.ChatCompletionReasoningEffort\\n\"\"\"\\n  Constrains effort on reasoning for reasoning models.\\n  Currently supported values are `low`, `medium`, and `high`. Reducing reasoning effort can\\n  result in faster responses and fewer tokens used on reasoning in a response.\\n  \"\"\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 122, 'url': 'https://pydantic.com'}, page_content='openai_reasoning_effort: ChatCompletionReasoningEffort'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 122, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 122, 'url': 'https://pydantic.com'}, page_content='@dataclass(init=False)\\nclassOpenAIModel(Model):\\n\"\"\"A model that uses the OpenAI API.\\n  Internally, this uses the OpenAI Python client to interact with the API.\\n  Apart from `__init__`, all methods are private or match those of the base class.\\n  \"\"\"\\n  client: AsyncOpenAI = field(repr=False)\\n  system_prompt_role: OpenAISystemPromptRole | None = field(default=None)\\n  _model_name: OpenAIModelName = field(repr=False)\\n  _system: str | None = field(repr=False)\\n  def__init__(\\n    self,\\n    model_name: OpenAIModelName,\\n    *,\\n    base_url: str | None = None,\\n    api_key: str | None = None,\\n    openai_client: AsyncOpenAI | None = None,\\n    http_client: AsyncHTTPClient | None = None,\\n    system_prompt_role: OpenAISystemPromptRole | None = None,\\n    system: str | None = \\'openai\\',\\n  ):\\n\"\"\"Initialize an OpenAI model.\\n    Args:\\n      model_name: The name of the OpenAI model to use. List of model names available\\n        here\\n        (Unfortunately, despite being ask to do so, OpenAI do not provide `.inv` files for their API).\\n      base_url: The base url for the OpenAI requests. If not provided, the `OPENAI_BASE_URL` environment variable\\n        will be used if available. Otherwise, defaults to OpenAI\\'s base url.\\n      api_key: The API key to use for authentication, if not provided, the `OPENAI_API_KEY` environment variable\\n        will be used if available.\\n      openai_client: An existing\\n        `AsyncOpenAI`\\n        client to use. If provided, `base_url`, `api_key`, and `http_client` must be `None`.\\n      http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.\\n      system_prompt_role: The role to use for the system prompt message. If not provided, defaults to `\\'system\\'`.\\n        In the future, this may be inferred from the model name.\\n      system: The model provider used, defaults to `openai`. This is for observability purposes, you must\\n        customize the `base_url` and `api_key` to use a different provider.\\n    \"\"\"\\n    self._model_name = model_name\\n    # This is a workaround for the OpenAI client requiring an API key, whilst locally served,\\n    # openai compatible models do not always need an API key, but a placeholder (non-empty) key is required.\\n    if api_key is None and \\'OPENAI_API_KEY\\' not in os.environ and base_url is not None and openai_client is None:\\n      api_key = \\'api-key-not-set\\'\\n    if openai_client is not None:\\n      assert http_client is None, \\'Cannot provide both `openai_client` and `http_client`\\'\\n      assert base_url is None, \\'Cannot provide both `openai_client` and `base_url`\\'\\n      assert api_key is None, \\'Cannot provide both `openai_client` and `api_key`\\'\\n      self.client = openai_client\\n    elif http_client is not None:\\n      self.client = AsyncOpenAI(base_url=base_url, api_key=api_key, http_client=http_client)\\n    else:\\n      self.client = AsyncOpenAI(base_url=base_url, api_key=api_key, http_client=cached_async_http_client())\\n    self.system_prompt_role = system_prompt_role\\n    self._system = system\\n  async defrequest(\\n    self,\\n    messages: list[ModelMessage],\\n    model_settings: ModelSettings | None,\\n    model_request_parameters: ModelRequestParameters,\\n  ) -> tuple[ModelResponse, usage.Usage]:\\n    check_allow_model_requests()\\n    response = await self._completions_create(\\n      messages, False, cast(OpenAIModelSettings, model_settings or {}), model_request_parameters\\n    )\\n    return self._process_response(response), _map_usage(response)\\n  @asynccontextmanager\\n  async defrequest_stream(\\n    self,\\n    messages: list[ModelMessage],\\n    model_settings: ModelSettings | None,\\n    model_request_parameters: ModelRequestParameters,\\n  ) -> AsyncIterator[StreamedResponse]:\\n    check_allow_model_requests()\\n    response = await self._completions_create(\\n      messages, True, cast(OpenAIModelSettings, model_settings or {}), model_request_parameters\\n    )\\n    async with response:\\n      yield await self._process_streamed_response(response)\\n  @property\\n  defmodel_name(self) -> OpenAIModelName:\\n\"\"\"The model name.\"\"\"\\n    return self._model_name\\n  @property\\n  defsystem(self) -> str | None:\\n\"\"\"The system / model provider.\"\"\"\\n    return self._system\\n  @overload\\n  async def_completions_create(\\n    self,\\n    messages: list[ModelMessage],\\n    stream: Literal[True],\\n    model_settings: OpenAIModelSettings,\\n    model_request_parameters: ModelRequestParameters,\\n  ) -> AsyncStream[ChatCompletionChunk]:\\n    pass\\n  @overload\\n  async def_completions_create(\\n    self,\\n    messages: list[ModelMessage],\\n    stream: Literal[False],\\n    model_settings: OpenAIModelSettings,\\n    model_request_parameters: ModelRequestParameters,\\n  ) -> chat.ChatCompletion:\\n    pass\\n  async def_completions_create(\\n    self,\\n    messages: list[ModelMessage],\\n    stream: bool,\\n    model_settings: OpenAIModelSettings,\\n    model_request_parameters: ModelRequestParameters,\\n  ) -> chat.ChatCompletion | AsyncStream[ChatCompletionChunk]:\\n    tools = self._get_tools(model_request_parameters)\\n    # standalone function to make it easier to override\\n    if not tools:\\n      tool_choice: Literal[\\'none\\', \\'required\\', \\'auto\\'] | None = None\\n    elif not model_request_parameters.allow_text_result:\\n      tool_choice = \\'required\\'\\n    else:\\n      tool_choice = \\'auto\\'\\n    openai_messages = list(chain(*(self._map_message(m) for m in messages)))\\n    return await self.client.chat.completions.create(\\n      model=self._model_name,\\n      messages=openai_messages,\\n      n=1,\\n      parallel_tool_calls=model_settings.get(\\'parallel_tool_calls\\', NOT_GIVEN),\\n      tools=tools or NOT_GIVEN,\\n      tool_choice=tool_choice or NOT_GIVEN,\\n      stream=stream,\\n      stream_options={\\'include_usage\\': True} if stream else NOT_GIVEN,\\n      max_tokens=model_settings.get(\\'max_tokens\\', NOT_GIVEN),\\n      temperature=model_settings.get(\\'temperature\\', NOT_GIVEN),\\n      top_p=model_settings.get(\\'top_p\\', NOT_GIVEN),\\n      timeout=model_settings.get(\\'timeout\\', NOT_GIVEN),\\n      seed=model_settings.get(\\'seed\\', NOT_GIVEN),\\n      presence_penalty=model_settings.get(\\'presence_penalty\\', NOT_GIVEN),\\n      frequency_penalty=model_settings.get(\\'frequency_penalty\\', NOT_GIVEN),\\n      logit_bias=model_settings.get(\\'logit_bias\\', NOT_GIVEN),\\n      reasoning_effort=model_settings.get(\\'openai_reasoning_effort\\', NOT_GIVEN),\\n    )\\n  def_process_response(self, response: chat.ChatCompletion) -> ModelResponse:\\n\"\"\"Process a non-streamed response, and prepare a message to return.\"\"\"\\n    timestamp = datetime.fromtimestamp(response.created, tz=timezone.utc)\\n    choice = response.choices[0]\\n    items: list[ModelResponsePart] = []\\n    if choice.message.content is not None:\\n      items.append(TextPart(choice.message.content))\\n    if choice.message.tool_calls is not None:\\n      for c in choice.message.tool_calls:\\n        items.append(ToolCallPart(c.function.name, c.function.arguments, c.id))\\n    return ModelResponse(items, model_name=response.model, timestamp=timestamp)\\n  async def_process_streamed_response(self, response: AsyncStream[ChatCompletionChunk]) -> OpenAIStreamedResponse:\\n\"\"\"Process a streamed response, and prepare a streaming response to return.\"\"\"\\n    peekable_response = _utils.PeekableAsyncStream(response)\\n    first_chunk = await peekable_response.peek()\\n    if isinstance(first_chunk, _utils.Unset):\\n      raise UnexpectedModelBehavior(\\'Streamed response ended without content or tool calls\\')\\n    return OpenAIStreamedResponse(\\n      _model_name=self._model_name,\\n      _response=peekable_response,\\n      _timestamp=datetime.fromtimestamp(first_chunk.created, tz=timezone.utc),\\n    )\\n  def_get_tools(self, model_request_parameters: ModelRequestParameters) -> list[chat.ChatCompletionToolParam]:\\n    tools = [self._map_tool_definition(r) for r in model_request_parameters.function_tools]\\n    if model_request_parameters.result_tools:\\n      tools += [self._map_tool_definition(r) for r in model_request_parameters.result_tools]\\n    return tools\\n  def_map_message(self, message: ModelMessage) -> Iterable[chat.ChatCompletionMessageParam]:\\n\"\"\"Just maps a `pydantic_ai.Message` to a `openai.types.ChatCompletionMessageParam`.\"\"\"\\n    if isinstance(message, ModelRequest):\\n      yield from self._map_user_message(message)\\n    elif isinstance(message, ModelResponse):\\n      texts: list[str] = []\\n      tool_calls: list[chat.ChatCompletionMessageToolCallParam] = []\\n      for item in message.parts:\\n        if isinstance(item, TextPart):\\n          texts.append(item.content)\\n        elif isinstance(item, ToolCallPart):\\n          tool_calls.append(self._map_tool_call(item))\\n        else:\\n          assert_never(item)\\n      message_param = chat.ChatCompletionAssistantMessageParam(role=\\'assistant\\')\\n      if texts:\\n        # Note: model responses from this model should only have one text item, so the following\\n        # shouldn\\'t merge multiple texts into one unless you switch models between runs:\\n        message_param[\\'content\\'] = \\'\\\\n\\\\n\\'.join(texts)\\n      if tool_calls:\\n        message_param[\\'tool_calls\\'] = tool_calls\\n      yield message_param\\n    else:\\n      assert_never(message)\\n  @staticmethod\\n  def_map_tool_call(t: ToolCallPart) -> chat.ChatCompletionMessageToolCallParam:\\n    return chat.ChatCompletionMessageToolCallParam(\\n      id=_guard_tool_call_id(t=t, model_source=\\'OpenAI\\'),\\n      type=\\'function\\',\\n      function={\\'name\\': t.tool_name, \\'arguments\\': t.args_as_json_str()},\\n    )\\n  @staticmethod\\n  def_map_tool_definition(f: ToolDefinition) -> chat.ChatCompletionToolParam:\\n    return {\\n      \\'type\\': \\'function\\',\\n      \\'function\\': {\\n        \\'name\\': f.name,\\n        \\'description\\': f.description,\\n        \\'parameters\\': f.parameters_json_schema,\\n      },\\n    }\\n  def_map_user_message(self, message: ModelRequest) -> Iterable[chat.ChatCompletionMessageParam]:\\n    for part in message.parts:\\n      if isinstance(part, SystemPromptPart):\\n        if self.system_prompt_role == \\'developer\\':\\n          yield chat.ChatCompletionDeveloperMessageParam(role=\\'developer\\', content=part.content)\\n        elif self.system_prompt_role == \\'user\\':\\n          yield chat.ChatCompletionUserMessageParam(role=\\'user\\', content=part.content)\\n        else:\\n          yield chat.ChatCompletionSystemMessageParam(role=\\'system\\', content=part.content)\\n      elif isinstance(part, UserPromptPart):\\n        yield chat.ChatCompletionUserMessageParam(role=\\'user\\', content=part.content)\\n      elif isinstance(part, ToolReturnPart):\\n        yield chat.ChatCompletionToolMessageParam(\\n          role=\\'tool\\',\\n          tool_call_id=_guard_tool_call_id(t=part, model_source=\\'OpenAI\\'),\\n          content=part.model_response_str(),\\n        )\\n      elif isinstance(part, RetryPromptPart):\\n        if part.tool_name is None:\\n          yield chat.ChatCompletionUserMessageParam(role=\\'user\\', content=part.model_response())\\n        else:\\n          yield chat.ChatCompletionToolMessageParam(\\n            role=\\'tool\\',\\n            tool_call_id=_guard_tool_call_id(t=part, model_source=\\'OpenAI\\'),\\n            content=part.model_response(),\\n          )\\n      else:\\n        assert_never(part)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 122, 'url': 'https://pydantic.com'}, page_content='__init__(\\n  model_name: OpenAIModelName,\\n  *,\\n  base_url: str | None = None,\\n  api_key: str | None = None,\\n  openai_client: AsyncOpenAI | None = None,\\n  http_client: AsyncClient | None = None,\\n  system_prompt_role: (\\n    OpenAISystemPromptRole | None\\n  ) = None,\\n  system: str | None = \"openai\"\\n)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 123, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 123, 'url': 'https://pydantic.com'}, page_content='def__init__(\\n  self,\\n  model_name: OpenAIModelName,\\n  *,\\n  base_url: str | None = None,\\n  api_key: str | None = None,\\n  openai_client: AsyncOpenAI | None = None,\\n  http_client: AsyncHTTPClient | None = None,\\n  system_prompt_role: OpenAISystemPromptRole | None = None,\\n  system: str | None = \\'openai\\',\\n):\\n\"\"\"Initialize an OpenAI model.\\n  Args:\\n    model_name: The name of the OpenAI model to use. List of model names available\\n      here\\n      (Unfortunately, despite being ask to do so, OpenAI do not provide `.inv` files for their API).\\n    base_url: The base url for the OpenAI requests. If not provided, the `OPENAI_BASE_URL` environment variable\\n      will be used if available. Otherwise, defaults to OpenAI\\'s base url.\\n    api_key: The API key to use for authentication, if not provided, the `OPENAI_API_KEY` environment variable\\n      will be used if available.\\n    openai_client: An existing\\n      `AsyncOpenAI`\\n      client to use. If provided, `base_url`, `api_key`, and `http_client` must be `None`.\\n    http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.\\n    system_prompt_role: The role to use for the system prompt message. If not provided, defaults to `\\'system\\'`.\\n      In the future, this may be inferred from the model name.\\n    system: The model provider used, defaults to `openai`. This is for observability purposes, you must\\n      customize the `base_url` and `api_key` to use a different provider.\\n  \"\"\"\\n  self._model_name = model_name\\n  # This is a workaround for the OpenAI client requiring an API key, whilst locally served,\\n  # openai compatible models do not always need an API key, but a placeholder (non-empty) key is required.\\n  if api_key is None and \\'OPENAI_API_KEY\\' not in os.environ and base_url is not None and openai_client is None:\\n    api_key = \\'api-key-not-set\\'\\n  if openai_client is not None:\\n    assert http_client is None, \\'Cannot provide both `openai_client` and `http_client`\\'\\n    assert base_url is None, \\'Cannot provide both `openai_client` and `base_url`\\'\\n    assert api_key is None, \\'Cannot provide both `openai_client` and `api_key`\\'\\n    self.client = openai_client\\n  elif http_client is not None:\\n    self.client = AsyncOpenAI(base_url=base_url, api_key=api_key, http_client=http_client)\\n  else:\\n    self.client = AsyncOpenAI(base_url=base_url, api_key=api_key, http_client=cached_async_http_client())\\n  self.system_prompt_role = system_prompt_role\\n  self._system = system'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 123, 'url': 'https://pydantic.com'}, page_content='model_name: OpenAIModelName'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 123, 'url': 'https://pydantic.com'}, page_content='system: str | None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 123, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 123, 'url': 'https://pydantic.com'}, page_content='@dataclass\\nclassOpenAIStreamedResponse(StreamedResponse):\\n\"\"\"Implementation of `StreamedResponse` for OpenAI models.\"\"\"\\n  _model_name: OpenAIModelName\\n  _response: AsyncIterable[ChatCompletionChunk]\\n  _timestamp: datetime\\n  async def_get_event_iterator(self) -> AsyncIterator[ModelResponseStreamEvent]:\\n    async for chunk in self._response:\\n      self._usage += _map_usage(chunk)\\n      try:\\n        choice = chunk.choices[0]\\n      except IndexError:\\n        continue\\n      # Handle the text part of the response\\n      content = choice.delta.content\\n      if content is not None:\\n        yield self._parts_manager.handle_text_delta(vendor_part_id=\\'content\\', content=content)\\n      for dtc in choice.delta.tool_calls or []:\\n        maybe_event = self._parts_manager.handle_tool_call_delta(\\n          vendor_part_id=dtc.index,\\n          tool_name=dtc.function and dtc.function.name,\\n          args=dtc.function and dtc.function.arguments,\\n          tool_call_id=dtc.id,\\n        )\\n        if maybe_event is not None:\\n          yield maybe_event\\n  @property\\n  defmodel_name(self) -> OpenAIModelName:\\n\"\"\"Get the model name of the response.\"\"\"\\n    return self._model_name\\n  @property\\n  deftimestamp(self) -> datetime:\\n\"\"\"Get the timestamp of the response.\"\"\"\\n    return self._timestamp'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 123, 'url': 'https://pydantic.com'}, page_content='model_name: OpenAIModelName'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 123, 'url': 'https://pydantic.com'}, page_content='timestamp: datetime'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 125, 'url': 'https://pydantic.com'}, page_content='frompydantic_aiimport Agent\\nfrompydantic_ai.models.testimport TestModel\\nmy_agent = Agent(\\'openai:gpt-4o\\', system_prompt=\\'...\\')\\n\\nasync deftest_my_agent():\\n\"\"\"Unit test for my_agent, to be run by pytest.\"\"\"\\n  m = TestModel()\\n  with my_agent.override(model=m):\\n    result = await my_agent.run(\\'Testing my agent...\\')\\n    assert result.data == \\'success (no tool calls)\\'\\n  assert m.last_model_request_parameters.function_tools == []'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 125, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 125, 'url': 'https://pydantic.com'}, page_content='@dataclass\\nclassTestModel(Model):\\n\"\"\"A model specifically for testing purposes.\\n  This will (by default) call all tools in the agent, then return a tool response if possible,\\n  otherwise a plain response.\\n  How useful this model is will vary significantly.\\n  Apart from `__init__` derived by the `dataclass` decorator, all methods are private or match those\\n  of the base class.\\n  \"\"\"\\n  # NOTE: Avoid test discovery by pytest.\\n  __test__ = False\\n  call_tools: list[str] | Literal[\\'all\\'] = \\'all\\'\\n\"\"\"List of tools to call. If `\\'all\\'`, all tools will be called.\"\"\"\\n  custom_result_text: str | None = None\\n\"\"\"If set, this text is returned as the final result.\"\"\"\\n  custom_result_args: Any | None = None\\n\"\"\"If set, these args will be passed to the result tool.\"\"\"\\n  seed: int = 0\\n\"\"\"Seed for generating random data.\"\"\"\\n  last_model_request_parameters: ModelRequestParameters | None = field(default=None, init=False)\\n\"\"\"The last ModelRequestParameters passed to the model in a request.\\n  The ModelRequestParameters contains information about the function and result tools available during request handling.\\n  This is set when a request is made, so will reflect the function tools from the last step of the last run.\\n  \"\"\"\\n  _model_name: str = field(default=\\'test\\', repr=False)\\n  _system: str | None = field(default=None, repr=False)\\n  async defrequest(\\n    self,\\n    messages: list[ModelMessage],\\n    model_settings: ModelSettings | None,\\n    model_request_parameters: ModelRequestParameters,\\n  ) -> tuple[ModelResponse, Usage]:\\n    self.last_model_request_parameters = model_request_parameters\\n    model_response = self._request(messages, model_settings, model_request_parameters)\\n    usage = _estimate_usage([*messages, model_response])\\n    return model_response, usage\\n  @asynccontextmanager\\n  async defrequest_stream(\\n    self,\\n    messages: list[ModelMessage],\\n    model_settings: ModelSettings | None,\\n    model_request_parameters: ModelRequestParameters,\\n  ) -> AsyncIterator[StreamedResponse]:\\n    self.last_model_request_parameters = model_request_parameters\\n    model_response = self._request(messages, model_settings, model_request_parameters)\\n    yield TestStreamedResponse(\\n      _model_name=self._model_name, _structured_response=model_response, _messages=messages\\n    )\\n  @property\\n  defmodel_name(self) -> str:\\n\"\"\"The model name.\"\"\"\\n    return self._model_name\\n  @property\\n  defsystem(self) -> str | None:\\n\"\"\"The system / model provider.\"\"\"\\n    return self._system\\n  defgen_tool_args(self, tool_def: ToolDefinition) -> Any:\\n    return _JsonSchemaTestData(tool_def.parameters_json_schema, self.seed).generate()\\n  def_get_tool_calls(self, model_request_parameters: ModelRequestParameters) -> list[tuple[str, ToolDefinition]]:\\n    if self.call_tools == \\'all\\':\\n      return [(r.name, r) for r in model_request_parameters.function_tools]\\n    else:\\n      function_tools_lookup = {t.name: t for t in model_request_parameters.function_tools}\\n      tools_to_call = (function_tools_lookup[name] for name in self.call_tools)\\n      return [(r.name, r) for r in tools_to_call]\\n  def_get_result(self, model_request_parameters: ModelRequestParameters) -> _TextResult | _FunctionToolResult:\\n    if self.custom_result_text is not None:\\n      assert model_request_parameters.allow_text_result, (\\n        \\'Plain response not allowed, but `custom_result_text` is set.\\'\\n      )\\n      assert self.custom_result_args is None, \\'Cannot set both `custom_result_text` and `custom_result_args`.\\'\\n      return _TextResult(self.custom_result_text)\\n    elif self.custom_result_args is not None:\\n      assert model_request_parameters.result_tools is not None, (\\n        \\'No result tools provided, but `custom_result_args` is set.\\'\\n      )\\n      result_tool = model_request_parameters.result_tools[0]\\n      if k := result_tool.outer_typed_dict_key:\\n        return _FunctionToolResult({k: self.custom_result_args})\\n      else:\\n        return _FunctionToolResult(self.custom_result_args)\\n    elif model_request_parameters.allow_text_result:\\n      return _TextResult(None)\\n    elif model_request_parameters.result_tools:\\n      return _FunctionToolResult(None)\\n    else:\\n      return _TextResult(None)\\n  def_request(\\n    self,\\n    messages: list[ModelMessage],\\n    model_settings: ModelSettings | None,\\n    model_request_parameters: ModelRequestParameters,\\n  ) -> ModelResponse:\\n    tool_calls = self._get_tool_calls(model_request_parameters)\\n    result = self._get_result(model_request_parameters)\\n    result_tools = model_request_parameters.result_tools\\n    # if there are tools, the first thing we want to do is call all of them\\n    if tool_calls and not any(isinstance(m, ModelResponse) for m in messages):\\n      return ModelResponse(\\n        parts=[ToolCallPart(name, self.gen_tool_args(args)) for name, args in tool_calls],\\n        model_name=self._model_name,\\n      )\\n    if messages:\\n      last_message = messages[-1]\\n      assert isinstance(last_message, ModelRequest), \\'Expected last message to be a `ModelRequest`.\\'\\n      # check if there are any retry prompts, if so retry them\\n      new_retry_names = {p.tool_name for p in last_message.parts if isinstance(p, RetryPromptPart)}\\n      if new_retry_names:\\n        # Handle retries for both function tools and result tools\\n        # Check function tools first\\n        retry_parts: list[ModelResponsePart] = [\\n          ToolCallPart(name, self.gen_tool_args(args)) for name, args in tool_calls if name in new_retry_names\\n        ]\\n        # Check result tools\\n        if result_tools:\\n          retry_parts.extend(\\n            [\\n              ToolCallPart(\\n                tool.name,\\n                result.value\\n                if isinstance(result, _FunctionToolResult) and result.value is not None\\n                else self.gen_tool_args(tool),\\n              )\\n              for tool in result_tools\\n              if tool.name in new_retry_names\\n            ]\\n          )\\n        return ModelResponse(parts=retry_parts, model_name=self._model_name)\\n    if isinstance(result, _TextResult):\\n      if (response_text := result.value) is None:\\n        # build up details of tool responses\\n        output: dict[str, Any] = {}\\n        for message in messages:\\n          if isinstance(message, ModelRequest):\\n            for part in message.parts:\\n              if isinstance(part, ToolReturnPart):\\n                output[part.tool_name] = part.content\\n        if output:\\n          return ModelResponse(\\n            parts=[TextPart(pydantic_core.to_json(output).decode())], model_name=self._model_name\\n          )\\n        else:\\n          return ModelResponse(parts=[TextPart(\\'success (no tool calls)\\')], model_name=self._model_name)\\n      else:\\n        return ModelResponse(parts=[TextPart(response_text)], model_name=self._model_name)\\n    else:\\n      assert result_tools, \\'No result tools provided\\'\\n      custom_result_args = result.value\\n      result_tool = result_tools[self.seed % len(result_tools)]\\n      if custom_result_args is not None:\\n        return ModelResponse(\\n          parts=[ToolCallPart(result_tool.name, custom_result_args)], model_name=self._model_name\\n        )\\n      else:\\n        response_args = self.gen_tool_args(result_tool)\\n        return ModelResponse(parts=[ToolCallPart(result_tool.name, response_args)], model_name=self._model_name)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 125, 'url': 'https://pydantic.com'}, page_content=\"call_tools: liststr[] | Literal['all'] = 'all'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 125, 'url': 'https://pydantic.com'}, page_content='custom_result_text: str | None = None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 125, 'url': 'https://pydantic.com'}, page_content='custom_result_args: Any | None = None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 125, 'url': 'https://pydantic.com'}, page_content='seed: int = 0'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 125, 'url': 'https://pydantic.com'}, page_content='last_model_request_parameters: (\\n  ModelRequestParameters | None\\n) = field(default=None, init=False)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 126, 'url': 'https://pydantic.com'}, page_content='model_name: str'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 126, 'url': 'https://pydantic.com'}, page_content='system: str | None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 126, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 126, 'url': 'https://pydantic.com'}, page_content='@dataclass\\nclassTestStreamedResponse(StreamedResponse):\\n\"\"\"A structured response that streams test data.\"\"\"\\n  _model_name: str\\n  _structured_response: ModelResponse\\n  _messages: InitVar[Iterable[ModelMessage]]\\n  _timestamp: datetime = field(default_factory=_utils.now_utc, init=False)\\n  def__post_init__(self, _messages: Iterable[ModelMessage]):\\n    self._usage = _estimate_usage(_messages)\\n  async def_get_event_iterator(self) -> AsyncIterator[ModelResponseStreamEvent]:\\n    for i, part in enumerate(self._structured_response.parts):\\n      if isinstance(part, TextPart):\\n        text = part.content\\n        *words, last_word = text.split(\\' \\')\\n        words = [f\\'{word} \\' for word in words]\\n        words.append(last_word)\\n        if len(words) == 1 and len(text) > 2:\\n          mid = len(text) // 2\\n          words = [text[:mid], text[mid:]]\\n        self._usage += _get_string_usage(\\'\\')\\n        yield self._parts_manager.handle_text_delta(vendor_part_id=i, content=\\'\\')\\n        for word in words:\\n          self._usage += _get_string_usage(word)\\n          yield self._parts_manager.handle_text_delta(vendor_part_id=i, content=word)\\n      else:\\n        yield self._parts_manager.handle_tool_call_part(\\n          vendor_part_id=i, tool_name=part.tool_name, args=part.args, tool_call_id=part.tool_call_id\\n        )\\n  @property\\n  defmodel_name(self) -> str:\\n\"\"\"Get the model name of the response.\"\"\"\\n    return self._model_name\\n  @property\\n  deftimestamp(self) -> datetime:\\n\"\"\"Get the timestamp of the response.\"\"\"\\n    return self._timestamp'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 126, 'url': 'https://pydantic.com'}, page_content='model_name: str'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 126, 'url': 'https://pydantic.com'}, page_content='timestamp: datetime'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 127, 'url': 'https://pydantic.com'}, page_content=\"frompydantic_aiimport Agent\\nfrompydantic_ai.models.vertexaiimport VertexAIModel\\nmodel = VertexAIModel('gemini-1.5-flash')\\nagent = Agent(model)\\nresult = agent.run_sync('Tell me a joke.')\\nprint(result.data)\\n#> Did you hear about the toothpaste scandal? They called it Colgate.\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 127, 'url': 'https://pydantic.com'}, page_content=\"frompydantic_aiimport Agent\\nfrompydantic_ai.models.vertexaiimport VertexAIModel\\nmodel = VertexAIModel(\\n  'gemini-1.5-flash',\\n  service_account_file='path/to/service-account.json',\\n)\\nagent = Agent(model)\\nresult = agent.run_sync('Tell me a joke.')\\nprint(result.data)\\n#> Did you hear about the toothpaste scandal? They called it Colgate.\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 127, 'url': 'https://pydantic.com'}, page_content='VERTEX_AI_URL_TEMPLATE = \"https://{region}-aiplatform.googleapis.com/v1/projects/{project_id}/locations/{region}/publishers/{model_publisher}/models/{model}:\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 128, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 128, 'url': 'https://pydantic.com'}, page_content='@dataclass(init=False)\\nclassVertexAIModel(GeminiModel):\\n\"\"\"A model that uses Gemini via the `*-aiplatform.googleapis.com` VertexAI API.\"\"\"\\n  service_account_file: Path | str | None\\n  project_id: str | None\\n  region: VertexAiRegion\\n  model_publisher: Literal[\\'google\\']\\n  url_template: str\\n  _model_name: GeminiModelName = field(repr=False)\\n  _system: str | None = field(default=\\'google-vertex\\', repr=False)\\n  # TODO __init__ can be removed once we drop 3.9 and we can set kw_only correctly on the dataclass\\n  def__init__(\\n    self,\\n    model_name: GeminiModelName,\\n    *,\\n    service_account_file: Path | str | None = None,\\n    project_id: str | None = None,\\n    region: VertexAiRegion = \\'us-central1\\',\\n    model_publisher: Literal[\\'google\\'] = \\'google\\',\\n    http_client: AsyncHTTPClient | None = None,\\n    url_template: str = VERTEX_AI_URL_TEMPLATE,\\n  ):\\n\"\"\"Initialize a Vertex AI Gemini model.\\n    Args:\\n      model_name: The name of the model to use. I couldn\\'t find a list of supported Google models, in VertexAI\\n        so for now this uses the same models as the [Gemini model][pydantic_ai.models.gemini.GeminiModel].\\n      service_account_file: Path to a service account file.\\n        If not provided, the default environment credentials will be used.\\n      project_id: The project ID to use, if not provided it will be taken from the credentials.\\n      region: The region to make requests to.\\n      model_publisher: The model publisher to use, I couldn\\'t find a good list of available publishers,\\n        and from trial and error it seems non-google models don\\'t work with the `generateContent` and\\n        `streamGenerateContent` functions, hence only `google` is currently supported.\\n        Please create an issue or PR if you know how to use other publishers.\\n      http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.\\n      url_template: URL template for Vertex AI, see\\n        [`VERTEX_AI_URL_TEMPLATE` docs][pydantic_ai.models.vertexai.VERTEX_AI_URL_TEMPLATE]\\n        for more information.\\n    \"\"\"\\n    self._model_name = model_name\\n    self.service_account_file = service_account_file\\n    self.project_id = project_id\\n    self.region = region\\n    self.model_publisher = model_publisher\\n    self.http_client = http_client or cached_async_http_client()\\n    self.url_template = url_template\\n    self._auth = None\\n    self._url = None\\n  async defainit(self) -> None:\\n\"\"\"Initialize the model, setting the URL and auth.\\n    This will raise an error if authentication fails.\\n    \"\"\"\\n    if self._url is not None and self._auth is not None:\\n      return\\n    if self.service_account_file is not None:\\n      creds: BaseCredentials | ServiceAccountCredentials = _creds_from_file(self.service_account_file)\\n      assert creds.project_id is None or isinstance(creds.project_id, str)\\n      creds_project_id: str | None = creds.project_id\\n      creds_source = \\'service account file\\'\\n    else:\\n      creds, creds_project_id = await _async_google_auth()\\n      creds_source = \\'`google.auth.default()`\\'\\n    if self.project_id is None:\\n      if creds_project_id is None:\\n        raise UserError(f\\'No project_id provided and none found in {creds_source}\\')\\n      project_id = creds_project_id\\n    else:\\n      project_id = self.project_id\\n    self._url = self.url_template.format(\\n      region=self.region,\\n      project_id=project_id,\\n      model_publisher=self.model_publisher,\\n      model=self._model_name,\\n    )\\n    self._auth = BearerTokenAuth(creds)\\n  async defrequest(\\n    self,\\n    messages: list[ModelMessage],\\n    model_settings: ModelSettings | None,\\n    model_request_parameters: ModelRequestParameters,\\n  ) -> tuple[ModelResponse, usage.Usage]:\\n    await self.ainit()\\n    return await super().request(messages, model_settings, model_request_parameters)\\n  @asynccontextmanager\\n  async defrequest_stream(\\n    self,\\n    messages: list[ModelMessage],\\n    model_settings: ModelSettings | None,\\n    model_request_parameters: ModelRequestParameters,\\n  ) -> AsyncIterator[StreamedResponse]:\\n    await self.ainit()\\n    async with super().request_stream(messages, model_settings, model_request_parameters) as value:\\n      yield value\\n  @property\\n  defmodel_name(self) -> GeminiModelName:\\n\"\"\"The model name.\"\"\"\\n    return self._model_name\\n  @property\\n  defsystem(self) -> str | None:\\n\"\"\"The system / model provider.\"\"\"\\n    return self._system'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 128, 'url': 'https://pydantic.com'}, page_content='__init__(\\n  model_name: GeminiModelName,\\n  *,\\n  service_account_file: Path | str | None = None,\\n  project_id: str | None = None,\\n  region: VertexAiRegion = \"us-central1\",\\n  model_publisher: Literal[\"google\"] = \"google\",\\n  http_client: AsyncClient | None = None,\\n  url_template: str = VERTEX_AI_URL_TEMPLATE\\n)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 128, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 128, 'url': 'https://pydantic.com'}, page_content='def__init__(\\n  self,\\n  model_name: GeminiModelName,\\n  *,\\n  service_account_file: Path | str | None = None,\\n  project_id: str | None = None,\\n  region: VertexAiRegion = \\'us-central1\\',\\n  model_publisher: Literal[\\'google\\'] = \\'google\\',\\n  http_client: AsyncHTTPClient | None = None,\\n  url_template: str = VERTEX_AI_URL_TEMPLATE,\\n):\\n\"\"\"Initialize a Vertex AI Gemini model.\\n  Args:\\n    model_name: The name of the model to use. I couldn\\'t find a list of supported Google models, in VertexAI\\n      so for now this uses the same models as the [Gemini model][pydantic_ai.models.gemini.GeminiModel].\\n    service_account_file: Path to a service account file.\\n      If not provided, the default environment credentials will be used.\\n    project_id: The project ID to use, if not provided it will be taken from the credentials.\\n    region: The region to make requests to.\\n    model_publisher: The model publisher to use, I couldn\\'t find a good list of available publishers,\\n      and from trial and error it seems non-google models don\\'t work with the `generateContent` and\\n      `streamGenerateContent` functions, hence only `google` is currently supported.\\n      Please create an issue or PR if you know how to use other publishers.\\n    http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.\\n    url_template: URL template for Vertex AI, see\\n      [`VERTEX_AI_URL_TEMPLATE` docs][pydantic_ai.models.vertexai.VERTEX_AI_URL_TEMPLATE]\\n      for more information.\\n  \"\"\"\\n  self._model_name = model_name\\n  self.service_account_file = service_account_file\\n  self.project_id = project_id\\n  self.region = region\\n  self.model_publisher = model_publisher\\n  self.http_client = http_client or cached_async_http_client()\\n  self.url_template = url_template\\n  self._auth = None\\n  self._url = None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 128, 'url': 'https://pydantic.com'}, page_content='ainit() -> None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 128, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 128, 'url': 'https://pydantic.com'}, page_content='async defainit(self) -> None:\\n\"\"\"Initialize the model, setting the URL and auth.\\n  This will raise an error if authentication fails.\\n  \"\"\"\\n  if self._url is not None and self._auth is not None:\\n    return\\n  if self.service_account_file is not None:\\n    creds: BaseCredentials | ServiceAccountCredentials = _creds_from_file(self.service_account_file)\\n    assert creds.project_id is None or isinstance(creds.project_id, str)\\n    creds_project_id: str | None = creds.project_id\\n    creds_source = \\'service account file\\'\\n  else:\\n    creds, creds_project_id = await _async_google_auth()\\n    creds_source = \\'`google.auth.default()`\\'\\n  if self.project_id is None:\\n    if creds_project_id is None:\\n      raise UserError(f\\'No project_id provided and none found in {creds_source}\\')\\n    project_id = creds_project_id\\n  else:\\n    project_id = self.project_id\\n  self._url = self.url_template.format(\\n    region=self.region,\\n    project_id=project_id,\\n    model_publisher=self.model_publisher,\\n    model=self._model_name,\\n  )\\n  self._auth = BearerTokenAuth(creds)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 128, 'url': 'https://pydantic.com'}, page_content='model_name: GeminiModelName'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 129, 'url': 'https://pydantic.com'}, page_content='system: str | None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 129, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 129, 'url': 'https://pydantic.com'}, page_content='@dataclass\\nclassBearerTokenAuth:\\n\"\"\"Authentication using a bearer token generated by google-auth.\"\"\"\\n  credentials: BaseCredentials | ServiceAccountCredentials\\n  token_created: datetime | None = field(default=None, init=False)\\n  async defheaders(self) -> dict[str, str]:\\n    if self.credentials.token is None or self._token_expired():\\n      await run_in_executor(self._refresh_token)\\n      self.token_created = datetime.now()\\n    return {\\'Authorization\\': f\\'Bearer {self.credentials.token}\\'}\\n  def_token_expired(self) -> bool:\\n    if self.token_created is None:\\n      return True\\n    else:\\n      return (datetime.now() - self.token_created) > MAX_TOKEN_AGE\\n  def_refresh_token(self) -> str:\\n    self.credentials.refresh(Request())\\n    assert isinstance(self.credentials.token, str), f\\'Expected token to be a string, got {self.credentials.token}\\'\\n    return self.credentials.token'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 129, 'url': 'https://pydantic.com'}, page_content='VertexAiRegion = Literal[\\n  \"us-central1\",\\n  \"us-east1\",\\n  \"us-east4\",\\n  \"us-south1\",\\n  \"us-west1\",\\n  \"us-west2\",\\n  \"us-west3\",\\n  \"us-west4\",\\n  \"us-east5\",\\n  \"europe-central2\",\\n  \"europe-north1\",\\n  \"europe-southwest1\",\\n  \"europe-west1\",\\n  \"europe-west2\",\\n  \"europe-west3\",\\n  \"europe-west4\",\\n  \"europe-west6\",\\n  \"europe-west8\",\\n  \"europe-west9\",\\n  \"europe-west12\",\\n  \"africa-south1\",\\n  \"asia-east1\",\\n  \"asia-east2\",\\n  \"asia-northeast1\",\\n  \"asia-northeast2\",\\n  \"asia-northeast3\",\\n  \"asia-south1\",\\n  \"asia-southeast1\",\\n  \"asia-southeast2\",\\n  \"australia-southeast1\",\\n  \"australia-southeast2\",\\n  \"me-central1\",\\n  \"me-central2\",\\n  \"me-west1\",\\n  \"northamerica-northeast1\",\\n  \"northamerica-northeast2\",\\n  \"southamerica-east1\",\\n  \"southamerica-west1\",\\n]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 130, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 130, 'url': 'https://pydantic.com'}, page_content='classGraphSetupError(TypeError):\\n\"\"\"Error caused by an incorrectly configured graph.\"\"\"\\n  message: str\\n\"\"\"Description of the mistake.\"\"\"\\n  def__init__(self, message: str):\\n    self.message = message\\n    super().__init__(message)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 130, 'url': 'https://pydantic.com'}, page_content='message: str = message'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 130, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 130, 'url': 'https://pydantic.com'}, page_content='classGraphRuntimeError(RuntimeError):\\n\"\"\"Error caused by an issue during graph execution.\"\"\"\\n  message: str\\n\"\"\"The error message.\"\"\"\\n  def__init__(self, message: str):\\n    self.message = message\\n    super().__init__(message)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 130, 'url': 'https://pydantic.com'}, page_content='message: str = message'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 131, 'url': 'https://pydantic.com'}, page_content='from__future__import annotations\\nfromdataclassesimport dataclass\\nfrompydantic_graphimport BaseNode, End, Graph, GraphRunContext\\n@dataclass\\nclassMyState:\\n  number: int\\n@dataclass\\nclassIncrement(BaseNode[MyState]):\\n  async defrun(self, ctx: GraphRunContext) -> Check42:\\n    ctx.state.number += 1\\n    return Check42()\\n@dataclass\\nclassCheck42(BaseNode[MyState, None, int]):\\n  async defrun(self, ctx: GraphRunContext) -> Increment | End[int]:\\n    if ctx.state.number == 42:\\n      return Increment()\\n    else:\\n      return End(ctx.state.number)\\nnever_42_graph = Graph(nodes=(Increment, Check42))'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 132, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 132, 'url': 'https://pydantic.com'}, page_content='@dataclass(init=False)\\nclassGraph(Generic[StateT, DepsT, RunEndT]):\\n\"\"\"Definition of a graph.\\n  In `pydantic-graph`, a graph is a collection of nodes that can be run in sequence. The nodes define\\n  their outgoing edges — e.g. which nodes may be run next, and thereby the structure of the graph.\\n  Here\\'s a very simple example of a graph which increments a number by 1, but makes sure the number is never\\n  42 at the end.'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 132, 'url': 'https://pydantic.com'}, page_content='_(This example is complete, it can be run \"as is\")_\\n  See [`run`][pydantic_graph.graph.Graph.run] For an example of running graph, and\\n  [`mermaid_code`][pydantic_graph.graph.Graph.mermaid_code] for an example of generating a mermaid diagram\\n  from the graph.\\n  \"\"\"\\n  name: str | None\\n  node_defs: dict[str, NodeDef[StateT, DepsT, RunEndT]]\\n  snapshot_state: Callable[[StateT], StateT]\\n  _state_type: type[StateT] | _utils.Unset = field(repr=False)\\n  _run_end_type: type[RunEndT] | _utils.Unset = field(repr=False)\\n  _auto_instrument: bool = field(repr=False)\\n  def__init__(\\n    self,\\n    *,\\n    nodes: Sequence[type[BaseNode[StateT, DepsT, RunEndT]]],\\n    name: str | None = None,\\n    state_type: type[StateT] | _utils.Unset = _utils.UNSET,\\n    run_end_type: type[RunEndT] | _utils.Unset = _utils.UNSET,\\n    snapshot_state: Callable[[StateT], StateT] = deep_copy_state,\\n    auto_instrument: bool = True,\\n  ):\\n\"\"\"Create a graph from a sequence of nodes.\\n    Args:\\n      nodes: The nodes which make up the graph, nodes need to be unique and all be generic in the same\\n        state type.\\n      name: Optional name for the graph, if not provided the name will be inferred from the calling frame\\n        on the first call to a graph method.\\n      state_type: The type of the state for the graph, this can generally be inferred from `nodes`.\\n      run_end_type: The type of the result of running the graph, this can generally be inferred from `nodes`.\\n      snapshot_state: A function to snapshot the state of the graph, this is used in\\n        [`NodeStep`][pydantic_graph.state.NodeStep] and [`EndStep`][pydantic_graph.state.EndStep] to record\\n        the state before each step.\\n      auto_instrument: Whether to create a span for the graph run and the execution of each node\\'s run method.\\n    \"\"\"\\n    self.name = name\\n    self._state_type = state_type\\n    self._run_end_type = run_end_type\\n    self._auto_instrument = auto_instrument\\n    self.snapshot_state = snapshot_state\\n    parent_namespace = _utils.get_parent_namespace(inspect.currentframe())\\n    self.node_defs: dict[str, NodeDef[StateT, DepsT, RunEndT]] = {}\\n    for node in nodes:\\n      self._register_node(node, parent_namespace)\\n    self._validate_edges()\\n  async defrun(\\n    self: Graph[StateT, DepsT, T],\\n    start_node: BaseNode[StateT, DepsT, T],\\n    *,\\n    state: StateT = None,\\n    deps: DepsT = None,\\n    infer_name: bool = True,\\n    span: LogfireSpan | None = None,\\n  ) -> GraphRunResult[StateT, T]:\\n\"\"\"Run the graph from a starting node until it ends.\\n    Args:\\n      start_node: the first node to run, since the graph definition doesn\\'t define the entry point in the graph,\\n        you need to provide the starting node.\\n      state: The initial state of the graph.\\n      deps: The dependencies of the graph.\\n      infer_name: Whether to infer the graph name from the calling frame.\\n      span: The span to use for the graph run. If not provided, a span will be created depending on the value of\\n        the `_auto_instrument` field.\\n    Returns:\\n      A `GraphRunResult` containing information about the run, including its final result.\\n    Here\\'s an example of running the graph from [above][pydantic_graph.graph.Graph]:'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 132, 'url': 'https://pydantic.com'}, page_content='\"\"\"\\n    if infer_name and self.name is None:\\n      self._infer_name(inspect.currentframe())\\n    with self.iter(start_node, state=state, deps=deps, infer_name=infer_name, span=span) as graph_run:\\n      async for _node in graph_run:\\n        pass\\n    final_result = graph_run.result\\n    assert final_result is not None, \\'GraphRun should have a final result\\'\\n    return final_result\\n  @contextmanager\\n  defiter(\\n    self: Graph[StateT, DepsT, T],\\n    start_node: BaseNode[StateT, DepsT, T],\\n    *,\\n    state: StateT = None,\\n    deps: DepsT = None,\\n    infer_name: bool = True,\\n    span: LogfireSpan | None = None,\\n  ) -> Iterator[GraphRun[StateT, DepsT, T]]:\\n\"\"\"A contextmanager which can be used to iterate over the graph\\'s nodes as they are executed.\\n    This method returns a `GraphRun` object which can be used to async-iterate over the nodes of this `Graph` as\\n    they are executed. This is the API to use if you want to record or interact with the nodes as the graph\\n    execution unfolds.\\n    The `GraphRun` can also be used to manually drive the graph execution by calling\\n    [`GraphRun.next`][pydantic_graph.graph.GraphRun.next].\\n    The `GraphRun` provides access to the full run history, state, deps, and the final result of the run once\\n    it has completed.\\n    For more details, see the API documentation of [`GraphRun`][pydantic_graph.graph.GraphRun].\\n    Args:\\n      start_node: the first node to run. Since the graph definition doesn\\'t define the entry point in the graph,\\n        you need to provide the starting node.\\n      state: The initial state of the graph.\\n      deps: The dependencies of the graph.\\n      infer_name: Whether to infer the graph name from the calling frame.\\n      span: The span to use for the graph run. If not provided, a new span will be created.\\n    Yields:\\n      A GraphRun that can be async iterated over to drive the graph to completion.\\n    \"\"\"\\n    if infer_name and self.name is None:\\n      self._infer_name(inspect.currentframe())\\n    if self._auto_instrument and span is None:\\n      span = logfire_api.span(\\'run graph {graph.name}\\', graph=self)\\n    with ExitStack() as stack:\\n      if span is not None:\\n        stack.enter_context(span)\\n      yield GraphRun[StateT, DepsT, T](\\n        self,\\n        start_node,\\n        history=[],\\n        state=state,\\n        deps=deps,\\n        auto_instrument=self._auto_instrument,\\n        span=span,\\n      )\\n  defrun_sync(\\n    self: Graph[StateT, DepsT, T],\\n    start_node: BaseNode[StateT, DepsT, T],\\n    *,\\n    state: StateT = None,\\n    deps: DepsT = None,\\n    infer_name: bool = True,\\n  ) -> GraphRunResult[StateT, T]:\\n\"\"\"Synchronously run the graph.\\n    This is a convenience method that wraps [`self.run`][pydantic_graph.Graph.run] with `loop.run_until_complete(...)`.\\n    You therefore can\\'t use this method inside async code or if there\\'s an active event loop.\\n    Args:\\n      start_node: the first node to run, since the graph definition doesn\\'t define the entry point in the graph,\\n        you need to provide the starting node.\\n      state: The initial state of the graph.\\n      deps: The dependencies of the graph.\\n      infer_name: Whether to infer the graph name from the calling frame.\\n    Returns:\\n      The result type from ending the run and the history of the run.\\n    \"\"\"\\n    if infer_name and self.name is None:\\n      self._infer_name(inspect.currentframe())\\n    return asyncio.get_event_loop().run_until_complete(\\n      self.run(start_node, state=state, deps=deps, infer_name=False)\\n    )\\n  async defnext(\\n    self: Graph[StateT, DepsT, T],\\n    node: BaseNode[StateT, DepsT, T],\\n    history: list[HistoryStep[StateT, T]],\\n    *,\\n    state: StateT = None,\\n    deps: DepsT = None,\\n    infer_name: bool = True,\\n  ) -> BaseNode[StateT, DepsT, Any] | End[T]:\\n\"\"\"Run a node in the graph and return the next node to run.\\n    Args:\\n      node: The node to run.\\n      history: The history of the graph run so far. NOTE: this will be mutated to add the new step.\\n      state: The current state of the graph.\\n      deps: The dependencies of the graph.\\n      infer_name: Whether to infer the graph name from the calling frame.\\n    Returns:\\n      The next node to run or [`End`][pydantic_graph.nodes.End] if the graph has finished.\\n    \"\"\"\\n    if infer_name and self.name is None:\\n      self._infer_name(inspect.currentframe())\\n    if isinstance(node, End):\\n      # While technically this is not compatible with the documented method signature, it\\'s an easy mistake to\\n      # make, and we should eagerly provide a more helpful error message than you\\'d get otherwise.\\n      raise exceptions.GraphRuntimeError(f\\'Cannot call `next` with an `End` node: {node!r}.\\')\\n    node_id = node.get_id()\\n    if node_id not in self.node_defs:\\n      raise exceptions.GraphRuntimeError(f\\'Node `{node}` is not in the graph.\\')\\n    with ExitStack() as stack:\\n      if self._auto_instrument:\\n        stack.enter_context(_logfire.span(\\'run node {node_id}\\', node_id=node_id, node=node))\\n      ctx = GraphRunContext(state, deps)\\n      start_ts = _utils.now_utc()\\n      start = perf_counter()\\n      next_node = await node.run(ctx)\\n      duration = perf_counter() - start\\n    history.append(\\n      NodeStep(state=state, node=node, start_ts=start_ts, duration=duration, snapshot_state=self.snapshot_state)\\n    )\\n    if isinstance(next_node, End):\\n      history.append(EndStep(result=next_node))\\n    elif not isinstance(next_node, BaseNode):\\n      if TYPE_CHECKING:\\n        typing_extensions.assert_never(next_node)\\n      else:\\n        raise exceptions.GraphRuntimeError(\\n          f\\'Invalid node return type: `{type(next_node).__name__}`. Expected `BaseNode` or `End`.\\'\\n        )\\n    return next_node\\n  defdump_history(\\n    self: Graph[StateT, DepsT, T], history: list[HistoryStep[StateT, T]], *, indent: int | None = None\\n  ) -> bytes:\\n\"\"\"Dump the history of a graph run as JSON.\\n    Args:\\n      history: The history of the graph run.\\n      indent: The number of spaces to indent the JSON.\\n    Returns:\\n      The JSON representation of the history.\\n    \"\"\"\\n    return self.history_type_adapter.dump_json(history, indent=indent)\\n  defload_history(self, json_bytes: str | bytes | bytearray) -> list[HistoryStep[StateT, RunEndT]]:\\n\"\"\"Load the history of a graph run from JSON.\\n    Args:\\n      json_bytes: The JSON representation of the history.\\n    Returns:\\n      The history of the graph run.\\n    \"\"\"\\n    return self.history_type_adapter.validate_json(json_bytes)\\n  @cached_property\\n  defhistory_type_adapter(self) -> pydantic.TypeAdapter[list[HistoryStep[StateT, RunEndT]]]:\\n    nodes = [node_def.node for node_def in self.node_defs.values()]\\n    state_t = self._get_state_type()\\n    end_t = self._get_run_end_type()\\n    token = nodes_schema_var.set(nodes)\\n    try:\\n      ta = pydantic.TypeAdapter(list[Annotated[HistoryStep[state_t, end_t], pydantic.Discriminator(\\'kind\\')]])\\n    finally:\\n      nodes_schema_var.reset(token)\\n    return ta\\n  defmermaid_code(\\n    self,\\n    *,\\n    start_node: Sequence[mermaid.NodeIdent] | mermaid.NodeIdent | None = None,\\n    title: str | None | typing_extensions.Literal[False] = None,\\n    edge_labels: bool = True,\\n    notes: bool = True,\\n    highlighted_nodes: Sequence[mermaid.NodeIdent] | mermaid.NodeIdent | None = None,\\n    highlight_css: str = mermaid.DEFAULT_HIGHLIGHT_CSS,\\n    infer_name: bool = True,\\n    direction: mermaid.StateDiagramDirection | None = None,\\n  ) -> str:\\n\"\"\"Generate a diagram representing the graph as mermaid diagram.\\n    This method calls [`pydantic_graph.mermaid.generate_code`][pydantic_graph.mermaid.generate_code].\\n    Args:\\n      start_node: The node or nodes which can start the graph.\\n      title: The title of the diagram, use `False` to not include a title.\\n      edge_labels: Whether to include edge labels.\\n      notes: Whether to include notes on each node.\\n      highlighted_nodes: Optional node or nodes to highlight.\\n      highlight_css: The CSS to use for highlighting nodes.\\n      infer_name: Whether to infer the graph name from the calling frame.\\n      direction: The direction of flow.\\n    Returns:\\n      The mermaid code for the graph, which can then be rendered as a diagram.\\n    Here\\'s an example of generating a diagram for the graph from [above][pydantic_graph.graph.Graph]:'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 132, 'url': 'https://pydantic.com'}, page_content='The rendered diagram will look like this:'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 133, 'url': 'https://pydantic.com'}, page_content='\"\"\"\\n    if infer_name and self.name is None:\\n      self._infer_name(inspect.currentframe())\\n    if title is None and self.name:\\n      title = self.name\\n    return mermaid.generate_code(\\n      self,\\n      start_node=start_node,\\n      highlighted_nodes=highlighted_nodes,\\n      highlight_css=highlight_css,\\n      title=title or None,\\n      edge_labels=edge_labels,\\n      notes=notes,\\n      direction=direction,\\n    )\\n  defmermaid_image(\\n    self, infer_name: bool = True, **kwargs: typing_extensions.Unpack[mermaid.MermaidConfig]\\n  ) -> bytes:\\n\"\"\"Generate a diagram representing the graph as an image.\\n    The format and diagram can be customized using `kwargs`,\\n    see [`pydantic_graph.mermaid.MermaidConfig`][pydantic_graph.mermaid.MermaidConfig].\\n    !!! note \"Uses external service\"\\n      This method makes a request to mermaid.ink to render the image, `mermaid.ink`\\n      is a free service not affiliated with Pydantic.\\n    Args:\\n      infer_name: Whether to infer the graph name from the calling frame.\\n      **kwargs: Additional arguments to pass to `mermaid.request_image`.\\n    Returns:\\n      The image bytes.\\n    \"\"\"\\n    if infer_name and self.name is None:\\n      self._infer_name(inspect.currentframe())\\n    if \\'title\\' not in kwargs and self.name:\\n      kwargs[\\'title\\'] = self.name\\n    return mermaid.request_image(self, **kwargs)\\n  defmermaid_save(\\n    self, path: Path | str, /, *, infer_name: bool = True, **kwargs: typing_extensions.Unpack[mermaid.MermaidConfig]\\n  ) -> None:\\n\"\"\"Generate a diagram representing the graph and save it as an image.\\n    The format and diagram can be customized using `kwargs`,\\n    see [`pydantic_graph.mermaid.MermaidConfig`][pydantic_graph.mermaid.MermaidConfig].\\n    !!! note \"Uses external service\"\\n      This method makes a request to mermaid.ink to render the image, `mermaid.ink`\\n      is a free service not affiliated with Pydantic.\\n    Args:\\n      path: The path to save the image to.\\n      infer_name: Whether to infer the graph name from the calling frame.\\n      **kwargs: Additional arguments to pass to `mermaid.save_image`.\\n    \"\"\"\\n    if infer_name and self.name is None:\\n      self._infer_name(inspect.currentframe())\\n    if \\'title\\' not in kwargs and self.name:\\n      kwargs[\\'title\\'] = self.name\\n    mermaid.save_image(path, self, **kwargs)\\n  def_get_state_type(self) -> type[StateT]:\\n    if _utils.is_set(self._state_type):\\n      return self._state_type\\n    for node_def in self.node_defs.values():\\n      for base in typing_extensions.get_original_bases(node_def.node):\\n        if typing_extensions.get_origin(base) is BaseNode:\\n          args = typing_extensions.get_args(base)\\n          if args:\\n            return args[0]\\n          # break the inner (bases) loop\\n          break\\n    # state defaults to None, so use that if we can\\'t infer it\\n    return type(None) # pyright: ignore[reportReturnType]\\n  def_get_run_end_type(self) -> type[RunEndT]:\\n    if _utils.is_set(self._run_end_type):\\n      return self._run_end_type\\n    for node_def in self.node_defs.values():\\n      for base in typing_extensions.get_original_bases(node_def.node):\\n        if typing_extensions.get_origin(base) is BaseNode:\\n          args = typing_extensions.get_args(base)\\n          if len(args) == 3:\\n            t = args[2]\\n            if not _utils.is_never(t):\\n              return t\\n          # break the inner (bases) loop\\n          break\\n    raise exceptions.GraphSetupError(\\'Could not infer run end type from nodes, please set `run_end_type`.\\')\\n  def_register_node(\\n    self: Graph[StateT, DepsT, T],\\n    node: type[BaseNode[StateT, DepsT, T]],\\n    parent_namespace: dict[str, Any] | None,\\n  ) -> None:\\n    node_id = node.get_id()\\n    if existing_node := self.node_defs.get(node_id):\\n      raise exceptions.GraphSetupError(\\n        f\\'Node ID `{node_id}` is not unique — found on {existing_node.node} and {node}\\'\\n      )\\n    else:\\n      self.node_defs[node_id] = node.get_node_def(parent_namespace)\\n  def_validate_edges(self):\\n    known_node_ids = self.node_defs.keys()\\n    bad_edges: dict[str, list[str]] = {}\\n    for node_id, node_def in self.node_defs.items():\\n      for edge in node_def.next_node_edges.keys():\\n        if edge not in known_node_ids:\\n          bad_edges.setdefault(edge, []).append(f\\'`{node_id}`\\')\\n    if bad_edges:\\n      bad_edges_list = [f\\'`{k}` is referenced by {_utils.comma_and(v)}\\' for k, v in bad_edges.items()]\\n      if len(bad_edges_list) == 1:\\n        raise exceptions.GraphSetupError(f\\'{bad_edges_list[0]} but not included in the graph.\\')\\n      else:\\n        b = \\'\\\\n\\'.join(f\\' {be}\\' for be in bad_edges_list)\\n        raise exceptions.GraphSetupError(\\n          f\\'Nodes are referenced in the graph but not included in the graph:\\\\n{b}\\'\\n        )\\n  def_infer_name(self, function_frame: types.FrameType | None) -> None:\\n\"\"\"Infer the agent name from the call frame.\\n    Usage should be `self._infer_name(inspect.currentframe())`.\\n    Copied from `Agent`.\\n    \"\"\"\\n    assert self.name is None, \\'Name already set\\'\\n    if function_frame is not None and (parent_frame := function_frame.f_back): # pragma: no branch\\n      for name, item in parent_frame.f_locals.items():\\n        if item is self:\\n          self.name = name\\n          return\\n      if parent_frame.f_locals != parent_frame.f_globals:\\n        # if we couldn\\'t find the agent in locals and globals are a different dict, try globals\\n        for name, item in parent_frame.f_globals.items():\\n          if item is self:\\n            self.name = name\\n            return'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 133, 'url': 'https://pydantic.com'}, page_content='__init__(\\n  *,\\n  nodes: Sequencetype[BaseNode[StateT[, DepsT, RunEndT]]],\\n  name: str | None = None,\\n  state_type: typeStateT[] | Unset = UNSET,\\n  run_end_type: typeRunEndT[] | Unset = UNSET,\\n  snapshot_state: Callable[\\n    StateT[], StateT\\n  ] = deep_copy_state,\\n  auto_instrument: bool = True\\n)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 133, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 133, 'url': 'https://pydantic.com'}, page_content='def__init__(\\n  self,\\n  *,\\n  nodes: Sequence[type[BaseNode[StateT, DepsT, RunEndT]]],\\n  name: str | None = None,\\n  state_type: type[StateT] | _utils.Unset = _utils.UNSET,\\n  run_end_type: type[RunEndT] | _utils.Unset = _utils.UNSET,\\n  snapshot_state: Callable[[StateT], StateT] = deep_copy_state,\\n  auto_instrument: bool = True,\\n):\\n\"\"\"Create a graph from a sequence of nodes.\\n  Args:\\n    nodes: The nodes which make up the graph, nodes need to be unique and all be generic in the same\\n      state type.\\n    name: Optional name for the graph, if not provided the name will be inferred from the calling frame\\n      on the first call to a graph method.\\n    state_type: The type of the state for the graph, this can generally be inferred from `nodes`.\\n    run_end_type: The type of the result of running the graph, this can generally be inferred from `nodes`.\\n    snapshot_state: A function to snapshot the state of the graph, this is used in\\n      [`NodeStep`][pydantic_graph.state.NodeStep] and [`EndStep`][pydantic_graph.state.EndStep] to record\\n      the state before each step.\\n    auto_instrument: Whether to create a span for the graph run and the execution of each node\\'s run method.\\n  \"\"\"\\n  self.name = name\\n  self._state_type = state_type\\n  self._run_end_type = run_end_type\\n  self._auto_instrument = auto_instrument\\n  self.snapshot_state = snapshot_state\\n  parent_namespace = _utils.get_parent_namespace(inspect.currentframe())\\n  self.node_defs: dict[str, NodeDef[StateT, DepsT, RunEndT]] = {}\\n  for node in nodes:\\n    self._register_node(node, parent_namespace)\\n  self._validate_edges()'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 133, 'url': 'https://pydantic.com'}, page_content='run(\\n  start_node: BaseNodeStateT[, DepsT, T],\\n  *,\\n  state: StateT = None,\\n  deps: DepsT = None,\\n  infer_name: bool = True,\\n  span: LogfireSpan | None = None\\n) -> GraphRunResultStateT[, T]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 134, 'url': 'https://pydantic.com'}, page_content='fromnever_42import Increment, MyState, never_42_graph\\nasync defmain():\\n  state = MyState(1)\\n  graph_run_result = await never_42_graph.run(Increment(), state=state)\\n  print(state)\\n  #> MyState(number=2)\\n  print(len(graph_run_result.history))\\n  #> 3\\n  state = MyState(41)\\n  graph_run_result = await never_42_graph.run(Increment(), state=state)\\n  print(state)\\n  #> MyState(number=43)\\n  print(len(graph_run_result.history))\\n  #> 5'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 134, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 134, 'url': 'https://pydantic.com'}, page_content='async defrun(\\n  self: Graph[StateT, DepsT, T],\\n  start_node: BaseNode[StateT, DepsT, T],\\n  *,\\n  state: StateT = None,\\n  deps: DepsT = None,\\n  infer_name: bool = True,\\n  span: LogfireSpan | None = None,\\n) -> GraphRunResult[StateT, T]:\\n\"\"\"Run the graph from a starting node until it ends.\\n  Args:\\n    start_node: the first node to run, since the graph definition doesn\\'t define the entry point in the graph,\\n      you need to provide the starting node.\\n    state: The initial state of the graph.\\n    deps: The dependencies of the graph.\\n    infer_name: Whether to infer the graph name from the calling frame.\\n    span: The span to use for the graph run. If not provided, a span will be created depending on the value of\\n      the `_auto_instrument` field.\\n  Returns:\\n    A `GraphRunResult` containing information about the run, including its final result.\\n  Here\\'s an example of running the graph from [above][pydantic_graph.graph.Graph]:'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 134, 'url': 'https://pydantic.com'}, page_content='\"\"\"\\n  if infer_name and self.name is None:\\n    self._infer_name(inspect.currentframe())\\n  with self.iter(start_node, state=state, deps=deps, infer_name=infer_name, span=span) as graph_run:\\n    async for _node in graph_run:\\n      pass\\n  final_result = graph_run.result\\n  assert final_result is not None, \\'GraphRun should have a final result\\'\\n  return final_result'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 134, 'url': 'https://pydantic.com'}, page_content='iter(\\n  start_node: BaseNodeStateT[, DepsT, T],\\n  *,\\n  state: StateT = None,\\n  deps: DepsT = None,\\n  infer_name: bool = True,\\n  span: LogfireSpan | None = None\\n) -> IteratorGraphRun[StateT[, DepsT, T]]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 135, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 135, 'url': 'https://pydantic.com'}, page_content='@contextmanager\\ndefiter(\\n  self: Graph[StateT, DepsT, T],\\n  start_node: BaseNode[StateT, DepsT, T],\\n  *,\\n  state: StateT = None,\\n  deps: DepsT = None,\\n  infer_name: bool = True,\\n  span: LogfireSpan | None = None,\\n) -> Iterator[GraphRun[StateT, DepsT, T]]:\\n\"\"\"A contextmanager which can be used to iterate over the graph\\'s nodes as they are executed.\\n  This method returns a `GraphRun` object which can be used to async-iterate over the nodes of this `Graph` as\\n  they are executed. This is the API to use if you want to record or interact with the nodes as the graph\\n  execution unfolds.\\n  The `GraphRun` can also be used to manually drive the graph execution by calling\\n  [`GraphRun.next`][pydantic_graph.graph.GraphRun.next].\\n  The `GraphRun` provides access to the full run history, state, deps, and the final result of the run once\\n  it has completed.\\n  For more details, see the API documentation of [`GraphRun`][pydantic_graph.graph.GraphRun].\\n  Args:\\n    start_node: the first node to run. Since the graph definition doesn\\'t define the entry point in the graph,\\n      you need to provide the starting node.\\n    state: The initial state of the graph.\\n    deps: The dependencies of the graph.\\n    infer_name: Whether to infer the graph name from the calling frame.\\n    span: The span to use for the graph run. If not provided, a new span will be created.\\n  Yields:\\n    A GraphRun that can be async iterated over to drive the graph to completion.\\n  \"\"\"\\n  if infer_name and self.name is None:\\n    self._infer_name(inspect.currentframe())\\n  if self._auto_instrument and span is None:\\n    span = logfire_api.span(\\'run graph {graph.name}\\', graph=self)\\n  with ExitStack() as stack:\\n    if span is not None:\\n      stack.enter_context(span)\\n    yield GraphRun[StateT, DepsT, T](\\n      self,\\n      start_node,\\n      history=[],\\n      state=state,\\n      deps=deps,\\n      auto_instrument=self._auto_instrument,\\n      span=span,\\n    )'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 135, 'url': 'https://pydantic.com'}, page_content='run_sync(\\n  start_node: BaseNodeStateT[, DepsT, T],\\n  *,\\n  state: StateT = None,\\n  deps: DepsT = None,\\n  infer_name: bool = True\\n) -> GraphRunResultStateT[, T]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 135, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 135, 'url': 'https://pydantic.com'}, page_content='defrun_sync(\\n  self: Graph[StateT, DepsT, T],\\n  start_node: BaseNode[StateT, DepsT, T],\\n  *,\\n  state: StateT = None,\\n  deps: DepsT = None,\\n  infer_name: bool = True,\\n) -> GraphRunResult[StateT, T]:\\n\"\"\"Synchronously run the graph.\\n  This is a convenience method that wraps [`self.run`][pydantic_graph.Graph.run] with `loop.run_until_complete(...)`.\\n  You therefore can\\'t use this method inside async code or if there\\'s an active event loop.\\n  Args:\\n    start_node: the first node to run, since the graph definition doesn\\'t define the entry point in the graph,\\n      you need to provide the starting node.\\n    state: The initial state of the graph.\\n    deps: The dependencies of the graph.\\n    infer_name: Whether to infer the graph name from the calling frame.\\n  Returns:\\n    The result type from ending the run and the history of the run.\\n  \"\"\"\\n  if infer_name and self.name is None:\\n    self._infer_name(inspect.currentframe())\\n  return asyncio.get_event_loop().run_until_complete(\\n    self.run(start_node, state=state, deps=deps, infer_name=False)\\n  )'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 135, 'url': 'https://pydantic.com'}, page_content='next(\\n  node: BaseNodeStateT[, DepsT, T],\\n  history: listHistoryStep[StateT[, T]],\\n  *,\\n  state: StateT = None,\\n  deps: DepsT = None,\\n  infer_name: bool = True\\n) -> BaseNodeStateT[, DepsT, Any] | End[T]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 136, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 136, 'url': 'https://pydantic.com'}, page_content='async defnext(\\n  self: Graph[StateT, DepsT, T],\\n  node: BaseNode[StateT, DepsT, T],\\n  history: list[HistoryStep[StateT, T]],\\n  *,\\n  state: StateT = None,\\n  deps: DepsT = None,\\n  infer_name: bool = True,\\n) -> BaseNode[StateT, DepsT, Any] | End[T]:\\n\"\"\"Run a node in the graph and return the next node to run.\\n  Args:\\n    node: The node to run.\\n    history: The history of the graph run so far. NOTE: this will be mutated to add the new step.\\n    state: The current state of the graph.\\n    deps: The dependencies of the graph.\\n    infer_name: Whether to infer the graph name from the calling frame.\\n  Returns:\\n    The next node to run or [`End`][pydantic_graph.nodes.End] if the graph has finished.\\n  \"\"\"\\n  if infer_name and self.name is None:\\n    self._infer_name(inspect.currentframe())\\n  if isinstance(node, End):\\n    # While technically this is not compatible with the documented method signature, it\\'s an easy mistake to\\n    # make, and we should eagerly provide a more helpful error message than you\\'d get otherwise.\\n    raise exceptions.GraphRuntimeError(f\\'Cannot call `next` with an `End` node: {node!r}.\\')\\n  node_id = node.get_id()\\n  if node_id not in self.node_defs:\\n    raise exceptions.GraphRuntimeError(f\\'Node `{node}` is not in the graph.\\')\\n  with ExitStack() as stack:\\n    if self._auto_instrument:\\n      stack.enter_context(_logfire.span(\\'run node {node_id}\\', node_id=node_id, node=node))\\n    ctx = GraphRunContext(state, deps)\\n    start_ts = _utils.now_utc()\\n    start = perf_counter()\\n    next_node = await node.run(ctx)\\n    duration = perf_counter() - start\\n  history.append(\\n    NodeStep(state=state, node=node, start_ts=start_ts, duration=duration, snapshot_state=self.snapshot_state)\\n  )\\n  if isinstance(next_node, End):\\n    history.append(EndStep(result=next_node))\\n  elif not isinstance(next_node, BaseNode):\\n    if TYPE_CHECKING:\\n      typing_extensions.assert_never(next_node)\\n    else:\\n      raise exceptions.GraphRuntimeError(\\n        f\\'Invalid node return type: `{type(next_node).__name__}`. Expected `BaseNode` or `End`.\\'\\n      )\\n  return next_node'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 136, 'url': 'https://pydantic.com'}, page_content='dump_history(\\n  history: listHistoryStep[StateT[, T]],\\n  *,\\n  indent: int | None = None\\n) -> bytes'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 136, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 136, 'url': 'https://pydantic.com'}, page_content='defdump_history(\\n  self: Graph[StateT, DepsT, T], history: list[HistoryStep[StateT, T]], *, indent: int | None = None\\n) -> bytes:\\n\"\"\"Dump the history of a graph run as JSON.\\n  Args:\\n    history: The history of the graph run.\\n    indent: The number of spaces to indent the JSON.\\n  Returns:\\n    The JSON representation of the history.\\n  \"\"\"\\n  return self.history_type_adapter.dump_json(history, indent=indent)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 136, 'url': 'https://pydantic.com'}, page_content='load_history(\\n  json_bytes: str | bytes | bytearray,\\n) -> listHistoryStep[StateT[, RunEndT]]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 136, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 136, 'url': 'https://pydantic.com'}, page_content='defload_history(self, json_bytes: str | bytes | bytearray) -> list[HistoryStep[StateT, RunEndT]]:\\n\"\"\"Load the history of a graph run from JSON.\\n  Args:\\n    json_bytes: The JSON representation of the history.\\n  Returns:\\n    The history of the graph run.\\n  \"\"\"\\n  return self.history_type_adapter.validate_json(json_bytes)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 136, 'url': 'https://pydantic.com'}, page_content='mermaid_code(\\n  *,\\n  start_node: (\\n    SequenceNodeIdent[] | NodeIdent | None\\n  ) = None,\\n  title: str | None | Literal[False] = None,\\n  edge_labels: bool = True,\\n  notes: bool = True,\\n  highlighted_nodes: (\\n    SequenceNodeIdent[] | NodeIdent | None\\n  ) = None,\\n  highlight_css: str = DEFAULT_HIGHLIGHT_CSS,\\n  infer_name: bool = True,\\n  direction: StateDiagramDirection | None = None\\n) -> str'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 137, 'url': 'https://pydantic.com'}, page_content=\"fromnever_42import Increment, never_42_graph\\nprint(never_42_graph.mermaid_code(start_node=Increment))\\n'''\\n---\\ntitle: never_42_graph\\n---\\nstateDiagram-v2\\n [*] --> Increment\\n Increment --> Check42\\n Check42 --> Increment\\n Check42 --> [*]\\n'''\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 137, 'url': 'https://pydantic.com'}, page_content='---\\ntitle: never_42_graph\\n---\\nstateDiagram-v2\\n [*] --> Increment\\n Increment --> Check42\\n Check42 --> Increment\\n Check42 --> [*]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 137, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 137, 'url': 'https://pydantic.com'}, page_content='defmermaid_code(\\n  self,\\n  *,\\n  start_node: Sequence[mermaid.NodeIdent] | mermaid.NodeIdent | None = None,\\n  title: str | None | typing_extensions.Literal[False] = None,\\n  edge_labels: bool = True,\\n  notes: bool = True,\\n  highlighted_nodes: Sequence[mermaid.NodeIdent] | mermaid.NodeIdent | None = None,\\n  highlight_css: str = mermaid.DEFAULT_HIGHLIGHT_CSS,\\n  infer_name: bool = True,\\n  direction: mermaid.StateDiagramDirection | None = None,\\n) -> str:\\n\"\"\"Generate a diagram representing the graph as mermaid diagram.\\n  This method calls [`pydantic_graph.mermaid.generate_code`][pydantic_graph.mermaid.generate_code].\\n  Args:\\n    start_node: The node or nodes which can start the graph.\\n    title: The title of the diagram, use `False` to not include a title.\\n    edge_labels: Whether to include edge labels.\\n    notes: Whether to include notes on each node.\\n    highlighted_nodes: Optional node or nodes to highlight.\\n    highlight_css: The CSS to use for highlighting nodes.\\n    infer_name: Whether to infer the graph name from the calling frame.\\n    direction: The direction of flow.\\n  Returns:\\n    The mermaid code for the graph, which can then be rendered as a diagram.\\n  Here\\'s an example of generating a diagram for the graph from [above][pydantic_graph.graph.Graph]:'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 137, 'url': 'https://pydantic.com'}, page_content='The rendered diagram will look like this:'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 137, 'url': 'https://pydantic.com'}, page_content='\"\"\"\\n  if infer_name and self.name is None:\\n    self._infer_name(inspect.currentframe())\\n  if title is None and self.name:\\n    title = self.name\\n  return mermaid.generate_code(\\n    self,\\n    start_node=start_node,\\n    highlighted_nodes=highlighted_nodes,\\n    highlight_css=highlight_css,\\n    title=title or None,\\n    edge_labels=edge_labels,\\n    notes=notes,\\n    direction=direction,\\n  )'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 137, 'url': 'https://pydantic.com'}, page_content='mermaid_image(\\n  infer_name: bool = True, **kwargs: UnpackMermaidConfig[]\\n) -> bytes'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 137, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 137, 'url': 'https://pydantic.com'}, page_content='defmermaid_image(\\n  self, infer_name: bool = True, **kwargs: typing_extensions.Unpack[mermaid.MermaidConfig]\\n) -> bytes:\\n\"\"\"Generate a diagram representing the graph as an image.\\n  The format and diagram can be customized using `kwargs`,\\n  see [`pydantic_graph.mermaid.MermaidConfig`][pydantic_graph.mermaid.MermaidConfig].\\n  !!! note \"Uses external service\"\\n    This method makes a request to mermaid.ink to render the image, `mermaid.ink`\\n    is a free service not affiliated with Pydantic.\\n  Args:\\n    infer_name: Whether to infer the graph name from the calling frame.\\n    **kwargs: Additional arguments to pass to `mermaid.request_image`.\\n  Returns:\\n    The image bytes.\\n  \"\"\"\\n  if infer_name and self.name is None:\\n    self._infer_name(inspect.currentframe())\\n  if \\'title\\' not in kwargs and self.name:\\n    kwargs[\\'title\\'] = self.name\\n  return mermaid.request_image(self, **kwargs)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 137, 'url': 'https://pydantic.com'}, page_content='mermaid_save(\\n  path: Path | str,\\n  /,\\n  *,\\n  infer_name: bool = True,\\n  **kwargs: UnpackMermaidConfig[],\\n) -> None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 138, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 138, 'url': 'https://pydantic.com'}, page_content='defmermaid_save(\\n  self, path: Path | str, /, *, infer_name: bool = True, **kwargs: typing_extensions.Unpack[mermaid.MermaidConfig]\\n) -> None:\\n\"\"\"Generate a diagram representing the graph and save it as an image.\\n  The format and diagram can be customized using `kwargs`,\\n  see [`pydantic_graph.mermaid.MermaidConfig`][pydantic_graph.mermaid.MermaidConfig].\\n  !!! note \"Uses external service\"\\n    This method makes a request to mermaid.ink to render the image, `mermaid.ink`\\n    is a free service not affiliated with Pydantic.\\n  Args:\\n    path: The path to save the image to.\\n    infer_name: Whether to infer the graph name from the calling frame.\\n    **kwargs: Additional arguments to pass to `mermaid.save_image`.\\n  \"\"\"\\n  if infer_name and self.name is None:\\n    self._infer_name(inspect.currentframe())\\n  if \\'title\\' not in kwargs and self.name:\\n    kwargs[\\'title\\'] = self.name\\n  mermaid.save_image(path, self, **kwargs)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 138, 'url': 'https://pydantic.com'}, page_content=\"fromcopyimport deepcopy\\nfromnever_42import Increment, MyState, never_42_graph\\nasync defmain():\\n  state = MyState(1)\\n  with never_42_graph.iter(Increment(), state=state) as graph_run:\\n    node_states = [(graph_run.next_node, deepcopy(graph_run.state))]\\n    async for node in graph_run:\\n      node_states.append((node, deepcopy(graph_run.state)))\\n    print(node_states)\\n'''\\n    [\\n      (Increment(), MyState(number=1)),\\n      (Check42(), MyState(number=2)),\\n      (End(data=2), MyState(number=2)),\\n    ]\\n    '''\\n  state = MyState(41)\\n  with never_42_graph.iter(Increment(), state=state) as graph_run:\\n    node_states = [(graph_run.next_node, deepcopy(graph_run.state))]\\n    async for node in graph_run:\\n      node_states.append((node, deepcopy(graph_run.state)))\\n    print(node_states)\\n'''\\n    [\\n      (Increment(), MyState(number=41)),\\n      (Check42(), MyState(number=42)),\\n      (Increment(), MyState(number=42)),\\n      (Check42(), MyState(number=43)),\\n      (End(data=43), MyState(number=43)),\\n    ]\\n    '''\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 139, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 139, 'url': 'https://pydantic.com'}, page_content='classGraphRun(Generic[StateT, DepsT, RunEndT]):\\n\"\"\"A stateful, async-iterable run of a [`Graph`][pydantic_graph.graph.Graph].\\n  You typically get a `GraphRun` instance from calling\\n  `with [my_graph.iter(...)][pydantic_graph.graph.Graph.iter] as graph_run:`. That gives you the ability to iterate\\n  through nodes as they run, either by `async for` iteration or by repeatedly calling `.next(...)`.\\n  Here\\'s an example of iterating over the graph from [above][pydantic_graph.graph.Graph]:'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 139, 'url': 'https://pydantic.com'}, page_content='See the [`GraphRun.next` documentation][pydantic_graph.graph.GraphRun.next] for an example of how to manually\\n  drive the graph run.\\n  \"\"\"\\n  def__init__(\\n    self,\\n    graph: Graph[StateT, DepsT, RunEndT],\\n    start_node: BaseNode[StateT, DepsT, RunEndT],\\n    *,\\n    history: list[HistoryStep[StateT, RunEndT]],\\n    state: StateT,\\n    deps: DepsT,\\n    auto_instrument: bool,\\n    span: LogfireSpan | None = None,\\n  ):\\n\"\"\"Create a new run for a given graph, starting at the specified node.\\n    Typically, you\\'ll use [`Graph.iter`][pydantic_graph.graph.Graph.iter] rather than calling this directly.\\n    Args:\\n      graph: The [`Graph`][pydantic_graph.graph.Graph] to run.\\n      start_node: The node where execution will begin.\\n      history: A list of [`HistoryStep`][pydantic_graph.state.HistoryStep] objects that describe\\n        each step of the run. Usually starts empty; can be populated if resuming.\\n      state: A shared state object or primitive (like a counter, dataclass, etc.) that is available\\n        to all nodes via `ctx.state`.\\n      deps: Optional dependencies that each node can access via `ctx.deps`, e.g. database connections,\\n        configuration, or logging clients.\\n      auto_instrument: Whether to automatically create instrumentation spans during the run.\\n      span: An optional existing Logfire span to nest node-level spans under (advanced usage).\\n    \"\"\"\\n    self.graph = graph\\n    self.history = history\\n    self.state = state\\n    self.deps = deps\\n    self._auto_instrument = auto_instrument\\n    self._span = span\\n    self._next_node: BaseNode[StateT, DepsT, RunEndT] | End[RunEndT] = start_node\\n  @property\\n  defnext_node(self) -> BaseNode[StateT, DepsT, RunEndT] | End[RunEndT]:\\n\"\"\"The next node that will be run in the graph.\\n    This is the next node that will be used during async iteration, or if a node is not passed to `self.next(...)`.\\n    \"\"\"\\n    return self._next_node\\n  @property\\n  defresult(self) -> GraphRunResult[StateT, RunEndT] | None:\\n\"\"\"The final result of the graph run if the run is completed, otherwise `None`.\"\"\"\\n    if not isinstance(self._next_node, End):\\n      return None # The GraphRun has not finished running\\n    return GraphRunResult(\\n      self._next_node.data,\\n      state=self.state,\\n      history=self.history,\\n    )\\n  async defnext(\\n    self: GraphRun[StateT, DepsT, T], node: BaseNode[StateT, DepsT, T] | None = None\\n  ) -> BaseNode[StateT, DepsT, T] | End[T]:\\n\"\"\"Manually drive the graph run by passing in the node you want to run next.\\n    This lets you inspect or mutate the node before continuing execution, or skip certain nodes\\n    under dynamic conditions. The graph run should stop when you return an [`End`][pydantic_graph.nodes.End] node.\\n    Here\\'s an example of using `next` to drive the graph from [above][pydantic_graph.graph.Graph]:'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 140, 'url': 'https://pydantic.com'}, page_content='Args:\\n      node: The node to run next in the graph. If not specified, uses `self.next_node`, which is initialized to\\n        the `start_node` of the run and updated each time a new node is returned.\\n    Returns:\\n      The next node returned by the graph logic, or an [`End`][pydantic_graph.nodes.End] node if\\n      the run has completed.\\n    \"\"\"\\n    if node is None:\\n      if isinstance(self._next_node, End):\\n        # Note: we could alternatively just return `self._next_node` here, but it\\'s easier to start with an\\n        # error and relax the behavior later, than vice versa.\\n        raise exceptions.GraphRuntimeError(\\'This graph run has already ended.\\')\\n      node = self._next_node\\n    history = self.history\\n    state = self.state\\n    deps = self.deps\\n    self._next_node = await self.graph.next(node, history, state=state, deps=deps, infer_name=False)\\n    return self._next_node\\n  def__aiter__(self) -> AsyncIterator[BaseNode[StateT, DepsT, RunEndT] | End[RunEndT]]:\\n    return self\\n  async def__anext__(self) -> BaseNode[StateT, DepsT, RunEndT] | End[RunEndT]:\\n\"\"\"Use the last returned node as the input to `Graph.next`.\"\"\"\\n    if isinstance(self._next_node, End):\\n      raise StopAsyncIteration\\n    return await self.next(self._next_node)\\n  def__repr__(self) -> str:\\n    return f\\'<GraphRun name={self.graph.nameor\"<unnamed>\"} step={len(self.history)+1}>\\''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 140, 'url': 'https://pydantic.com'}, page_content='__init__(\\n  graph: GraphStateT[, DepsT, RunEndT],\\n  start_node: BaseNodeStateT[, DepsT, RunEndT],\\n  *,\\n  history: listHistoryStep[StateT[, RunEndT]],\\n  state: StateT,\\n  deps: DepsT,\\n  auto_instrument: bool,\\n  span: LogfireSpan | None = None\\n)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 140, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 140, 'url': 'https://pydantic.com'}, page_content='def__init__(\\n  self,\\n  graph: Graph[StateT, DepsT, RunEndT],\\n  start_node: BaseNode[StateT, DepsT, RunEndT],\\n  *,\\n  history: list[HistoryStep[StateT, RunEndT]],\\n  state: StateT,\\n  deps: DepsT,\\n  auto_instrument: bool,\\n  span: LogfireSpan | None = None,\\n):\\n\"\"\"Create a new run for a given graph, starting at the specified node.\\n  Typically, you\\'ll use [`Graph.iter`][pydantic_graph.graph.Graph.iter] rather than calling this directly.\\n  Args:\\n    graph: The [`Graph`][pydantic_graph.graph.Graph] to run.\\n    start_node: The node where execution will begin.\\n    history: A list of [`HistoryStep`][pydantic_graph.state.HistoryStep] objects that describe\\n      each step of the run. Usually starts empty; can be populated if resuming.\\n    state: A shared state object or primitive (like a counter, dataclass, etc.) that is available\\n      to all nodes via `ctx.state`.\\n    deps: Optional dependencies that each node can access via `ctx.deps`, e.g. database connections,\\n      configuration, or logging clients.\\n    auto_instrument: Whether to automatically create instrumentation spans during the run.\\n    span: An optional existing Logfire span to nest node-level spans under (advanced usage).\\n  \"\"\"\\n  self.graph = graph\\n  self.history = history\\n  self.state = state\\n  self.deps = deps\\n  self._auto_instrument = auto_instrument\\n  self._span = span\\n  self._next_node: BaseNode[StateT, DepsT, RunEndT] | End[RunEndT] = start_node'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 140, 'url': 'https://pydantic.com'}, page_content='next_node: BaseNodeStateT[, DepsT, RunEndT] | EndRunEndT[]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 140, 'url': 'https://pydantic.com'}, page_content='result: GraphRunResultStateT[, RunEndT] | None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 140, 'url': 'https://pydantic.com'}, page_content='next(\\n  node: BaseNodeStateT[, DepsT, T] | None = None,\\n) -> BaseNodeStateT[, DepsT, T] | End[T]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 141, 'url': 'https://pydantic.com'}, page_content=\"fromcopyimport deepcopy\\nfrompydantic_graphimport End\\nfromnever_42import Increment, MyState, never_42_graph\\nasync defmain():\\n  state = MyState(48)\\n  with never_42_graph.iter(Increment(), state=state) as graph_run:\\n    next_node = graph_run.next_node # start with the first node\\n    node_states = [(next_node, deepcopy(graph_run.state))]\\n    while not isinstance(next_node, End):\\n      if graph_run.state.number == 50:\\n        graph_run.state.number = 42\\n      next_node = await graph_run.next(next_node)\\n      node_states.append((next_node, deepcopy(graph_run.state)))\\n    print(node_states)\\n'''\\n    [\\n      (Increment(), MyState(number=48)),\\n      (Check42(), MyState(number=49)),\\n      (End(data=49), MyState(number=49)),\\n    ]\\n    '''\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 141, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 141, 'url': 'https://pydantic.com'}, page_content='async defnext(\\n  self: GraphRun[StateT, DepsT, T], node: BaseNode[StateT, DepsT, T] | None = None\\n) -> BaseNode[StateT, DepsT, T] | End[T]:\\n\"\"\"Manually drive the graph run by passing in the node you want to run next.\\n  This lets you inspect or mutate the node before continuing execution, or skip certain nodes\\n  under dynamic conditions. The graph run should stop when you return an [`End`][pydantic_graph.nodes.End] node.\\n  Here\\'s an example of using `next` to drive the graph from [above][pydantic_graph.graph.Graph]:'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 141, 'url': 'https://pydantic.com'}, page_content='Args:\\n    node: The node to run next in the graph. If not specified, uses `self.next_node`, which is initialized to\\n      the `start_node` of the run and updated each time a new node is returned.\\n  Returns:\\n    The next node returned by the graph logic, or an [`End`][pydantic_graph.nodes.End] node if\\n    the run has completed.\\n  \"\"\"\\n  if node is None:\\n    if isinstance(self._next_node, End):\\n      # Note: we could alternatively just return `self._next_node` here, but it\\'s easier to start with an\\n      # error and relax the behavior later, than vice versa.\\n      raise exceptions.GraphRuntimeError(\\'This graph run has already ended.\\')\\n    node = self._next_node\\n  history = self.history\\n  state = self.state\\n  deps = self.deps\\n  self._next_node = await self.graph.next(node, history, state=state, deps=deps, infer_name=False)\\n  return self._next_node'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 141, 'url': 'https://pydantic.com'}, page_content='__anext__() -> (\\n  BaseNodeStateT[, DepsT, RunEndT] | EndRunEndT[]\\n)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 141, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 141, 'url': 'https://pydantic.com'}, page_content='async def__anext__(self) -> BaseNode[StateT, DepsT, RunEndT] | End[RunEndT]:\\n\"\"\"Use the last returned node as the input to `Graph.next`.\"\"\"\\n  if isinstance(self._next_node, End):\\n    raise StopAsyncIteration\\n  return await self.next(self._next_node)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 142, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 142, 'url': 'https://pydantic.com'}, page_content='@dataclass\\nclassGraphRunResult(Generic[StateT, RunEndT]):\\n\"\"\"The final result of running a graph.\"\"\"\\n  output: RunEndT\\n  state: StateT\\n  history: list[HistoryStep[StateT, RunEndT]] = field(repr=False)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 143, 'url': 'https://pydantic.com'}, page_content=\"DEFAULT_HIGHLIGHT_CSS = 'fill:#fdff32'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 143, 'url': 'https://pydantic.com'}, page_content=\"StateDiagramDirection = Literal['TB', 'LR', 'RL', 'BT']\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 143, 'url': 'https://pydantic.com'}, page_content='generate_code(\\n  graph: GraphAny[, Any, Any],\\n  /,\\n  *,\\n  start_node: (\\n    SequenceNodeIdent[] | NodeIdent | None\\n  ) = None,\\n  highlighted_nodes: (\\n    SequenceNodeIdent[] | NodeIdent | None\\n  ) = None,\\n  highlight_css: str = DEFAULT_HIGHLIGHT_CSS,\\n  title: str | None = None,\\n  edge_labels: bool = True,\\n  notes: bool = True,\\n  direction: StateDiagramDirection | None,\\n) -> str'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 144, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 144, 'url': 'https://pydantic.com'}, page_content='defgenerate_code( # noqa: C901\\n  graph: Graph[Any, Any, Any],\\n  /,\\n  *,\\n  start_node: Sequence[NodeIdent] | NodeIdent | None = None,\\n  highlighted_nodes: Sequence[NodeIdent] | NodeIdent | None = None,\\n  highlight_css: str = DEFAULT_HIGHLIGHT_CSS,\\n  title: str | None = None,\\n  edge_labels: bool = True,\\n  notes: bool = True,\\n  direction: StateDiagramDirection | None,\\n) -> str:\\n\"\"\"Generate Mermaid state diagram code for a graph.\\n  Args:\\n    graph: The graph to generate the image for.\\n    start_node: Identifiers of nodes that start the graph.\\n    highlighted_nodes: Identifiers of nodes to highlight.\\n    highlight_css: CSS to use for highlighting nodes.\\n    title: The title of the diagram.\\n    edge_labels: Whether to include edge labels in the diagram.\\n    notes: Whether to include notes in the diagram.\\n    direction: The direction of flow.\\n\\n  Returns:\\n    The Mermaid code for the graph.\\n  \"\"\"\\n  start_node_ids = set(_node_ids(start_node or ()))\\n  for node_id in start_node_ids:\\n    if node_id not in graph.node_defs:\\n      raise LookupError(f\\'Start node \"{node_id}\" is not in the graph.\\')\\n  lines: list[str] = []\\n  if title:\\n    lines = [\\'---\\', f\\'title: {title}\\', \\'---\\']\\n  lines.append(\\'stateDiagram-v2\\')\\n  if direction is not None:\\n    lines.append(f\\' direction {direction}\\')\\n  for node_id, node_def in graph.node_defs.items():\\n    # we use round brackets (rounded box) for nodes other than the start and end\\n    if node_id in start_node_ids:\\n      lines.append(f\\' [*] --> {node_id}\\')\\n    if node_def.returns_base_node:\\n      for next_node_id in graph.node_defs:\\n        lines.append(f\\' {node_id} --> {next_node_id}\\')\\n    else:\\n      for next_node_id, edge in node_def.next_node_edges.items():\\n        line = f\\' {node_id} --> {next_node_id}\\'\\n        if edge_labels and edge.label:\\n          line += f\\': {edge.label}\\'\\n        lines.append(line)\\n    if end_edge := node_def.end_edge:\\n      line = f\\' {node_id} --> [*]\\'\\n      if edge_labels and end_edge.label:\\n        line += f\\': {end_edge.label}\\'\\n      lines.append(line)\\n    if notes and node_def.note:\\n      lines.append(f\\' note right of {node_id}\\')\\n      # mermaid doesn\\'t like multiple paragraphs in a note, and shows if so\\n      clean_docs = re.sub(\\'\\\\n{2,}\\', \\'\\\\n\\', node_def.note)\\n      lines.append(indent(clean_docs, \\'  \\'))\\n      lines.append(\\' end note\\')\\n  if highlighted_nodes:\\n    lines.append(\\'\\')\\n    lines.append(f\\'classDef highlighted {highlight_css}\\')\\n    for node_id in _node_ids(highlighted_nodes):\\n      if node_id not in graph.node_defs:\\n        raise LookupError(f\\'Highlighted node \"{node_id}\" is not in the graph.\\')\\n      lines.append(f\\'class {node_id} highlighted\\')\\n  return \\'\\\\n\\'.join(lines)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 144, 'url': 'https://pydantic.com'}, page_content='request_image(\\n  graph: GraphAny[, Any, Any],\\n  /,\\n  **kwargs: UnpackMermaidConfig[],\\n) -> bytes'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 144, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 144, 'url': 'https://pydantic.com'}, page_content='defrequest_image(\\n  graph: Graph[Any, Any, Any],\\n  /,\\n  **kwargs: Unpack[MermaidConfig],\\n) -> bytes:\\n\"\"\"Generate an image of a Mermaid diagram using mermaid.ink.\\n  Args:\\n    graph: The graph to generate the image for.\\n    **kwargs: Additional parameters to configure mermaid chart generation.\\n  Returns:\\n    The image data.\\n  \"\"\"\\n  code = generate_code(\\n    graph,\\n    start_node=kwargs.get(\\'start_node\\'),\\n    highlighted_nodes=kwargs.get(\\'highlighted_nodes\\'),\\n    highlight_css=kwargs.get(\\'highlight_css\\', DEFAULT_HIGHLIGHT_CSS),\\n    title=kwargs.get(\\'title\\'),\\n    edge_labels=kwargs.get(\\'edge_labels\\', True),\\n    notes=kwargs.get(\\'notes\\', True),\\n    direction=kwargs.get(\\'direction\\'),\\n  )\\n  code_base64 = base64.b64encode(code.encode()).decode()\\n  params: dict[str, str | float] = {}\\n  if kwargs.get(\\'image_type\\') == \\'pdf\\':\\n    url = f\\'https://mermaid.ink/pdf/{code_base64}\\'\\n    if kwargs.get(\\'pdf_fit\\'):\\n      params[\\'fit\\'] = \\'\\'\\n    if kwargs.get(\\'pdf_landscape\\'):\\n      params[\\'landscape\\'] = \\'\\'\\n    if pdf_paper := kwargs.get(\\'pdf_paper\\'):\\n      params[\\'paper\\'] = pdf_paper\\n  elif kwargs.get(\\'image_type\\') == \\'svg\\':\\n    url = f\\'https://mermaid.ink/svg/{code_base64}\\'\\n  else:\\n    url = f\\'https://mermaid.ink/img/{code_base64}\\'\\n    if image_type := kwargs.get(\\'image_type\\'):\\n      params[\\'type\\'] = image_type\\n  if background_color := kwargs.get(\\'background_color\\'):\\n    params[\\'bgColor\\'] = background_color\\n  if theme := kwargs.get(\\'theme\\'):\\n    params[\\'theme\\'] = theme\\n  if width := kwargs.get(\\'width\\'):\\n    params[\\'width\\'] = width\\n  if height := kwargs.get(\\'height\\'):\\n    params[\\'height\\'] = height\\n  if scale := kwargs.get(\\'scale\\'):\\n    params[\\'scale\\'] = scale\\n  httpx_client = kwargs.get(\\'httpx_client\\') or httpx.Client()\\n  response = httpx_client.get(url, params=params)\\n  if not response.is_success:\\n    raise httpx.HTTPStatusError(\\n      f\\'{response.status_code} error generating image:\\\\n{response.text}\\',\\n      request=response.request,\\n      response=response,\\n    )\\n  return response.content'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 144, 'url': 'https://pydantic.com'}, page_content='save_image(\\n  path: Path | str,\\n  graph: GraphAny[, Any, Any],\\n  /,\\n  **kwargs: UnpackMermaidConfig[],\\n) -> None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 144, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 144, 'url': 'https://pydantic.com'}, page_content='defsave_image(\\n  path: Path | str,\\n  graph: Graph[Any, Any, Any],\\n  /,\\n  **kwargs: Unpack[MermaidConfig],\\n) -> None:\\n\"\"\"Generate an image of a Mermaid diagram using mermaid.ink and save it to a local file.\\n  Args:\\n    path: The path to save the image to.\\n    graph: The graph to generate the image for.\\n    **kwargs: Additional parameters to configure mermaid chart generation.\\n  \"\"\"\\n  if isinstance(path, str):\\n    path = Path(path)\\n  if \\'image_type\\' not in kwargs:\\n    ext = path.suffix.lower()[1:]\\n    # no need to check for .jpeg/.jpg, as it is the default\\n    if ext in (\\'png\\', \\'webp\\', \\'svg\\', \\'pdf\\'):\\n      kwargs[\\'image_type\\'] = ext\\n  image_data = request_image(graph, **kwargs)\\n  path.write_bytes(image_data)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 144, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 144, 'url': 'https://pydantic.com'}, page_content='classMermaidConfig(TypedDict, total=False):\\n\"\"\"Parameters to configure mermaid chart generation.\"\"\"\\n  start_node: Sequence[NodeIdent] | NodeIdent\\n\"\"\"Identifiers of nodes that start the graph.\"\"\"\\n  highlighted_nodes: Sequence[NodeIdent] | NodeIdent\\n\"\"\"Identifiers of nodes to highlight.\"\"\"\\n  highlight_css: str\\n\"\"\"CSS to use for highlighting nodes.\"\"\"\\n  title: str | None\\n\"\"\"The title of the diagram.\"\"\"\\n  edge_labels: bool\\n\"\"\"Whether to include edge labels in the diagram.\"\"\"\\n  notes: bool\\n\"\"\"Whether to include notes on nodes in the diagram, defaults to true.\"\"\"\\n  image_type: Literal[\\'jpeg\\', \\'png\\', \\'webp\\', \\'svg\\', \\'pdf\\']\\n\"\"\"The image type to generate. If unspecified, the default behavior is `\\'jpeg\\'`.\"\"\"\\n  pdf_fit: bool\\n\"\"\"When using image_type=\\'pdf\\', whether to fit the diagram to the PDF page.\"\"\"\\n  pdf_landscape: bool\\n\"\"\"When using image_type=\\'pdf\\', whether to use landscape orientation for the PDF.\\n  This has no effect if using `pdf_fit`.\\n  \"\"\"\\n  pdf_paper: Literal[\\'letter\\', \\'legal\\', \\'tabloid\\', \\'ledger\\', \\'a0\\', \\'a1\\', \\'a2\\', \\'a3\\', \\'a4\\', \\'a5\\', \\'a6\\']\\n\"\"\"When using image_type=\\'pdf\\', the paper size of the PDF.\"\"\"\\n  background_color: str\\n\"\"\"The background color of the diagram.\\n  If None, the default transparent background is used. The color value is interpreted as a hexadecimal color\\n  code by default (and should not have a leading \\'#\\'), but you can also use named colors by prefixing the\\n  value with `\\'!\\'`. For example, valid choices include `background_color=\\'!white\\'` or `background_color=\\'FF0000\\'`.\\n  \"\"\"\\n  theme: Literal[\\'default\\', \\'neutral\\', \\'dark\\', \\'forest\\']\\n\"\"\"The theme of the diagram. Defaults to \\'default\\'.\"\"\"\\n  width: int\\n\"\"\"The width of the diagram.\"\"\"\\n  height: int\\n\"\"\"The height of the diagram.\"\"\"\\n  scale: Annotated[float, Ge(1), Le(3)]\\n\"\"\"The scale of the diagram.\\n  The scale must be a number between 1 and 3, and you can only set a scale if one or both of width and height are set.\\n  \"\"\"\\n  httpx_client: httpx.Client\\n\"\"\"An HTTPX client to use for requests, mostly for testing purposes.\"\"\"\\n  direction: StateDiagramDirection\\n\"\"\"The direction of the state diagram.\"\"\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 144, 'url': 'https://pydantic.com'}, page_content='start_node: SequenceNodeIdent[] | NodeIdent'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 144, 'url': 'https://pydantic.com'}, page_content='highlighted_nodes: SequenceNodeIdent[] | NodeIdent'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 144, 'url': 'https://pydantic.com'}, page_content='highlight_css: str'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 144, 'url': 'https://pydantic.com'}, page_content='title: str | None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 144, 'url': 'https://pydantic.com'}, page_content='edge_labels: bool'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 144, 'url': 'https://pydantic.com'}, page_content='notes: bool'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 145, 'url': 'https://pydantic.com'}, page_content=\"image_type: Literal['jpeg', 'png', 'webp', 'svg', 'pdf']\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 145, 'url': 'https://pydantic.com'}, page_content='pdf_fit: bool'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 145, 'url': 'https://pydantic.com'}, page_content='pdf_landscape: bool'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 145, 'url': 'https://pydantic.com'}, page_content='pdf_paper: Literal[\\n  \"letter\",\\n  \"legal\",\\n  \"tabloid\",\\n  \"ledger\",\\n  \"a0\",\\n  \"a1\",\\n  \"a2\",\\n  \"a3\",\\n  \"a4\",\\n  \"a5\",\\n  \"a6\",\\n]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 145, 'url': 'https://pydantic.com'}, page_content='background_color: str'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 145, 'url': 'https://pydantic.com'}, page_content=\"theme: Literal['default', 'neutral', 'dark', 'forest']\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 145, 'url': 'https://pydantic.com'}, page_content='width: int'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 145, 'url': 'https://pydantic.com'}, page_content='height: int'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 145, 'url': 'https://pydantic.com'}, page_content='scale: Annotatedfloat[, Ge(1), Le(3)]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 145, 'url': 'https://pydantic.com'}, page_content='httpx_client: Client'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 145, 'url': 'https://pydantic.com'}, page_content='direction: StateDiagramDirection'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 145, 'url': 'https://pydantic.com'}, page_content='NodeIdent: TypeAlias = (\\n  \"type[BaseNode[Any, Any, Any]] | BaseNode[Any, Any, Any] | str\"\\n)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 146, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 146, 'url': 'https://pydantic.com'}, page_content='@dataclass\\nclassGraphRunContext(Generic[StateT, DepsT]):\\n\"\"\"Context for a graph.\"\"\"\\n  # TODO: Can we get rid of this struct and just pass both these things around..?\\n  state: StateT\\n\"\"\"The state of the graph.\"\"\"\\n  deps: DepsT\\n\"\"\"Dependencies for the graph.\"\"\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 146, 'url': 'https://pydantic.com'}, page_content='state: StateT'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 146, 'url': 'https://pydantic.com'}, page_content='deps: DepsT'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 146, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 146, 'url': 'https://pydantic.com'}, page_content='classBaseNode(ABC, Generic[StateT, DepsT, NodeRunEndT]):\\n\"\"\"Base class for a node.\"\"\"\\n  docstring_notes: ClassVar[bool] = False\\n\"\"\"Set to `True` to generate mermaid diagram notes from the class\\'s docstring.\\n  While this can add valuable information to the diagram, it can make diagrams harder to view, hence\\n  it is disabled by default. You can also customise notes overriding the\\n  [`get_note`][pydantic_graph.nodes.BaseNode.get_note] method.\\n  \"\"\"\\n  @abstractmethod\\n  async defrun(self, ctx: GraphRunContext[StateT, DepsT]) -> BaseNode[StateT, DepsT, Any] | End[NodeRunEndT]:\\n\"\"\"Run the node.\\n    This is an abstract method that must be implemented by subclasses.\\n    !!! note \"Return types used at runtime\"\\n      The return type of this method are read by `pydantic_graph` at runtime and used to define which\\n      nodes can be called next in the graph. This is displayed in [mermaid diagrams](mermaid.md)\\n      and enforced when running the graph.\\n    Args:\\n      ctx: The graph context.\\n    Returns:\\n      The next node to run or [`End`][pydantic_graph.nodes.End] to signal the end of the graph.\\n    \"\"\"\\n    ...\\n  @classmethod\\n  @cache\\n  defget_id(cls) -> str:\\n\"\"\"Get the ID of the node.\"\"\"\\n    return cls.__name__\\n  @classmethod\\n  defget_note(cls) -> str | None:\\n\"\"\"Get a note about the node to render on mermaid charts.\\n    By default, this returns a note only if [`docstring_notes`][pydantic_graph.nodes.BaseNode.docstring_notes]\\n    is `True`. You can override this method to customise the node notes.\\n    \"\"\"\\n    if not cls.docstring_notes:\\n      return None\\n    docstring = cls.__doc__\\n    # dataclasses get an automatic docstring which is just their signature, we don\\'t want that\\n    if docstring and is_dataclass(cls) and docstring.startswith(f\\'{cls.__name__}(\\'):\\n      docstring = None\\n    if docstring:\\n      # remove indentation from docstring\\n      importinspect\\n      docstring = inspect.cleandoc(docstring)\\n    return docstring\\n  @classmethod\\n  defget_node_def(cls, local_ns: dict[str, Any] | None) -> NodeDef[StateT, DepsT, NodeRunEndT]:\\n\"\"\"Get the node definition.\"\"\"\\n    type_hints = get_type_hints(cls.run, localns=local_ns, include_extras=True)\\n    try:\\n      return_hint = type_hints[\\'return\\']\\n    except KeyError as e:\\n      raise exceptions.GraphSetupError(f\\'Node {cls} is missing a return type hint on its `run` method\\') frome\\n    next_node_edges: dict[str, Edge] = {}\\n    end_edge: Edge | None = None\\n    returns_base_node: bool = False\\n    for return_type in _utils.get_union_args(return_hint):\\n      return_type, annotations = _utils.unpack_annotated(return_type)\\n      edge = next((a for a in annotations if isinstance(a, Edge)), Edge(None))\\n      return_type_origin = get_origin(return_type) or return_type\\n      if return_type_origin is End:\\n        end_edge = edge\\n      elif return_type_origin is BaseNode:\\n        # TODO: Should we disallow this?\\n        returns_base_node = True\\n      elif issubclass(return_type_origin, BaseNode):\\n        next_node_edges[return_type.get_id()] = edge\\n      else:\\n        raise exceptions.GraphSetupError(f\\'Invalid return type: {return_type}\\')\\n    return NodeDef(\\n      cls,\\n      cls.get_id(),\\n      cls.get_note(),\\n      next_node_edges,\\n      end_edge,\\n      returns_base_node,\\n    )'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 146, 'url': 'https://pydantic.com'}, page_content='docstring_notes: bool = False'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 146, 'url': 'https://pydantic.com'}, page_content='run(\\n  ctx: GraphRunContextStateT[, DepsT],\\n) -> BaseNodeStateT[, DepsT, Any] | EndNodeRunEndT[]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 147, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 147, 'url': 'https://pydantic.com'}, page_content='@abstractmethod\\nasync defrun(self, ctx: GraphRunContext[StateT, DepsT]) -> BaseNode[StateT, DepsT, Any] | End[NodeRunEndT]:\\n\"\"\"Run the node.\\n  This is an abstract method that must be implemented by subclasses.\\n  !!! note \"Return types used at runtime\"\\n    The return type of this method are read by `pydantic_graph` at runtime and used to define which\\n    nodes can be called next in the graph. This is displayed in [mermaid diagrams](mermaid.md)\\n    and enforced when running the graph.\\n  Args:\\n    ctx: The graph context.\\n  Returns:\\n    The next node to run or [`End`][pydantic_graph.nodes.End] to signal the end of the graph.\\n  \"\"\"\\n  ...'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 147, 'url': 'https://pydantic.com'}, page_content='get_id() -> str'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 147, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 147, 'url': 'https://pydantic.com'}, page_content='@classmethod\\n@cache\\ndefget_id(cls) -> str:\\n\"\"\"Get the ID of the node.\"\"\"\\n  return cls.__name__'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 147, 'url': 'https://pydantic.com'}, page_content='get_note() -> str | None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 147, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 147, 'url': 'https://pydantic.com'}, page_content='@classmethod\\ndefget_note(cls) -> str | None:\\n\"\"\"Get a note about the node to render on mermaid charts.\\n  By default, this returns a note only if [`docstring_notes`][pydantic_graph.nodes.BaseNode.docstring_notes]\\n  is `True`. You can override this method to customise the node notes.\\n  \"\"\"\\n  if not cls.docstring_notes:\\n    return None\\n  docstring = cls.__doc__\\n  # dataclasses get an automatic docstring which is just their signature, we don\\'t want that\\n  if docstring and is_dataclass(cls) and docstring.startswith(f\\'{cls.__name__}(\\'):\\n    docstring = None\\n  if docstring:\\n    # remove indentation from docstring\\n    importinspect\\n    docstring = inspect.cleandoc(docstring)\\n  return docstring'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 147, 'url': 'https://pydantic.com'}, page_content='get_node_def(\\n  local_ns: dictstr[, Any] | None,\\n) -> NodeDefStateT[, DepsT, NodeRunEndT]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 147, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 147, 'url': 'https://pydantic.com'}, page_content='@classmethod\\ndefget_node_def(cls, local_ns: dict[str, Any] | None) -> NodeDef[StateT, DepsT, NodeRunEndT]:\\n\"\"\"Get the node definition.\"\"\"\\n  type_hints = get_type_hints(cls.run, localns=local_ns, include_extras=True)\\n  try:\\n    return_hint = type_hints[\\'return\\']\\n  except KeyError as e:\\n    raise exceptions.GraphSetupError(f\\'Node {cls} is missing a return type hint on its `run` method\\') frome\\n  next_node_edges: dict[str, Edge] = {}\\n  end_edge: Edge | None = None\\n  returns_base_node: bool = False\\n  for return_type in _utils.get_union_args(return_hint):\\n    return_type, annotations = _utils.unpack_annotated(return_type)\\n    edge = next((a for a in annotations if isinstance(a, Edge)), Edge(None))\\n    return_type_origin = get_origin(return_type) or return_type\\n    if return_type_origin is End:\\n      end_edge = edge\\n    elif return_type_origin is BaseNode:\\n      # TODO: Should we disallow this?\\n      returns_base_node = True\\n    elif issubclass(return_type_origin, BaseNode):\\n      next_node_edges[return_type.get_id()] = edge\\n    else:\\n      raise exceptions.GraphSetupError(f\\'Invalid return type: {return_type}\\')\\n  return NodeDef(\\n    cls,\\n    cls.get_id(),\\n    cls.get_note(),\\n    next_node_edges,\\n    end_edge,\\n    returns_base_node,\\n  )'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 147, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 147, 'url': 'https://pydantic.com'}, page_content='@dataclass\\nclassEnd(Generic[RunEndT]):\\n\"\"\"Type to return from a node to signal the end of the graph.\"\"\"\\n  data: RunEndT\\n\"\"\"Data to return from the graph.\"\"\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 147, 'url': 'https://pydantic.com'}, page_content='data: RunEndT'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 147, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 147, 'url': 'https://pydantic.com'}, page_content='@dataclass\\nclassEdge:\\n\"\"\"Annotation to apply a label to an edge in a graph.\"\"\"\\n  label: str | None\\n\"\"\"Label for the edge.\"\"\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 147, 'url': 'https://pydantic.com'}, page_content='label: str | None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 147, 'url': 'https://pydantic.com'}, page_content=\"DepsT = TypeVar('DepsT', default=None, contravariant=True)\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 147, 'url': 'https://pydantic.com'}, page_content=\"RunEndT = TypeVar('RunEndT', covariant=True, default=None)\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 147, 'url': 'https://pydantic.com'}, page_content='NodeRunEndT = TypeVar(\\n  \"NodeRunEndT\", covariant=True, default=Never\\n)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 148, 'url': 'https://pydantic.com'}, page_content=\"StateT = TypeVar('StateT', default=None)\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 148, 'url': 'https://pydantic.com'}, page_content='deep_copy_state(state: StateT) -> StateT'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 148, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 148, 'url': 'https://pydantic.com'}, page_content='defdeep_copy_state(state: StateT) -> StateT:\\n\"\"\"Default method for snapshotting the state in a graph run, uses [`copy.deepcopy`][copy.deepcopy].\"\"\"\\n  if state is None:\\n    return state\\n  else:\\n    return copy.deepcopy(state)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 148, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 148, 'url': 'https://pydantic.com'}, page_content='@dataclass\\nclassNodeStep(Generic[StateT, RunEndT]):\\n\"\"\"History step describing the execution of a node in a graph.\"\"\"\\n  state: StateT\\n\"\"\"The state of the graph after the node has been run.\"\"\"\\n  node: Annotated[BaseNode[StateT, Any, RunEndT], CustomNodeSchema()]\\n\"\"\"The node that was run.\"\"\"\\n  start_ts: datetime = field(default_factory=_utils.now_utc)\\n\"\"\"The timestamp when the node started running.\"\"\"\\n  duration: float | None = None\\n\"\"\"The duration of the node run in seconds.\"\"\"\\n  kind: Literal[\\'node\\'] = \\'node\\'\\n\"\"\"The kind of history step, can be used as a discriminator when deserializing history.\"\"\"\\n  # TODO waiting for https://github.com/pydantic/pydantic/issues/11264, should be an InitVar\\n  snapshot_state: Annotated[Callable[[StateT], StateT], pydantic.Field(exclude=True, repr=False)] = field(\\n    default=deep_copy_state, repr=False\\n  )\\n\"\"\"Function to snapshot the state of the graph.\"\"\"\\n  def__post_init__(self):\\n    # Copy the state to prevent it from being modified by other code\\n    self.state = self.snapshot_state(self.state)\\n  defdata_snapshot(self) -> BaseNode[StateT, Any, RunEndT]:\\n\"\"\"Returns a deep copy of [`self.node`][pydantic_graph.state.NodeStep.node].\\n    Useful for summarizing history.\\n    \"\"\"\\n    return copy.deepcopy(self.node)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 148, 'url': 'https://pydantic.com'}, page_content='state: StateT'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 148, 'url': 'https://pydantic.com'}, page_content='node: Annotated[\\n  BaseNodeStateT[, Any, RunEndT], CustomNodeSchema()\\n]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 148, 'url': 'https://pydantic.com'}, page_content='start_ts: datetime = field(default_factory=now_utc)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 148, 'url': 'https://pydantic.com'}, page_content='duration: float | None = None'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 148, 'url': 'https://pydantic.com'}, page_content=\"kind: Literal['node'] = 'node'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 148, 'url': 'https://pydantic.com'}, page_content='snapshot_state: Annotated[\\n  Callable[StateT[], StateT],\\n  Field(exclude=True, repr=False),\\n] = field(default=deep_copy_state, repr=False)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 148, 'url': 'https://pydantic.com'}, page_content='data_snapshot() -> BaseNodeStateT[, Any, RunEndT]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 148, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 148, 'url': 'https://pydantic.com'}, page_content='defdata_snapshot(self) -> BaseNode[StateT, Any, RunEndT]:\\n\"\"\"Returns a deep copy of [`self.node`][pydantic_graph.state.NodeStep.node].\\n  Useful for summarizing history.\\n  \"\"\"\\n  return copy.deepcopy(self.node)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 149, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 149, 'url': 'https://pydantic.com'}, page_content='@dataclass\\nclassEndStep(Generic[RunEndT]):\\n\"\"\"History step describing the end of a graph run.\"\"\"\\n  result: End[RunEndT]\\n\"\"\"The result of the graph run.\"\"\"\\n  ts: datetime = field(default_factory=_utils.now_utc)\\n\"\"\"The timestamp when the graph run ended.\"\"\"\\n  kind: Literal[\\'end\\'] = \\'end\\'\\n\"\"\"The kind of history step, can be used as a discriminator when deserializing history.\"\"\"\\n  defdata_snapshot(self) -> End[RunEndT]:\\n\"\"\"Returns a deep copy of [`self.result`][pydantic_graph.state.EndStep.result].\\n    Useful for summarizing history.\\n    \"\"\"\\n    return copy.deepcopy(self.result)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 149, 'url': 'https://pydantic.com'}, page_content='result: EndRunEndT[]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 149, 'url': 'https://pydantic.com'}, page_content='ts: datetime = field(default_factory=now_utc)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 149, 'url': 'https://pydantic.com'}, page_content=\"kind: Literal['end'] = 'end'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 149, 'url': 'https://pydantic.com'}, page_content='data_snapshot() -> EndRunEndT[]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 149, 'url': 'https://pydantic.com'}, page_content=''),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 149, 'url': 'https://pydantic.com'}, page_content='defdata_snapshot(self) -> End[RunEndT]:\\n\"\"\"Returns a deep copy of [`self.result`][pydantic_graph.state.EndStep.result].\\n  Useful for summarizing history.\\n  \"\"\"\\n  return copy.deepcopy(self.result)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 149, 'url': 'https://pydantic.com'}, page_content='HistoryStep = Union[\\n  NodeStepStateT[, RunEndT], EndStepRunEndT[]\\n]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 150, 'url': 'https://pydantic.com'}, page_content=\"pipinstall'pydantic-ai[examples]'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 150, 'url': 'https://pydantic.com'}, page_content=\"uvadd'pydantic-ai[examples]'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 150, 'url': 'https://pydantic.com'}, page_content='exportOPENAI_API_KEY=your-api-key'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 150, 'url': 'https://pydantic.com'}, page_content='exportGEMINI_API_KEY=your-api-key'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 150, 'url': 'https://pydantic.com'}, page_content='python-mpydantic_ai_examples.<example_module_name>'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 150, 'url': 'https://pydantic.com'}, page_content='uvrun-mpydantic_ai_examples.<example_module_name>'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 150, 'url': 'https://pydantic.com'}, page_content='python-mpydantic_ai_examples.pydantic_model'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 150, 'url': 'https://pydantic.com'}, page_content='uvrun-mpydantic_ai_examples.pydantic_model'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 150, 'url': 'https://pydantic.com'}, page_content=\"OPENAI_API_KEY='your-api-key'\\\\\\nuvrun--with'pydantic-ai[examples]'\\\\\\n-mpydantic_ai_examples.pydantic_model\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 150, 'url': 'https://pydantic.com'}, page_content='python-mpydantic_ai_examples--copy-toexamples/'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 150, 'url': 'https://pydantic.com'}, page_content='uvrun-mpydantic_ai_examples--copy-toexamples/'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 151, 'url': 'https://pydantic.com'}, page_content='python-mpydantic_ai_examples.bank_support'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 151, 'url': 'https://pydantic.com'}, page_content='uvrun-mpydantic_ai_examples.bank_support'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 151, 'url': 'https://pydantic.com'}, page_content='fromdataclassesimport dataclass\\nfrompydanticimport BaseModel, Field\\nfrompydantic_aiimport Agent, RunContext\\n\\nclassDatabaseConn:\\n\"\"\"This is a fake database for example purposes.\\n  In reality, you\\'d be connecting to an external database\\n  (e.g. PostgreSQL) to get information about customers.\\n  \"\"\"\\n  @classmethod\\n  async defcustomer_name(cls, *, id: int) -> str | None:\\n    if id == 123:\\n      return \\'John\\'\\n  @classmethod\\n  async defcustomer_balance(cls, *, id: int, include_pending: bool) -> float:\\n    if id == 123 and include_pending:\\n      return 123.45\\n    else:\\n      raise ValueError(\\'Customer not found\\')\\n\\n@dataclass\\nclassSupportDependencies:\\n  customer_id: int\\n  db: DatabaseConn\\n\\nclassSupportResult(BaseModel):\\n  support_advice: str = Field(description=\\'Advice returned to the customer\\')\\n  block_card: bool = Field(description=\\'Whether to block their card or not\\')\\n  risk: int = Field(description=\\'Risk level of query\\', ge=0, le=10)\\n\\nsupport_agent = Agent(\\n  \\'openai:gpt-4o\\',\\n  deps_type=SupportDependencies,\\n  result_type=SupportResult,\\n  system_prompt=(\\n    \\'You are a support agent in our bank, give the \\'\\n    \\'customer support and judge the risk level of their query. \\'\\n    \"Reply using the customer\\'s name.\"\\n  ),\\n)\\n\\n@support_agent.system_prompt\\nasync defadd_customer_name(ctx: RunContext[SupportDependencies]) -> str:\\n  customer_name = await ctx.deps.db.customer_name(id=ctx.deps.customer_id)\\n  return f\"The customer\\'s name is {customer_name!r}\"\\n\\n@support_agent.tool\\nasync defcustomer_balance(\\n  ctx: RunContext[SupportDependencies], include_pending: bool\\n) -> str:\\n\"\"\"Returns the customer\\'s current account balance.\"\"\"\\n  balance = await ctx.deps.db.customer_balance(\\n    id=ctx.deps.customer_id,\\n    include_pending=include_pending,\\n  )\\n  return f\\'${balance:.2f}\\'\\n\\nif __name__ == \\'__main__\\':\\n  deps = SupportDependencies(customer_id=123, db=DatabaseConn())\\n  result = support_agent.run_sync(\\'What is my balance?\\', deps=deps)\\n  print(result.data)\\n\"\"\"\\n  support_advice=\\'Hello John, your current account balance, including pending transactions, is $123.45.\\' block_card=False risk=1\\n  \"\"\"\\n  result = support_agent.run_sync(\\'I just lost my card!\\', deps=deps)\\n  print(result.data)\\n\"\"\"\\n  support_advice=\"I\\'m sorry to hear that, John. We are temporarily blocking your card to prevent unauthorized transactions.\" block_card=True risk=8\\n  \"\"\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 152, 'url': 'https://pydantic.com'}, page_content='python-mpydantic_ai_examples.chat_app'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 152, 'url': 'https://pydantic.com'}, page_content='uvrun-mpydantic_ai_examples.chat_app'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 152, 'url': 'https://pydantic.com'}, page_content='from__future__import annotations as _annotations\\nimportasyncio\\nimportjson\\nimportsqlite3\\nfromcollections.abcimport AsyncIterator\\nfromconcurrent.futures.threadimport ThreadPoolExecutor\\nfromcontextlibimport asynccontextmanager\\nfromdataclassesimport dataclass\\nfromdatetimeimport datetime, timezone\\nfromfunctoolsimport partial\\nfrompathlibimport Path\\nfromtypingimport Annotated, Any, Callable, Literal, TypeVar\\nimportfastapi\\nimportlogfire\\nfromfastapiimport Depends, Request\\nfromfastapi.responsesimport FileResponse, Response, StreamingResponse\\nfromtyping_extensionsimport LiteralString, ParamSpec, TypedDict\\nfrompydantic_aiimport Agent\\nfrompydantic_ai.exceptionsimport UnexpectedModelBehavior\\nfrompydantic_ai.messagesimport (\\n  ModelMessage,\\n  ModelMessagesTypeAdapter,\\n  ModelRequest,\\n  ModelResponse,\\n  TextPart,\\n  UserPromptPart,\\n)\\n# \\'if-token-present\\' means nothing will be sent (and the example will work) if you don\\'t have logfire configured\\nlogfire.configure(send_to_logfire=\\'if-token-present\\')\\nagent = Agent(\\'openai:gpt-4o\\')\\nTHIS_DIR = Path(__file__).parent\\n\\n@asynccontextmanager\\nasync deflifespan(_app: fastapi.FastAPI):\\n  async with Database.connect() as db:\\n    yield {\\'db\\': db}\\n\\napp = fastapi.FastAPI(lifespan=lifespan)\\nlogfire.instrument_fastapi(app)\\n\\n@app.get(\\'/\\')\\nasync defindex() -> FileResponse:\\n  return FileResponse((THIS_DIR / \\'chat_app.html\\'), media_type=\\'text/html\\')\\n\\n@app.get(\\'/chat_app.ts\\')\\nasync defmain_ts() -> FileResponse:\\n\"\"\"Get the raw typescript code, it\\'s compiled in the browser, forgive me.\"\"\"\\n  return FileResponse((THIS_DIR / \\'chat_app.ts\\'), media_type=\\'text/plain\\')\\n\\nasync defget_db(request: Request) -> Database:\\n  return request.state.db\\n\\n@app.get(\\'/chat/\\')\\nasync defget_chat(database: Database = Depends(get_db)) -> Response:\\n  msgs = await database.get_messages()\\n  return Response(\\n    b\\'\\\\n\\'.join(json.dumps(to_chat_message(m)).encode(\\'utf-8\\') for m in msgs),\\n    media_type=\\'text/plain\\',\\n  )\\n\\nclassChatMessage(TypedDict):\\n\"\"\"Format of messages sent to the browser.\"\"\"\\n  role: Literal[\\'user\\', \\'model\\']\\n  timestamp: str\\n  content: str\\n\\ndefto_chat_message(m: ModelMessage) -> ChatMessage:\\n  first_part = m.parts[0]\\n  if isinstance(m, ModelRequest):\\n    if isinstance(first_part, UserPromptPart):\\n      return {\\n        \\'role\\': \\'user\\',\\n        \\'timestamp\\': first_part.timestamp.isoformat(),\\n        \\'content\\': first_part.content,\\n      }\\n  elif isinstance(m, ModelResponse):\\n    if isinstance(first_part, TextPart):\\n      return {\\n        \\'role\\': \\'model\\',\\n        \\'timestamp\\': m.timestamp.isoformat(),\\n        \\'content\\': first_part.content,\\n      }\\n  raise UnexpectedModelBehavior(f\\'Unexpected message type for chat app: {m}\\')\\n\\n@app.post(\\'/chat/\\')\\nasync defpost_chat(\\n  prompt: Annotated[str, fastapi.Form()], database: Database = Depends(get_db)\\n) -> StreamingResponse:\\n  async defstream_messages():\\n\"\"\"Streams new line delimited JSON `Message`s to the client.\"\"\"\\n    # stream the user prompt so that can be displayed straight away\\n    yield (\\n      json.dumps(\\n        {\\n          \\'role\\': \\'user\\',\\n          \\'timestamp\\': datetime.now(tz=timezone.utc).isoformat(),\\n          \\'content\\': prompt,\\n        }\\n      ).encode(\\'utf-8\\')\\n      + b\\'\\\\n\\'\\n    )\\n    # get the chat history so far to pass as context to the agent\\n    messages = await database.get_messages()\\n    # run the agent with the user prompt and the chat history\\n    async with agent.run_stream(prompt, message_history=messages) as result:\\n      async for text in result.stream(debounce_by=0.01):\\n        # text here is a `str` and the frontend wants\\n        # JSON encoded ModelResponse, so we create one\\n        m = ModelResponse(parts=[TextPart(text)], timestamp=result.timestamp())\\n        yield json.dumps(to_chat_message(m)).encode(\\'utf-8\\') + b\\'\\\\n\\'\\n    # add new messages (e.g. the user prompt and the agent response in this case) to the database\\n    await database.add_messages(result.new_messages_json())\\n  return StreamingResponse(stream_messages(), media_type=\\'text/plain\\')\\n\\nP = ParamSpec(\\'P\\')\\nR = TypeVar(\\'R\\')\\n\\n@dataclass\\nclassDatabase:\\n\"\"\"Rudimentary database to store chat messages in SQLite.\\n  The SQLite standard library package is synchronous, so we\\n  use a thread pool executor to run queries asynchronously.\\n  \"\"\"\\n  con: sqlite3.Connection\\n  _loop: asyncio.AbstractEventLoop\\n  _executor: ThreadPoolExecutor\\n  @classmethod\\n  @asynccontextmanager\\n  async defconnect(\\n    cls, file: Path = THIS_DIR / \\'.chat_app_messages.sqlite\\'\\n  ) -> AsyncIterator[Database]:\\n    with logfire.span(\\'connect to DB\\'):\\n      loop = asyncio.get_event_loop()\\n      executor = ThreadPoolExecutor(max_workers=1)\\n      con = await loop.run_in_executor(executor, cls._connect, file)\\n      slf = cls(con, loop, executor)\\n    try:\\n      yield slf\\n    finally:\\n      await slf._asyncify(con.close)\\n  @staticmethod\\n  def_connect(file: Path) -> sqlite3.Connection:\\n    con = sqlite3.connect(str(file))\\n    con = logfire.instrument_sqlite3(con)\\n    cur = con.cursor()\\n    cur.execute(\\n      \\'CREATE TABLE IF NOT EXISTS messages (id INT PRIMARY KEY, message_list TEXT);\\'\\n    )\\n    con.commit()\\n    return con\\n  async defadd_messages(self, messages: bytes):\\n    await self._asyncify(\\n      self._execute,\\n      \\'INSERT INTO messages (message_list) VALUES (?);\\',\\n      messages,\\n      commit=True,\\n    )\\n    await self._asyncify(self.con.commit)\\n  async defget_messages(self) -> list[ModelMessage]:\\n    c = await self._asyncify(\\n      self._execute, \\'SELECT message_list FROM messages order by id\\'\\n    )\\n    rows = await self._asyncify(c.fetchall)\\n    messages: list[ModelMessage] = []\\n    for row in rows:\\n      messages.extend(ModelMessagesTypeAdapter.validate_json(row[0]))\\n    return messages\\n  def_execute(\\n    self, sql: LiteralString, *args: Any, commit: bool = False\\n  ) -> sqlite3.Cursor:\\n    cur = self.con.cursor()\\n    cur.execute(sql, args)\\n    if commit:\\n      self.con.commit()\\n    return cur\\n  async def_asyncify(\\n    self, func: Callable[P, R], *args: P.args, **kwargs: P.kwargs\\n  ) -> R:\\n    return await self._loop.run_in_executor( # type: ignore\\n      self._executor,\\n      partial(func, **kwargs),\\n      *args, # type: ignore\\n    )\\n\\nif __name__ == \\'__main__\\':\\n  importuvicorn\\n  uvicorn.run(\\n    \\'pydantic_ai_examples.chat_app:app\\', reload=True, reload_dirs=[str(THIS_DIR)]\\n  )'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 152, 'url': 'https://pydantic.com'}, page_content='<!DOCTYPE html>\\n<html lang=\"en\">\\n<head>\\n <meta charset=\"UTF-8\">\\n <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\\n <title>Chat App</title>\\n <link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css\" rel=\"stylesheet\">\\n <style>\\nmain{\\nmax-width:700px;\\n}\\n#conversation.user::before{\\ncontent:\\'You asked: \\';\\nfont-weight:bold;\\ndisplay:block;\\n}\\n#conversation.model::before{\\ncontent:\\'AI Response: \\';\\nfont-weight:bold;\\ndisplay:block;\\n}\\n#spinner{\\nopacity:0;\\ntransition:opacity500msease-in;\\nwidth:30px;\\nheight:30px;\\nborder:3pxsolid#222;\\nborder-bottom-color:transparent;\\nborder-radius:50%;\\nanimation:rotation1slinearinfinite;\\n}\\n@keyframesrotation{\\n0%{transform:rotate(0deg);}\\n100%{transform:rotate(360deg);}\\n}\\n#spinner.active{\\nopacity:1;\\n}\\n</style>\\n</head>\\n<body>\\n <main class=\"border rounded mx-auto my-5 p-4\">\\n  <h1>Chat App</h1>\\n  <p>Ask me anything...</p>\\n  <div id=\"conversation\" class=\"px-2\"></div>\\n  <div class=\"d-flex justify-content-center mb-3\">\\n   <div id=\"spinner\"></div>\\n  </div>\\n  <form method=\"post\">\\n   <input id=\"prompt-input\" name=\"prompt\" class=\"form-control\"/>\\n   <div class=\"d-flex justify-content-end\">\\n    <button class=\"btn btn-primary mt-2\">Send</button>\\n   </div>\\n  </form>\\n  <div id=\"error\" class=\"d-none text-danger\">\\n   Error occurred, check the browser developer console for more information.\\n  </div>\\n </main>\\n</body>\\n</html>\\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/typescript/5.6.3/typescript.min.js\" crossorigin=\"anonymous\" referrerpolicy=\"no-referrer\"></script>\\n<script type=\"module\">\\n// to let me write TypeScript, without adding the burden of npm we do a dirty, non-production-ready hack\\n// and transpile the TypeScript code in the browser\\n// this is (arguably) A neat demo trick, but not suitable for production!\\nasyncfunctionloadTs(){\\nconstresponse=awaitfetch(\\'/chat_app.ts\\');\\nconsttsCode=awaitresponse.text();\\nconstjsCode=window.ts.transpile(tsCode,{target:\"es2015\"});\\nletscript=document.createElement(\\'script\\');\\nscript.type=\\'module\\';\\nscript.text=jsCode;\\ndocument.body.appendChild(script);\\n}\\nloadTs().catch((e)=>{\\nconsole.error(e);\\ndocument.getElementById(\\'error\\').classList.remove(\\'d-none\\');\\ndocument.getElementById(\\'spinner\\').classList.remove(\\'active\\');\\n});\\n</script>'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 152, 'url': 'https://pydantic.com'}, page_content=\"// BIG FAT WARNING: to avoid the complexity of npm, this typescript is compiled in the browser\\n// there's currently no static type checking\\nimport{marked}from'https://cdnjs.cloudflare.com/ajax/libs/marked/15.0.0/lib/marked.esm.js'\\nconstconvElement=document.getElementById('conversation')\\nconstpromptInput=document.getElementById('prompt-input')asHTMLInputElement\\nconstspinner=document.getElementById('spinner')\\n// stream the response and render messages as each chunk is received\\n// data is sent as newline-delimited JSON\\nasyncfunctiononFetchResponse(response:Response):Promise<void>{\\nlettext=''\\nletdecoder=newTextDecoder()\\nif(response.ok){\\nconstreader=response.body.getReader()\\nwhile(true){\\nconst{done,value}=awaitreader.read()\\nif(done){\\nbreak\\n}\\ntext+=decoder.decode(value)\\naddMessages(text)\\nspinner.classList.remove('active')\\n}\\naddMessages(text)\\npromptInput.disabled=false\\npromptInput.focus()\\n}else{\\nconsttext=awaitresponse.text()\\nconsole.error(`Unexpected response: ${response.status}`,{response,text})\\nthrownewError(`Unexpected response: ${response.status}`)\\n}\\n}\\n// The format of messages, this matches pydantic-ai both for brevity and understanding\\n// in production, you might not want to keep this format all the way to the frontend\\ninterfaceMessage{\\nrole:string\\ncontent:string\\ntimestamp:string\\n}\\n// take raw response text and render messages into the `#conversation` element\\n// Message timestamp is assumed to be a unique identifier of a message, and is used to deduplicate\\n// hence you can send data about the same message multiple times, and it will be updated\\n// instead of creating a new message elements\\nfunctionaddMessages(responseText:string){\\nconstlines=responseText.split('\\\\n')\\nconstmessages:Message[]=lines.filter(line=>line.length>1).map(j=>JSON.parse(j))\\nfor(constmessageofmessages){\\n// we use the timestamp as a crude element id\\nconst{timestamp,role,content}=message\\nconstid=`msg-${timestamp}`\\nletmsgDiv=document.getElementById(id)\\nif(!msgDiv){\\nmsgDiv=document.createElement('div')\\nmsgDiv.id=id\\nmsgDiv.title=`${role} at ${timestamp}`\\nmsgDiv.classList.add('border-top','pt-2',role)\\nconvElement.appendChild(msgDiv)\\n}\\nmsgDiv.innerHTML=marked.parse(content)\\n}\\nwindow.scrollTo({top:document.body.scrollHeight,behavior:'smooth'})\\n}\\nfunctiononError(error:any){\\nconsole.error(error)\\ndocument.getElementById('error').classList.remove('d-none')\\ndocument.getElementById('spinner').classList.remove('active')\\n}\\nasyncfunctiononSubmit(e:SubmitEvent):Promise<void>{\\ne.preventDefault()\\nspinner.classList.add('active')\\nconstbody=newFormData(e.targetasHTMLFormElement)\\npromptInput.value=''\\npromptInput.disabled=true\\nconstresponse=awaitfetch('/chat/',{method:'POST',body})\\nawaitonFetchResponse(response)\\n}\\n// call onSubmit when the form is submitted (e.g. user clicks the send button or hits Enter)\\ndocument.querySelector('form').addEventListener('submit',(e)=>onSubmit(e).catch(onError))\\n// load messages on page load\\nfetch('/chat/').then(onFetchResponse).catch(onError)\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 153, 'url': 'https://pydantic.com'}, page_content='graph TD\\n START --> search_agent(\"search agent\")\\n search_agent --> extraction_agent(\"extraction agent\")\\n extraction_agent --> search_agent\\n search_agent --> human_confirm(\"human confirm\")\\n human_confirm --> search_agent\\n search_agent --> FAILED\\n human_confirm --> find_seat_function(\"find seat function\")\\n find_seat_function --> human_seat_choice(\"human seat choice\")\\n human_seat_choice --> find_seat_agent(\"find seat agent\")\\n find_seat_agent --> find_seat_function\\n find_seat_function --> buy_flights(\"buy flights\")\\n buy_flights --> SUCCESS'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 153, 'url': 'https://pydantic.com'}, page_content='python-mpydantic_ai_examples.flight_booking'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 153, 'url': 'https://pydantic.com'}, page_content='uvrun-mpydantic_ai_examples.flight_booking'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 153, 'url': 'https://pydantic.com'}, page_content='importdatetime\\nfromdataclassesimport dataclass\\nfromtypingimport Literal\\nimportlogfire\\nfrompydanticimport BaseModel, Field\\nfromrich.promptimport Prompt\\nfrompydantic_aiimport Agent, ModelRetry, RunContext\\nfrompydantic_ai.messagesimport ModelMessage\\nfrompydantic_ai.usageimport Usage, UsageLimits\\n# \\'if-token-present\\' means nothing will be sent (and the example will work) if you don\\'t have logfire configured\\nlogfire.configure(send_to_logfire=\\'if-token-present\\')\\n\\nclassFlightDetails(BaseModel):\\n\"\"\"Details of the most suitable flight.\"\"\"\\n  flight_number: str\\n  price: int\\n  origin: str = Field(description=\\'Three-letter airport code\\')\\n  destination: str = Field(description=\\'Three-letter airport code\\')\\n  date: datetime.date\\n\\nclassNoFlightFound(BaseModel):\\n\"\"\"When no valid flight is found.\"\"\"\\n\\n@dataclass\\nclassDeps:\\n  web_page_text: str\\n  req_origin: str\\n  req_destination: str\\n  req_date: datetime.date\\n\\n# This agent is responsible for controlling the flow of the conversation.\\nsearch_agent = Agent[Deps, FlightDetails | NoFlightFound](\\n  \\'openai:gpt-4o\\',\\n  result_type=FlightDetails | NoFlightFound, # type: ignore\\n  retries=4,\\n  system_prompt=(\\n    \\'Your job is to find the cheapest flight for the user on the given date. \\'\\n  ),\\n)\\n\\n# This agent is responsible for extracting flight details from web page text.\\nextraction_agent = Agent(\\n  \\'openai:gpt-4o\\',\\n  result_type=list[FlightDetails],\\n  system_prompt=\\'Extract all the flight details from the given text.\\',\\n)\\n\\n@search_agent.tool\\nasync defextract_flights(ctx: RunContext[Deps]) -> list[FlightDetails]:\\n\"\"\"Get details of all flights.\"\"\"\\n  # we pass the usage to the search agent so requests within this agent are counted\\n  result = await extraction_agent.run(ctx.deps.web_page_text, usage=ctx.usage)\\n  logfire.info(\\'found {flight_count} flights\\', flight_count=len(result.data))\\n  return result.data\\n\\n@search_agent.result_validator\\nasync defvalidate_result(\\n  ctx: RunContext[Deps], result: FlightDetails | NoFlightFound\\n) -> FlightDetails | NoFlightFound:\\n\"\"\"Procedural validation that the flight meets the constraints.\"\"\"\\n  if isinstance(result, NoFlightFound):\\n    return result\\n  errors: list[str] = []\\n  if result.origin != ctx.deps.req_origin:\\n    errors.append(\\n      f\\'Flight should have origin {ctx.deps.req_origin}, not {result.origin}\\'\\n    )\\n  if result.destination != ctx.deps.req_destination:\\n    errors.append(\\n      f\\'Flight should have destination {ctx.deps.req_destination}, not {result.destination}\\'\\n    )\\n  if result.date != ctx.deps.req_date:\\n    errors.append(f\\'Flight should be on {ctx.deps.req_date}, not {result.date}\\')\\n  if errors:\\n    raise ModelRetry(\\'\\\\n\\'.join(errors))\\n  else:\\n    return result\\n\\nclassSeatPreference(BaseModel):\\n  row: int = Field(ge=1, le=30)\\n  seat: Literal[\\'A\\', \\'B\\', \\'C\\', \\'D\\', \\'E\\', \\'F\\']\\n\\nclassFailed(BaseModel):\\n\"\"\"Unable to extract a seat selection.\"\"\"\\n\\n# This agent is responsible for extracting the user\\'s seat selection\\nseat_preference_agent = Agent[None, SeatPreference | Failed](\\n  \\'openai:gpt-4o\\',\\n  result_type=SeatPreference | Failed, # type: ignore\\n  system_prompt=(\\n    \"Extract the user\\'s seat preference. \"\\n    \\'Seats A and F are window seats. \\'\\n    \\'Row 1 is the front row and has extra leg room. \\'\\n    \\'Rows 14, and 20 also have extra leg room. \\'\\n  ),\\n)\\n\\n# in reality this would be downloaded from a booking site,\\n# potentially using another agent to navigate the site\\nflights_web_page = \"\"\"\\n1. Flight SFO-AK123\\n- Price: $350\\n- Origin: San Francisco International Airport (SFO)\\n- Destination: Ted Stevens Anchorage International Airport (ANC)\\n- Date: January 10, 2025\\n2. Flight SFO-AK456\\n- Price: $370\\n- Origin: San Francisco International Airport (SFO)\\n- Destination: Fairbanks International Airport (FAI)\\n- Date: January 10, 2025\\n3. Flight SFO-AK789\\n- Price: $400\\n- Origin: San Francisco International Airport (SFO)\\n- Destination: Juneau International Airport (JNU)\\n- Date: January 20, 2025\\n4. Flight NYC-LA101\\n- Price: $250\\n- Origin: San Francisco International Airport (SFO)\\n- Destination: Ted Stevens Anchorage International Airport (ANC)\\n- Date: January 10, 2025\\n5. Flight CHI-MIA202\\n- Price: $200\\n- Origin: Chicago O\\'Hare International Airport (ORD)\\n- Destination: Miami International Airport (MIA)\\n- Date: January 12, 2025\\n6. Flight BOS-SEA303\\n- Price: $120\\n- Origin: Boston Logan International Airport (BOS)\\n- Destination: Ted Stevens Anchorage International Airport (ANC)\\n- Date: January 12, 2025\\n7. Flight DFW-DEN404\\n- Price: $150\\n- Origin: Dallas/Fort Worth International Airport (DFW)\\n- Destination: Denver International Airport (DEN)\\n- Date: January 10, 2025\\n8. Flight ATL-HOU505\\n- Price: $180\\n- Origin: Hartsfield-Jackson Atlanta International Airport (ATL)\\n- Destination: George Bush Intercontinental Airport (IAH)\\n- Date: January 10, 2025\\n\"\"\"\\n# restrict how many requests this app can make to the LLM\\nusage_limits = UsageLimits(request_limit=15)\\n\\nasync defmain():\\n  deps = Deps(\\n    web_page_text=flights_web_page,\\n    req_origin=\\'SFO\\',\\n    req_destination=\\'ANC\\',\\n    req_date=datetime.date(2025, 1, 10),\\n  )\\n  message_history: list[ModelMessage] | None = None\\n  usage: Usage = Usage()\\n  # run the agent until a satisfactory flight is found\\n  while True:\\n    result = await search_agent.run(\\n      f\\'Find me a flight from {deps.req_origin} to {deps.req_destination} on {deps.req_date}\\',\\n      deps=deps,\\n      usage=usage,\\n      message_history=message_history,\\n      usage_limits=usage_limits,\\n    )\\n    if isinstance(result.data, NoFlightFound):\\n      print(\\'No flight found\\')\\n      break\\n    else:\\n      flight = result.data\\n      print(f\\'Flight found: {flight}\\')\\n      answer = Prompt.ask(\\n        \\'Do you want to buy this flight, or keep searching? (buy/*search)\\',\\n        choices=[\\'buy\\', \\'search\\', \\'\\'],\\n        show_choices=False,\\n      )\\n      if answer == \\'buy\\':\\n        seat = await find_seat(usage)\\n        await buy_tickets(flight, seat)\\n        break\\n      else:\\n        message_history = result.all_messages(\\n          result_tool_return_content=\\'Please suggest another flight\\'\\n        )\\n\\nasync deffind_seat(usage: Usage) -> SeatPreference:\\n  message_history: list[ModelMessage] | None = None\\n  while True:\\n    answer = Prompt.ask(\\'What seat would you like?\\')\\n    result = await seat_preference_agent.run(\\n      answer,\\n      message_history=message_history,\\n      usage=usage,\\n      usage_limits=usage_limits,\\n    )\\n    if isinstance(result.data, SeatPreference):\\n      return result.data\\n    else:\\n      print(\\'Could not understand seat preference. Please try again.\\')\\n      message_history = result.all_messages()\\n\\nasync defbuy_tickets(flight_details: FlightDetails, seat: SeatPreference):\\n  print(f\\'Purchasing flight {flight_details=!r}{seat=!r}...\\')\\n\\nif __name__ == \\'__main__\\':\\n  importasyncio\\n  asyncio.run(main())'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 154, 'url': 'https://pydantic.com'}, page_content='python-mpydantic_ai_examples.pydantic_model'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 154, 'url': 'https://pydantic.com'}, page_content='uvrun-mpydantic_ai_examples.pydantic_model'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 154, 'url': 'https://pydantic.com'}, page_content='PYDANTIC_AI_MODEL=gemini-1.5-propython-mpydantic_ai_examples.pydantic_model'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 154, 'url': 'https://pydantic.com'}, page_content='PYDANTIC_AI_MODEL=gemini-1.5-prouvrun-mpydantic_ai_examples.pydantic_model'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 154, 'url': 'https://pydantic.com'}, page_content=\"importos\\nfromtypingimport cast\\nimportlogfire\\nfrompydanticimport BaseModel\\nfrompydantic_aiimport Agent\\nfrompydantic_ai.modelsimport KnownModelName\\n# 'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured\\nlogfire.configure(send_to_logfire='if-token-present')\\n\\nclassMyModel(BaseModel):\\n  city: str\\n  country: str\\n\\nmodel = cast(KnownModelName, os.getenv('PYDANTIC_AI_MODEL', 'openai:gpt-4o'))\\nprint(f'Using model: {model}')\\nagent = Agent(model, result_type=MyModel)\\nif __name__ == '__main__':\\n  result = agent.run_sync('The windy city in the US of A.')\\n  print(result.data)\\n  print(result.usage())\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 155, 'url': 'https://pydantic.com'}, page_content='python-mpydantic_ai_examples.question_graph'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 155, 'url': 'https://pydantic.com'}, page_content='uvrun-mpydantic_ai_examples.question_graph'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 155, 'url': 'https://pydantic.com'}, page_content=\"from__future__import annotations as _annotations\\nfromdataclassesimport dataclass, field\\nfrompathlibimport Path\\nfromtypingimport Annotated\\nimportlogfire\\nfromdevtoolsimport debug\\nfrompydantic_graphimport BaseNode, Edge, End, Graph, GraphRunContext, HistoryStep\\nfrompydantic_aiimport Agent\\nfrompydantic_ai.format_as_xmlimport format_as_xml\\nfrompydantic_ai.messagesimport ModelMessage\\n# 'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured\\nlogfire.configure(send_to_logfire='if-token-present')\\nask_agent = Agent('openai:gpt-4o', result_type=str)\\n\\n@dataclass\\nclassQuestionState:\\n  question: str | None = None\\n  ask_agent_messages: list[ModelMessage] = field(default_factory=list)\\n  evaluate_agent_messages: list[ModelMessage] = field(default_factory=list)\\n\\n@dataclass\\nclassAsk(BaseNode[QuestionState]):\\n  async defrun(self, ctx: GraphRunContext[QuestionState]) -> Answer:\\n    result = await ask_agent.run(\\n      'Ask a simple question with a single correct answer.',\\n      message_history=ctx.state.ask_agent_messages,\\n    )\\n    ctx.state.ask_agent_messages += result.all_messages()\\n    ctx.state.question = result.data\\n    return Answer()\\n\\n@dataclass\\nclassAnswer(BaseNode[QuestionState]):\\n  answer: str | None = None\\n  async defrun(self, ctx: GraphRunContext[QuestionState]) -> Evaluate:\\n    assert self.answer is not None\\n    return Evaluate(self.answer)\\n\\n@dataclass\\nclassEvaluationResult:\\n  correct: bool\\n  comment: str\\n\\nevaluate_agent = Agent(\\n  'openai:gpt-4o',\\n  result_type=EvaluationResult,\\n  system_prompt='Given a question and answer, evaluate if the answer is correct.',\\n)\\n\\n@dataclass\\nclassEvaluate(BaseNode[QuestionState]):\\n  answer: str\\n  async defrun(\\n    self,\\n    ctx: GraphRunContext[QuestionState],\\n  ) -> Congratulate | Reprimand:\\n    assert ctx.state.question is not None\\n    result = await evaluate_agent.run(\\n      format_as_xml({'question': ctx.state.question, 'answer': self.answer}),\\n      message_history=ctx.state.evaluate_agent_messages,\\n    )\\n    ctx.state.evaluate_agent_messages += result.all_messages()\\n    if result.data.correct:\\n      return Congratulate(result.data.comment)\\n    else:\\n      return Reprimand(result.data.comment)\\n\\n@dataclass\\nclassCongratulate(BaseNode[QuestionState, None, None]):\\n  comment: str\\n  async defrun(\\n    self, ctx: GraphRunContext[QuestionState]\\n  ) -> Annotated[End, Edge(label='success')]:\\n    print(f'Correct answer! {self.comment}')\\n    return End(None)\\n\\n@dataclass\\nclassReprimand(BaseNode[QuestionState]):\\n  comment: str\\n  async defrun(self, ctx: GraphRunContext[QuestionState]) -> Ask:\\n    print(f'Comment: {self.comment}')\\n    # > Comment: Vichy is no longer the capital of France.\\n    ctx.state.question = None\\n    return Ask()\\n\\nquestion_graph = Graph(\\n  nodes=(Ask, Answer, Evaluate, Congratulate, Reprimand), state_type=QuestionState\\n)\\n\\nasync defrun_as_continuous():\\n  state = QuestionState()\\n  node = Ask()\\n  history: list[HistoryStep[QuestionState, None]] = []\\n  with logfire.span('run questions graph'):\\n    while True:\\n      node = await question_graph.next(node, history, state=state)\\n      if isinstance(node, End):\\n        debug([e.data_snapshot() for e in history])\\n        break\\n      elif isinstance(node, Answer):\\n        assert state.question\\n        node.answer = input(f'{state.question} ')\\n      # otherwise just continue\\n\\nasync defrun_as_cli(answer: str | None):\\n  history_file = Path('question_graph_history.json')\\n  history = (\\n    question_graph.load_history(history_file.read_bytes())\\n    if history_file.exists()\\n    else []\\n  )\\n  if history:\\n    last = history[-1]\\n    assert last.kind == 'node', 'expected last step to be a node'\\n    state = last.state\\n    assert answer is not None, 'answer is required to continue from history'\\n    node = Answer(answer)\\n  else:\\n    state = QuestionState()\\n    node = Ask()\\n  debug(state, node)\\n  with logfire.span('run questions graph'):\\n    while True:\\n      node = await question_graph.next(node, history, state=state)\\n      if isinstance(node, End):\\n        debug([e.data_snapshot() for e in history])\\n        print('Finished!')\\n        break\\n      elif isinstance(node, Answer):\\n        print(state.question)\\n        break\\n      # otherwise just continue\\n  history_file.write_bytes(question_graph.dump_history(history, indent=2))\\n\\nif __name__ == '__main__':\\n  importasyncio\\n  importsys\\n  try:\\n    sub_command = sys.argv[1]\\n    assert sub_command in ('continuous', 'cli', 'mermaid')\\n  except (IndexError, AssertionError):\\n    print(\\n      'Usage:\\\\n'\\n      ' uv run -m pydantic_ai_examples.question_graph mermaid\\\\n'\\n      'or:\\\\n'\\n      ' uv run -m pydantic_ai_examples.question_graph continuous\\\\n'\\n      'or:\\\\n'\\n      ' uv run -m pydantic_ai_examples.question_graph cli [answer]',\\n      file=sys.stderr,\\n    )\\n    sys.exit(1)\\n  if sub_command == 'mermaid':\\n    print(question_graph.mermaid_code(start_node=Ask))\\n  elif sub_command == 'continuous':\\n    asyncio.run(run_as_continuous())\\n  else:\\n    a = sys.argv[2] if len(sys.argv) > 2 else None\\n    asyncio.run(run_as_cli(a))\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 155, 'url': 'https://pydantic.com'}, page_content='---\\ntitle: question_graph\\n---\\nstateDiagram-v2\\n [*] --> Ask\\n Ask --> Answer: ask the question\\n Answer --> Evaluate: answer the question\\n Evaluate --> Congratulate\\n Evaluate --> Castigate\\n Congratulate --> [*]: success\\n Castigate --> Ask: try again'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 156, 'url': 'https://pydantic.com'}, page_content='mkdirpostgres-data\\ndockerrun--rm\\\\\\n-ePOSTGRES_PASSWORD=postgres\\\\\\n-p54320:5432\\\\\\n-v`pwd`/postgres-data:/var/lib/postgresql/data\\\\\\npgvector/pgvector:pg17'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 156, 'url': 'https://pydantic.com'}, page_content='python-mpydantic_ai_examples.ragbuild'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 156, 'url': 'https://pydantic.com'}, page_content='uvrun-mpydantic_ai_examples.ragbuild'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 156, 'url': 'https://pydantic.com'}, page_content='python-mpydantic_ai_examples.ragsearch\"How do I configure logfire to work with FastAPI?\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 156, 'url': 'https://pydantic.com'}, page_content='uvrun-mpydantic_ai_examples.ragsearch\"How do I configure logfire to work with FastAPI?\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 156, 'url': 'https://pydantic.com'}, page_content='from__future__import annotations as _annotations\\nimportasyncio\\nimportre\\nimportsys\\nimportunicodedata\\nfromcontextlibimport asynccontextmanager\\nfromdataclassesimport dataclass\\nimportasyncpg\\nimporthttpx\\nimportlogfire\\nimportpydantic_core\\nfromopenaiimport AsyncOpenAI\\nfrompydanticimport TypeAdapter\\nfromtyping_extensionsimport AsyncGenerator\\nfrompydantic_aiimport RunContext\\nfrompydantic_ai.agentimport Agent\\n# \\'if-token-present\\' means nothing will be sent (and the example will work) if you don\\'t have logfire configured\\nlogfire.configure(send_to_logfire=\\'if-token-present\\')\\nlogfire.instrument_asyncpg()\\n\\n@dataclass\\nclassDeps:\\n  openai: AsyncOpenAI\\n  pool: asyncpg.Pool\\n\\nagent = Agent(\\'openai:gpt-4o\\', deps_type=Deps)\\n\\n@agent.tool\\nasync defretrieve(context: RunContext[Deps], search_query: str) -> str:\\n\"\"\"Retrieve documentation sections based on a search query.\\n  Args:\\n    context: The call context.\\n    search_query: The search query.\\n  \"\"\"\\n  with logfire.span(\\n    \\'create embedding for {search_query=}\\', search_query=search_query\\n  ):\\n    embedding = await context.deps.openai.embeddings.create(\\n      input=search_query,\\n      model=\\'text-embedding-3-small\\',\\n    )\\n  assert len(embedding.data) == 1, (\\n    f\\'Expected 1 embedding, got {len(embedding.data)}, doc query: {search_query!r}\\'\\n  )\\n  embedding = embedding.data[0].embedding\\n  embedding_json = pydantic_core.to_json(embedding).decode()\\n  rows = await context.deps.pool.fetch(\\n    \\'SELECT url, title, content FROM doc_sections ORDER BY embedding <-> $1 LIMIT 8\\',\\n    embedding_json,\\n  )\\n  return \\'\\\\n\\\\n\\'.join(\\n    f\\'# {row[\"title\"]}\\\\nDocumentation URL:{row[\"url\"]}\\\\n\\\\n{row[\"content\"]}\\\\n\\'\\n    for row in rows\\n  )\\n\\nasync defrun_agent(question: str):\\n\"\"\"Entry point to run the agent and perform RAG based question answering.\"\"\"\\n  openai = AsyncOpenAI()\\n  logfire.instrument_openai(openai)\\n  logfire.info(\\'Asking \"{question}\"\\', question=question)\\n  async with database_connect(False) as pool:\\n    deps = Deps(openai=openai, pool=pool)\\n    answer = await agent.run(question, deps=deps)\\n  print(answer.data)\\n\\n#######################################################\\n# The rest of this file is dedicated to preparing the #\\n# search database, and some utilities.        #\\n#######################################################\\n# JSON document from\\n# https://gist.github.com/samuelcolvin/4b5bb9bb163b1122ff17e29e48c10992\\nDOCS_JSON = (\\n  \\'https://gist.githubusercontent.com/\\'\\n  \\'samuelcolvin/4b5bb9bb163b1122ff17e29e48c10992/raw/\\'\\n  \\'80c5925c42f1442c24963aaf5eb1a324d47afe95/logfire_docs.json\\'\\n)\\n\\nasync defbuild_search_db():\\n\"\"\"Build the search database.\"\"\"\\n  async with httpx.AsyncClient() as client:\\n    response = await client.get(DOCS_JSON)\\n    response.raise_for_status()\\n  sections = sessions_ta.validate_json(response.content)\\n  openai = AsyncOpenAI()\\n  logfire.instrument_openai(openai)\\n  async with database_connect(True) as pool:\\n    with logfire.span(\\'create schema\\'):\\n      async with pool.acquire() as conn:\\n        async with conn.transaction():\\n          await conn.execute(DB_SCHEMA)\\n    sem = asyncio.Semaphore(10)\\n    async with asyncio.TaskGroup() as tg:\\n      for section in sections:\\n        tg.create_task(insert_doc_section(sem, openai, pool, section))\\n\\nasync definsert_doc_section(\\n  sem: asyncio.Semaphore,\\n  openai: AsyncOpenAI,\\n  pool: asyncpg.Pool,\\n  section: DocsSection,\\n) -> None:\\n  async with sem:\\n    url = section.url()\\n    exists = await pool.fetchval(\\'SELECT 1 FROM doc_sections WHERE url = $1\\', url)\\n    if exists:\\n      logfire.info(\\'Skipping {url=}\\', url=url)\\n      return\\n    with logfire.span(\\'create embedding for {url=}\\', url=url):\\n      embedding = await openai.embeddings.create(\\n        input=section.embedding_content(),\\n        model=\\'text-embedding-3-small\\',\\n      )\\n    assert len(embedding.data) == 1, (\\n      f\\'Expected 1 embedding, got {len(embedding.data)}, doc section: {section}\\'\\n    )\\n    embedding = embedding.data[0].embedding\\n    embedding_json = pydantic_core.to_json(embedding).decode()\\n    await pool.execute(\\n      \\'INSERT INTO doc_sections (url, title, content, embedding) VALUES ($1, $2, $3, $4)\\',\\n      url,\\n      section.title,\\n      section.content,\\n      embedding_json,\\n    )\\n\\n@dataclass\\nclassDocsSection:\\n  id: int\\n  parent: int | None\\n  path: str\\n  level: int\\n  title: str\\n  content: str\\n  defurl(self) -> str:\\n    url_path = re.sub(r\\'\\\\.md$\\', \\'\\', self.path)\\n    return (\\n      f\\'https://logfire.pydantic.dev/docs/{url_path}/#{slugify(self.title,\"-\")}\\'\\n    )\\n  defembedding_content(self) -> str:\\n    return \\'\\\\n\\\\n\\'.join((f\\'path: {self.path}\\', f\\'title: {self.title}\\', self.content))\\n\\nsessions_ta = TypeAdapter(list[DocsSection])\\n\\n# pyright: reportUnknownMemberType=false\\n# pyright: reportUnknownVariableType=false\\n@asynccontextmanager\\nasync defdatabase_connect(\\n  create_db: bool = False,\\n) -> AsyncGenerator[asyncpg.Pool, None]:\\n  server_dsn, database = (\\n    \\'postgresql://postgres:postgres@localhost:54320\\',\\n    \\'pydantic_ai_rag\\',\\n  )\\n  if create_db:\\n    with logfire.span(\\'check and create DB\\'):\\n      conn = await asyncpg.connect(server_dsn)\\n      try:\\n        db_exists = await conn.fetchval(\\n          \\'SELECT 1 FROM pg_database WHERE datname = $1\\', database\\n        )\\n        if not db_exists:\\n          await conn.execute(f\\'CREATE DATABASE {database}\\')\\n      finally:\\n        await conn.close()\\n  pool = await asyncpg.create_pool(f\\'{server_dsn}/{database}\\')\\n  try:\\n    yield pool\\n  finally:\\n    await pool.close()\\n\\nDB_SCHEMA = \"\"\"\\nCREATE EXTENSION IF NOT EXISTS vector;\\nCREATE TABLE IF NOT EXISTS doc_sections (\\n  id serial PRIMARY KEY,\\n  url text NOT NULL UNIQUE,\\n  title text NOT NULL,\\n  content text NOT NULL,\\n  -- text-embedding-3-small returns a vector of 1536 floats\\n  embedding vector(1536) NOT NULL\\n);\\nCREATE INDEX IF NOT EXISTS idx_doc_sections_embedding ON doc_sections USING hnsw (embedding vector_l2_ops);\\n\"\"\"\\n\\ndefslugify(value: str, separator: str, unicode: bool = False) -> str:\\n\"\"\"Slugify a string, to make it URL friendly.\"\"\"\\n  # Taken unchanged from https://github.com/Python-Markdown/markdown/blob/3.7/markdown/extensions/toc.py#L38\\n  if not unicode:\\n    # Replace Extended Latin characters with ASCII, i.e. `žlutý` => `zluty`\\n    value = unicodedata.normalize(\\'NFKD\\', value)\\n    value = value.encode(\\'ascii\\', \\'ignore\\').decode(\\'ascii\\')\\n  value = re.sub(r\\'[^\\\\w\\\\s-]\\', \\'\\', value).strip().lower()\\n  return re.sub(rf\\'[{separator}\\\\s]+\\', separator, value)\\n\\nif __name__ == \\'__main__\\':\\n  action = sys.argv[1] if len(sys.argv) > 1 else None\\n  if action == \\'build\\':\\n    asyncio.run(build_search_db())\\n  elif action == \\'search\\':\\n    if len(sys.argv) == 3:\\n      q = sys.argv[2]\\n    else:\\n      q = \\'How do I configure logfire to work with FastAPI?\\'\\n    asyncio.run(run_agent(q))\\n  else:\\n    print(\\n      \\'uv run --extra examples -m pydantic_ai_examples.rag build|search\\',\\n      file=sys.stderr,\\n    )\\n    sys.exit(1)'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 157, 'url': 'https://pydantic.com'}, page_content='dockerrun--rm-ePOSTGRES_PASSWORD=postgres-p54320:5432postgres'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 157, 'url': 'https://pydantic.com'}, page_content='python-mpydantic_ai_examples.sql_gen'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 157, 'url': 'https://pydantic.com'}, page_content='uvrun-mpydantic_ai_examples.sql_gen'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 157, 'url': 'https://pydantic.com'}, page_content='python-mpydantic_ai_examples.sql_gen\"find me errors\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 157, 'url': 'https://pydantic.com'}, page_content='uvrun-mpydantic_ai_examples.sql_gen\"find me errors\"'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 157, 'url': 'https://pydantic.com'}, page_content='importasyncio\\nimportsys\\nfromcollections.abcimport AsyncGenerator\\nfromcontextlibimport asynccontextmanager\\nfromdataclassesimport dataclass\\nfromdatetimeimport date\\nfromtypingimport Annotated, Any, Union\\nimportasyncpg\\nimportlogfire\\nfromannotated_typesimport MinLen\\nfromdevtoolsimport debug\\nfrompydanticimport BaseModel, Field\\nfromtyping_extensionsimport TypeAlias\\nfrompydantic_aiimport Agent, ModelRetry, RunContext\\nfrompydantic_ai.format_as_xmlimport format_as_xml\\n# \\'if-token-present\\' means nothing will be sent (and the example will work) if you don\\'t have logfire configured\\nlogfire.configure(send_to_logfire=\\'if-token-present\\')\\nlogfire.instrument_asyncpg()\\nDB_SCHEMA = \"\"\"\\nCREATE TABLE records (\\n  created_at timestamptz,\\n  start_timestamp timestamptz,\\n  end_timestamp timestamptz,\\n  trace_id text,\\n  span_id text,\\n  parent_span_id text,\\n  level log_level,\\n  span_name text,\\n  message text,\\n  attributes_json_schema text,\\n  attributes jsonb,\\n  tags text[],\\n  is_exception boolean,\\n  otel_status_message text,\\n  service_name text\\n);\\n\"\"\"\\nSQL_EXAMPLES = [\\n  {\\n    \\'request\\': \\'show me records where foobar is false\\',\\n    \\'response\\': \"SELECT * FROM records WHERE attributes->>\\'foobar\\' = false\",\\n  },\\n  {\\n    \\'request\\': \\'show me records where attributes include the key \"foobar\"\\',\\n    \\'response\\': \"SELECT * FROM records WHERE attributes ? \\'foobar\\'\",\\n  },\\n  {\\n    \\'request\\': \\'show me records from yesterday\\',\\n    \\'response\\': \"SELECT * FROM records WHERE start_timestamp::date > CURRENT_TIMESTAMP - INTERVAL \\'1 day\\'\",\\n  },\\n  {\\n    \\'request\\': \\'show me error records with the tag \"foobar\"\\',\\n    \\'response\\': \"SELECT * FROM records WHERE level = \\'error\\' and \\'foobar\\' = ANY(tags)\",\\n  },\\n]\\n\\n@dataclass\\nclassDeps:\\n  conn: asyncpg.Connection\\n\\nclassSuccess(BaseModel):\\n\"\"\"Response when SQL could be successfully generated.\"\"\"\\n  sql_query: Annotated[str, MinLen(1)]\\n  explanation: str = Field(\\n    \\'\\', description=\\'Explanation of the SQL query, as markdown\\'\\n  )\\n\\nclassInvalidRequest(BaseModel):\\n\"\"\"Response the user input didn\\'t include enough information to generate SQL.\"\"\"\\n  error_message: str\\n\\nResponse: TypeAlias = Union[Success, InvalidRequest]\\nagent: Agent[Deps, Response] = Agent(\\n  \\'google-gla:gemini-1.5-flash\\',\\n  # Type ignore while we wait for PEP-0747, nonetheless unions will work fine everywhere else\\n  result_type=Response, # type: ignore\\n  deps_type=Deps,\\n)\\n\\n@agent.system_prompt\\nasync defsystem_prompt() -> str:\\n  return f\"\"\"\\\\\\nGiven the following PostgreSQL table of records, your job is to\\nwrite a SQL query that suits the user\\'s request.\\nDatabase schema:\\n{DB_SCHEMA}\\ntoday\\'s date = {date.today()}\\n{format_as_xml(SQL_EXAMPLES)}\\n\"\"\"\\n\\n@agent.result_validator\\nasync defvalidate_result(ctx: RunContext[Deps], result: Response) -> Response:\\n  if isinstance(result, InvalidRequest):\\n    return result\\n  # gemini often adds extraneous backslashes to SQL\\n  result.sql_query = result.sql_query.replace(\\'\\\\\\\\\\', \\'\\')\\n  if not result.sql_query.upper().startswith(\\'SELECT\\'):\\n    raise ModelRetry(\\'Please create a SELECT query\\')\\n  try:\\n    await ctx.deps.conn.execute(f\\'EXPLAIN {result.sql_query}\\')\\n  except asyncpg.exceptions.PostgresError as e:\\n    raise ModelRetry(f\\'Invalid query: {e}\\') frome\\n  else:\\n    return result\\n\\nasync defmain():\\n  if len(sys.argv) == 1:\\n    prompt = \\'show me logs from yesterday, with level \"error\"\\'\\n  else:\\n    prompt = sys.argv[1]\\n  async with database_connect(\\n    \\'postgresql://postgres:postgres@localhost:54320\\', \\'pydantic_ai_sql_gen\\'\\n  ) as conn:\\n    deps = Deps(conn)\\n    result = await agent.run(prompt, deps=deps)\\n  debug(result.data)\\n\\n# pyright: reportUnknownMemberType=false\\n# pyright: reportUnknownVariableType=false\\n@asynccontextmanager\\nasync defdatabase_connect(server_dsn: str, database: str) -> AsyncGenerator[Any, None]:\\n  with logfire.span(\\'check and create DB\\'):\\n    conn = await asyncpg.connect(server_dsn)\\n    try:\\n      db_exists = await conn.fetchval(\\n        \\'SELECT 1 FROM pg_database WHERE datname = $1\\', database\\n      )\\n      if not db_exists:\\n        await conn.execute(f\\'CREATE DATABASE {database}\\')\\n    finally:\\n      await conn.close()\\n  conn = await asyncpg.connect(f\\'{server_dsn}/{database}\\')\\n  try:\\n    with logfire.span(\\'create schema\\'):\\n      async with conn.transaction():\\n        if not db_exists:\\n          await conn.execute(\\n            \"CREATE TYPE log_level AS ENUM (\\'debug\\', \\'info\\', \\'warning\\', \\'error\\', \\'critical\\')\"\\n          )\\n        await conn.execute(DB_SCHEMA)\\n    yield conn\\n  finally:\\n    await conn.close()\\n\\nif __name__ == \\'__main__\\':\\n  asyncio.run(main())'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 158, 'url': 'https://pydantic.com'}, page_content='python-mpydantic_ai_examples.stream_markdown'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 158, 'url': 'https://pydantic.com'}, page_content='uvrun-mpydantic_ai_examples.stream_markdown'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 158, 'url': 'https://pydantic.com'}, page_content='importasyncio\\nimportos\\nimportlogfire\\nfromrich.consoleimport Console, ConsoleOptions, RenderResult\\nfromrich.liveimport Live\\nfromrich.markdownimport CodeBlock, Markdown\\nfromrich.syntaximport Syntax\\nfromrich.textimport Text\\nfrompydantic_aiimport Agent\\nfrompydantic_ai.modelsimport KnownModelName\\n# \\'if-token-present\\' means nothing will be sent (and the example will work) if you don\\'t have logfire configured\\nlogfire.configure(send_to_logfire=\\'if-token-present\\')\\nagent = Agent()\\n# models to try, and the appropriate env var\\nmodels: list[tuple[KnownModelName, str]] = [\\n  (\\'google-gla:gemini-1.5-flash\\', \\'GEMINI_API_KEY\\'),\\n  (\\'openai:gpt-4o-mini\\', \\'OPENAI_API_KEY\\'),\\n  (\\'groq:llama-3.3-70b-versatile\\', \\'GROQ_API_KEY\\'),\\n]\\n\\nasync defmain():\\n  prettier_code_blocks()\\n  console = Console()\\n  prompt = \\'Show me a short example of using Pydantic.\\'\\n  console.log(f\\'Asking: {prompt}...\\', style=\\'cyan\\')\\n  for model, env_var in models:\\n    if env_var in os.environ:\\n      console.log(f\\'Using model: {model}\\')\\n      with Live(\\'\\', console=console, vertical_overflow=\\'visible\\') as live:\\n        async with agent.run_stream(prompt, model=model) as result:\\n          async for message in result.stream():\\n            live.update(Markdown(message))\\n      console.log(result.usage())\\n    else:\\n      console.log(f\\'{model} requires {env_var} to be set.\\')\\n\\ndefprettier_code_blocks():\\n\"\"\"Make rich code blocks prettier and easier to copy.\\n  From https://github.com/samuelcolvin/aicli/blob/v0.8.0/samuelcolvin_aicli.py#L22\\n  \"\"\"\\n  classSimpleCodeBlock(CodeBlock):\\n    def__rich_console__(\\n      self, console: Console, options: ConsoleOptions\\n    ) -> RenderResult:\\n      code = str(self.text).rstrip()\\n      yield Text(self.lexer_name, style=\\'dim\\')\\n      yield Syntax(\\n        code,\\n        self.lexer_name,\\n        theme=self.theme,\\n        background_color=\\'default\\',\\n        word_wrap=True,\\n      )\\n      yield Text(f\\'/{self.lexer_name}\\', style=\\'dim\\')\\n  Markdown.elements[\\'fence\\'] = SimpleCodeBlock\\n\\nif __name__ == \\'__main__\\':\\n  asyncio.run(main())'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 159, 'url': 'https://pydantic.com'}, page_content='python-mpydantic_ai_examples.stream_whales'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 159, 'url': 'https://pydantic.com'}, page_content='uvrun-mpydantic_ai_examples.stream_whales'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 159, 'url': 'https://pydantic.com'}, page_content='fromtypingimport Annotated\\nimportlogfire\\nfrompydanticimport Field, ValidationError\\nfromrich.consoleimport Console\\nfromrich.liveimport Live\\nfromrich.tableimport Table\\nfromtyping_extensionsimport NotRequired, TypedDict\\nfrompydantic_aiimport Agent\\n# \\'if-token-present\\' means nothing will be sent (and the example will work) if you don\\'t have logfire configured\\nlogfire.configure(send_to_logfire=\\'if-token-present\\')\\n\\nclassWhale(TypedDict):\\n  name: str\\n  length: Annotated[\\n    float, Field(description=\\'Average length of an adult whale in meters.\\')\\n  ]\\n  weight: NotRequired[\\n    Annotated[\\n      float,\\n      Field(description=\\'Average weight of an adult whale in kilograms.\\', ge=50),\\n    ]\\n  ]\\n  ocean: NotRequired[str]\\n  description: NotRequired[Annotated[str, Field(description=\\'Short Description\\')]]\\n\\nagent = Agent(\\'openai:gpt-4\\', result_type=list[Whale])\\n\\nasync defmain():\\n  console = Console()\\n  with Live(\\'\\\\n\\' * 36, console=console) as live:\\n    console.print(\\'Requesting data...\\', style=\\'cyan\\')\\n    async with agent.run_stream(\\n      \\'Generate me details of 5 species of Whale.\\'\\n    ) as result:\\n      console.print(\\'Response:\\', style=\\'green\\')\\n      async for message, last in result.stream_structured(debounce_by=0.01):\\n        try:\\n          whales = await result.validate_structured_result(\\n            message, allow_partial=not last\\n          )\\n        except ValidationError as exc:\\n          if all(\\n            e[\\'type\\'] == \\'missing\\' and e[\\'loc\\'] == (\\'response\\',)\\n            for e in exc.errors()\\n          ):\\n            continue\\n          else:\\n            raise\\n        table = Table(\\n          title=\\'Species of Whale\\',\\n          caption=\\'Streaming Structured responses from GPT-4\\',\\n          width=120,\\n        )\\n        table.add_column(\\'ID\\', justify=\\'right\\')\\n        table.add_column(\\'Name\\')\\n        table.add_column(\\'Avg. Length (m)\\', justify=\\'right\\')\\n        table.add_column(\\'Avg. Weight (kg)\\', justify=\\'right\\')\\n        table.add_column(\\'Ocean\\')\\n        table.add_column(\\'Description\\', justify=\\'right\\')\\n        for wid, whale in enumerate(whales, start=1):\\n          table.add_row(\\n            str(wid),\\n            whale[\\'name\\'],\\n            f\\'{whale[\"length\"]:0.0f}\\',\\n            f\\'{w:0.0f}\\' if (w := whale.get(\\'weight\\')) else \\'…\\',\\n            whale.get(\\'ocean\\') or \\'…\\',\\n            whale.get(\\'description\\') or \\'…\\',\\n          )\\n        live.update(table)\\n\\nif __name__ == \\'__main__\\':\\n  importasyncio\\n  asyncio.run(main())'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 160, 'url': 'https://pydantic.com'}, page_content='python-mpydantic_ai_examples.weather_agent'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 160, 'url': 'https://pydantic.com'}, page_content='uvrun-mpydantic_ai_examples.weather_agent'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 160, 'url': 'https://pydantic.com'}, page_content='from__future__import annotations as _annotations\\nimportasyncio\\nimportos\\nfromdataclassesimport dataclass\\nfromtypingimport Any\\nimportlogfire\\nfromdevtoolsimport debug\\nfromhttpximport AsyncClient\\nfrompydantic_aiimport Agent, ModelRetry, RunContext\\n# \\'if-token-present\\' means nothing will be sent (and the example will work) if you don\\'t have logfire configured\\nlogfire.configure(send_to_logfire=\\'if-token-present\\')\\n\\n@dataclass\\nclassDeps:\\n  client: AsyncClient\\n  weather_api_key: str | None\\n  geo_api_key: str | None\\n\\nweather_agent = Agent(\\n  \\'openai:gpt-4o\\',\\n  # \\'Be concise, reply with one sentence.\\' is enough for some models (like openai) to use\\n  # the below tools appropriately, but others like anthropic and gemini require a bit more direction.\\n  system_prompt=(\\n    \\'Be concise, reply with one sentence.\\'\\n    \\'Use the `get_lat_lng` tool to get the latitude and longitude of the locations, \\'\\n    \\'then use the `get_weather` tool to get the weather.\\'\\n  ),\\n  deps_type=Deps,\\n  retries=2,\\n)\\n\\n@weather_agent.tool\\nasync defget_lat_lng(\\n  ctx: RunContext[Deps], location_description: str\\n) -> dict[str, float]:\\n\"\"\"Get the latitude and longitude of a location.\\n  Args:\\n    ctx: The context.\\n    location_description: A description of a location.\\n  \"\"\"\\n  if ctx.deps.geo_api_key is None:\\n    # if no API key is provided, return a dummy response (London)\\n    return {\\'lat\\': 51.1, \\'lng\\': -0.1}\\n  params = {\\n    \\'q\\': location_description,\\n    \\'api_key\\': ctx.deps.geo_api_key,\\n  }\\n  with logfire.span(\\'calling geocode API\\', params=params) as span:\\n    r = await ctx.deps.client.get(\\'https://geocode.maps.co/search\\', params=params)\\n    r.raise_for_status()\\n    data = r.json()\\n    span.set_attribute(\\'response\\', data)\\n  if data:\\n    return {\\'lat\\': data[0][\\'lat\\'], \\'lng\\': data[0][\\'lon\\']}\\n  else:\\n    raise ModelRetry(\\'Could not find the location\\')\\n\\n@weather_agent.tool\\nasync defget_weather(ctx: RunContext[Deps], lat: float, lng: float) -> dict[str, Any]:\\n\"\"\"Get the weather at a location.\\n  Args:\\n    ctx: The context.\\n    lat: Latitude of the location.\\n    lng: Longitude of the location.\\n  \"\"\"\\n  if ctx.deps.weather_api_key is None:\\n    # if no API key is provided, return a dummy response\\n    return {\\'temperature\\': \\'21 °C\\', \\'description\\': \\'Sunny\\'}\\n  params = {\\n    \\'apikey\\': ctx.deps.weather_api_key,\\n    \\'location\\': f\\'{lat},{lng}\\',\\n    \\'units\\': \\'metric\\',\\n  }\\n  with logfire.span(\\'calling weather API\\', params=params) as span:\\n    r = await ctx.deps.client.get(\\n      \\'https://api.tomorrow.io/v4/weather/realtime\\', params=params\\n    )\\n    r.raise_for_status()\\n    data = r.json()\\n    span.set_attribute(\\'response\\', data)\\n  values = data[\\'data\\'][\\'values\\']\\n  # https://docs.tomorrow.io/reference/data-layers-weather-codes\\n  code_lookup = {\\n    1000: \\'Clear, Sunny\\',\\n    1100: \\'Mostly Clear\\',\\n    1101: \\'Partly Cloudy\\',\\n    1102: \\'Mostly Cloudy\\',\\n    1001: \\'Cloudy\\',\\n    2000: \\'Fog\\',\\n    2100: \\'Light Fog\\',\\n    4000: \\'Drizzle\\',\\n    4001: \\'Rain\\',\\n    4200: \\'Light Rain\\',\\n    4201: \\'Heavy Rain\\',\\n    5000: \\'Snow\\',\\n    5001: \\'Flurries\\',\\n    5100: \\'Light Snow\\',\\n    5101: \\'Heavy Snow\\',\\n    6000: \\'Freezing Drizzle\\',\\n    6001: \\'Freezing Rain\\',\\n    6200: \\'Light Freezing Rain\\',\\n    6201: \\'Heavy Freezing Rain\\',\\n    7000: \\'Ice Pellets\\',\\n    7101: \\'Heavy Ice Pellets\\',\\n    7102: \\'Light Ice Pellets\\',\\n    8000: \\'Thunderstorm\\',\\n  }\\n  return {\\n    \\'temperature\\': f\\'{values[\"temperatureApparent\"]:0.0f}°C\\',\\n    \\'description\\': code_lookup.get(values[\\'weatherCode\\'], \\'Unknown\\'),\\n  }\\n\\nasync defmain():\\n  async with AsyncClient() as client:\\n    # create a free API key at https://www.tomorrow.io/weather-api/\\n    weather_api_key = os.getenv(\\'WEATHER_API_KEY\\')\\n    # create a free API key at https://geocode.maps.co/\\n    geo_api_key = os.getenv(\\'GEO_API_KEY\\')\\n    deps = Deps(\\n      client=client, weather_api_key=weather_api_key, geo_api_key=geo_api_key\\n    )\\n    result = await weather_agent.run(\\n      \\'What is the weather like in London and in Wiltshire?\\', deps=deps\\n    )\\n    debug(result)\\n    print(\\'Response:\\', result.data)\\n\\nif __name__ == \\'__main__\\':\\n  asyncio.run(main())'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 160, 'url': 'https://pydantic.com'}, page_content='pipinstallgradio>=5.9.0\\npython/uv-run-mpydantic_ai_examples.weather_agent_gradio'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 160, 'url': 'https://pydantic.com'}, page_content='from__future__import annotations as _annotations\\nimportjson\\nimportos\\nfromhttpximport AsyncClient\\nfrompydantic_ai.messagesimport ToolCallPart, ToolReturnPart\\nfrompydantic_ai_examples.weather_agentimport Deps, weather_agent\\ntry:\\n  importgradioasgr\\nexcept ImportError as e:\\n  raise ImportError(\\n    \\'Please install gradio with `pip install gradio`. You must use python>=3.10.\\'\\n  ) frome\\nTOOL_TO_DISPLAY_NAME = {\\'get_lat_lng\\': \\'Geocoding API\\', \\'get_weather\\': \\'Weather API\\'}\\nclient = AsyncClient()\\nweather_api_key = os.getenv(\\'WEATHER_API_KEY\\')\\n# create a free API key at https://geocode.maps.co/\\ngeo_api_key = os.getenv(\\'GEO_API_KEY\\')\\ndeps = Deps(client=client, weather_api_key=weather_api_key, geo_api_key=geo_api_key)\\n\\nasync defstream_from_agent(prompt: str, chatbot: list[dict], past_messages: list):\\n  chatbot.append({\\'role\\': \\'user\\', \\'content\\': prompt})\\n  yield gr.Textbox(interactive=False, value=\\'\\'), chatbot, gr.skip()\\n  async with weather_agent.run_stream(\\n    prompt, deps=deps, message_history=past_messages\\n  ) as result:\\n    for message in result.new_messages():\\n      for call in message.parts:\\n        if isinstance(call, ToolCallPart):\\n          call_args = (\\n            call.args.args_json\\n            if hasattr(call.args, \\'args_json\\')\\n            else json.dumps(call.args.args_dict)\\n          )\\n          metadata = {\\n            \\'title\\': f\\'🛠️ Using {TOOL_TO_DISPLAY_NAME[call.tool_name]}\\',\\n          }\\n          if call.tool_call_id is not None:\\n            metadata[\\'id\\'] = {call.tool_call_id}\\n          gr_message = {\\n            \\'role\\': \\'assistant\\',\\n            \\'content\\': \\'Parameters: \\' + call_args,\\n            \\'metadata\\': metadata,\\n          }\\n          chatbot.append(gr_message)\\n        if isinstance(call, ToolReturnPart):\\n          for gr_message in chatbot:\\n            if (\\n              gr_message.get(\\'metadata\\', {}).get(\\'id\\', \\'\\')\\n              == call.tool_call_id\\n            ):\\n              gr_message[\\'content\\'] += (\\n                f\\'\\\\nOutput: {json.dumps(call.content)}\\'\\n              )\\n        yield gr.skip(), chatbot, gr.skip()\\n    chatbot.append({\\'role\\': \\'assistant\\', \\'content\\': \\'\\'})\\n    async for message in result.stream_text():\\n      chatbot[-1][\\'content\\'] = message\\n      yield gr.skip(), chatbot, gr.skip()\\n    past_messages = result.all_messages()\\n    yield gr.Textbox(interactive=True), gr.skip(), past_messages\\n\\nasync defhandle_retry(chatbot, past_messages: list, retry_data: gr.RetryData):\\n  new_history = chatbot[: retry_data.index]\\n  previous_prompt = chatbot[retry_data.index][\\'content\\']\\n  past_messages = past_messages[: retry_data.index]\\n  async for update in stream_from_agent(previous_prompt, new_history, past_messages):\\n    yield update\\n\\ndefundo(chatbot, past_messages: list, undo_data: gr.UndoData):\\n  new_history = chatbot[: undo_data.index]\\n  past_messages = past_messages[: undo_data.index]\\n  return chatbot[undo_data.index][\\'content\\'], new_history, past_messages\\n\\ndefselect_data(message: gr.SelectData) -> str:\\n  return message.value[\\'text\\']\\n\\nwith gr.Blocks() as demo:\\n  gr.HTML(\\n\"\"\"\\n<div style=\"display: flex; justify-content: center; align-items: center; gap: 2rem; padding: 1rem; width: 100%\">\\n  <img src=\"https://ai.pydantic.dev/img/logo-white.svg\" style=\"max-width: 200px; height: auto\">\\n  <div>\\n    <h1 style=\"margin: 0 0 1rem 0\">Weather Assistant</h1>\\n    <h3 style=\"margin: 0 0 0.5rem 0\">\\n      This assistant answer your weather questions.\\n    </h3>\\n  </div>\\n</div>\\n\"\"\"\\n  )\\n  past_messages = gr.State([])\\n  chatbot = gr.Chatbot(\\n    label=\\'Packing Assistant\\',\\n    type=\\'messages\\',\\n    avatar_images=(None, \\'https://ai.pydantic.dev/img/logo-white.svg\\'),\\n    examples=[\\n      {\\'text\\': \\'What is the weather like in Miami?\\'},\\n      {\\'text\\': \\'What is the weather like in London?\\'},\\n    ],\\n  )\\n  with gr.Row():\\n    prompt = gr.Textbox(\\n      lines=1,\\n      show_label=False,\\n      placeholder=\\'What is the weather like in New York City?\\',\\n    )\\n  generation = prompt.submit(\\n    stream_from_agent,\\n    inputs=[prompt, chatbot, past_messages],\\n    outputs=[prompt, chatbot, past_messages],\\n  )\\n  chatbot.example_select(select_data, None, [prompt])\\n  chatbot.retry(\\n    handle_retry, [chatbot, past_messages], [prompt, chatbot, past_messages]\\n  )\\n  chatbot.undo(undo, [chatbot, past_messages], [prompt, chatbot, past_messages])\\n\\nif __name__ == \\'__main__\\':\\n  demo.launch()')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_code_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = []\n",
    "for i in total_code_chunks:\n",
    "    if \"weather\" in i.page_content:\n",
    "        fin.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'type': 'code', 'parent_text_id': 42, 'url': 'https://pydantic.com'}, page_content='importasyncio\\nfromdatetimeimport date\\nfrompydantic_aiimport Agent, RunContext\\nfromfake_databaseimport DatabaseConn \\nDatabaseConn is a class that holds a database connection\\n\\nfromweather_serviceimport WeatherService \\nWeatherService has methods to get weather forecasts and historic data about the weather\\n\\nweather_agent = Agent(\\n  \\'openai:gpt-4o\\',\\n  deps_type=WeatherService,\\n  system_prompt=\\'Providing a weather forecast at the locations the user provides.\\',\\n)\\n\\n@weather_agent.tool\\ndefweather_forecast(\\n  ctx: RunContext[WeatherService], location: str, forecast_date: date\\n) -> str:\\n  if forecast_date < date.today(): \\nWe need to call a different endpoint depending on whether the date is in the past or the future, you\\'ll see why this nuance is important below\\n\\n    return ctx.deps.get_historic_weather(location, forecast_date)\\n  else:\\n    return ctx.deps.get_forecast(location, forecast_date)\\n\\nasync defrun_weather_forecast( \\nThis function is the code we want to test, together with the agent it uses\\n\\n  user_prompts: list[tuple[str, int]], conn: DatabaseConn\\n):\\n\"\"\"Run weather forecast for a list of user prompts and save.\"\"\"\\n  async with WeatherService() as weather_service:\\n    async defrun_forecast(prompt: str, user_id: int):\\n      result = await weather_agent.run(prompt, deps=weather_service)\\n      await conn.store_forecast(user_id, result.data)\\n    # run all prompts in parallel\\n    await asyncio.gather(\\n      *(run_forecast(prompt, user_id) for (prompt, user_id) in user_prompts)\\n    )'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 42, 'url': 'https://pydantic.com'}, page_content='fromdatetimeimport timezone\\nimportpytest\\nfromdirty_equalsimport IsNow\\nfrompydantic_aiimport models, capture_run_messages\\nfrompydantic_ai.models.testimport TestModel\\nfrompydantic_ai.messagesimport (\\n  ModelResponse,\\n  SystemPromptPart,\\n  TextPart,\\n  ToolCallPart,\\n  ToolReturnPart,\\n  UserPromptPart,\\n  ModelRequest,\\n)\\nfromfake_databaseimport DatabaseConn\\nfromweather_appimport run_weather_forecast, weather_agent\\npytestmark = pytest.mark.anyio \\nWe\\'re using anyio to run async tests.\\n\\nmodels.ALLOW_MODEL_REQUESTS = False \\nThis is a safety measure to make sure we don\\'t accidentally make real requests to the LLM while testing, see ALLOW_MODEL_REQUESTS for more details.\\n\\nasync deftest_forecast():\\n  conn = DatabaseConn()\\n  user_id = 1\\n  with capture_run_messages() as messages:\\n    with weather_agent.override(model=TestModel()): \\nWe\\'re using Agent.override to replace the agent\\'s model with TestModel, the nice thing about override is that we can replace the model inside agent without needing access to the agent run* methods call site.\\n\\n      prompt = \\'What will the weather be like in London on 2024-11-28?\\'\\n      await run_weather_forecast([(prompt, user_id)], conn) \\nNow we call the function we want to test inside the override context manager.\\n\\n  forecast = await conn.get_forecast(user_id)\\n  assert forecast == \\'{\"weather_forecast\":\"Sunny with a chance of rain\"}\\' \\nBut default, TestModel will return a JSON string summarising the tools calls made, and what was returned. If you wanted to customise the response to something more closely aligned with the domain, you could add custom_result_text=\\'Sunny\\' when defining TestModel.\\n\\n  assert messages == [ \\nSo far we don\\'t actually know which tools were called and with which values, we can use capture_run_messages to inspect messages from the most recent run and assert the exchange between the agent and the model occurred as expected.\\n\\n    ModelRequest(\\n      parts=[\\n        SystemPromptPart(\\n          content=\\'Providing a weather forecast at the locations the user provides.\\',\\n        ),\\n        UserPromptPart(\\n          content=\\'What will the weather be like in London on 2024-11-28?\\',\\n          timestamp=IsNow(tz=timezone.utc), \\nThe IsNow helper allows us to use declarative asserts even with data which will contain timestamps that change over time.\\n\\n        ),\\n      ]\\n    ),\\n    ModelResponse(\\n      parts=[\\n        ToolCallPart(\\n          tool_name=\\'weather_forecast\\',\\n          args={\\n            \\'location\\': \\'a\\',\\n            \\'forecast_date\\': \\'2024-01-01\\', \\nTestModel isn\\'t doing anything clever to extract values from the prompt, so these values are hardcoded.\\n\\n          },\\n          tool_call_id=None,\\n        )\\n      ],\\n      model_name=\\'test\\',\\n      timestamp=IsNow(tz=timezone.utc),\\n    ),\\n    ModelRequest(\\n      parts=[\\n        ToolReturnPart(\\n          tool_name=\\'weather_forecast\\',\\n          content=\\'Sunny with a chance of rain\\',\\n          tool_call_id=None,\\n          timestamp=IsNow(tz=timezone.utc),\\n        ),\\n      ],\\n    ),\\n    ModelResponse(\\n      parts=[\\n        TextPart(\\n          content=\\'{\"weather_forecast\":\"Sunny with a chance of rain\"}\\',\\n        )\\n      ],\\n      model_name=\\'test\\',\\n      timestamp=IsNow(tz=timezone.utc),\\n    ),\\n  ]'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 42, 'url': 'https://pydantic.com'}, page_content=\"importre\\nimportpytest\\nfrompydantic_aiimport models\\nfrompydantic_ai.messagesimport (\\n  ModelMessage,\\n  ModelResponse,\\n  TextPart,\\n  ToolCallPart,\\n)\\nfrompydantic_ai.models.functionimport AgentInfo, FunctionModel\\nfromfake_databaseimport DatabaseConn\\nfromweather_appimport run_weather_forecast, weather_agent\\npytestmark = pytest.mark.anyio\\nmodels.ALLOW_MODEL_REQUESTS = False\\n\\ndefcall_weather_forecast( \\nWe define a function call_weather_forecast that will be called by FunctionModel in place of the LLM, this function has access to the list of ModelMessages that make up the run, and AgentInfo which contains information about the agent and the function tools and return tools.\\n\\n  messages: list[ModelMessage], info: AgentInfo\\n) -> ModelResponse:\\n  if len(messages) == 1:\\n    # first call, call the weather forecast tool\\n    user_prompt = messages[0].parts[-1]\\n    m = re.search(r'\\\\d{4}-\\\\d{2}-\\\\d{2}', user_prompt.content)\\n    assert m is not None\\n    args = {'location': 'London', 'forecast_date': m.group()} \\nOur function is slightly intelligent in that it tries to extract a date from the prompt, but just hard codes the location.\\n\\n    return ModelResponse(parts=[ToolCallPart('weather_forecast', args)])\\n  else:\\n    # second call, return the forecast\\n    msg = messages[-1].parts[0]\\n    assert msg.part_kind == 'tool-return'\\n    return ModelResponse(parts=[TextPart(f'The forecast is: {msg.content}')])\\n\\nasync deftest_forecast_future():\\n  conn = DatabaseConn()\\n  user_id = 1\\n  with weather_agent.override(model=FunctionModel(call_weather_forecast)): \\nWe use FunctionModel to replace the agent's model with our custom function.\\n\\n    prompt = 'What will the weather be like in London on 2032-01-01?'\\n    await run_weather_forecast([(prompt, user_id)], conn)\\n  forecast = await conn.get_forecast(user_id)\\n  assert forecast == 'The forecast is: Rainy with a chance of sun'\"),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 43, 'url': 'https://pydantic.com'}, page_content='importpytest\\nfromweather_appimport weather_agent\\nfrompydantic_ai.models.testimport TestModel\\n\\n@pytest.fixture\\ndefoverride_weather_agent():\\n  with weather_agent.override(model=TestModel()):\\n    yield\\n\\nasync deftest_forecast(override_weather_agent: None):\\n  ...\\n  # test code here'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 160, 'url': 'https://pydantic.com'}, page_content='python-mpydantic_ai_examples.weather_agent'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 160, 'url': 'https://pydantic.com'}, page_content='uvrun-mpydantic_ai_examples.weather_agent'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 160, 'url': 'https://pydantic.com'}, page_content='from__future__import annotations as _annotations\\nimportasyncio\\nimportos\\nfromdataclassesimport dataclass\\nfromtypingimport Any\\nimportlogfire\\nfromdevtoolsimport debug\\nfromhttpximport AsyncClient\\nfrompydantic_aiimport Agent, ModelRetry, RunContext\\n# \\'if-token-present\\' means nothing will be sent (and the example will work) if you don\\'t have logfire configured\\nlogfire.configure(send_to_logfire=\\'if-token-present\\')\\n\\n@dataclass\\nclassDeps:\\n  client: AsyncClient\\n  weather_api_key: str | None\\n  geo_api_key: str | None\\n\\nweather_agent = Agent(\\n  \\'openai:gpt-4o\\',\\n  # \\'Be concise, reply with one sentence.\\' is enough for some models (like openai) to use\\n  # the below tools appropriately, but others like anthropic and gemini require a bit more direction.\\n  system_prompt=(\\n    \\'Be concise, reply with one sentence.\\'\\n    \\'Use the `get_lat_lng` tool to get the latitude and longitude of the locations, \\'\\n    \\'then use the `get_weather` tool to get the weather.\\'\\n  ),\\n  deps_type=Deps,\\n  retries=2,\\n)\\n\\n@weather_agent.tool\\nasync defget_lat_lng(\\n  ctx: RunContext[Deps], location_description: str\\n) -> dict[str, float]:\\n\"\"\"Get the latitude and longitude of a location.\\n  Args:\\n    ctx: The context.\\n    location_description: A description of a location.\\n  \"\"\"\\n  if ctx.deps.geo_api_key is None:\\n    # if no API key is provided, return a dummy response (London)\\n    return {\\'lat\\': 51.1, \\'lng\\': -0.1}\\n  params = {\\n    \\'q\\': location_description,\\n    \\'api_key\\': ctx.deps.geo_api_key,\\n  }\\n  with logfire.span(\\'calling geocode API\\', params=params) as span:\\n    r = await ctx.deps.client.get(\\'https://geocode.maps.co/search\\', params=params)\\n    r.raise_for_status()\\n    data = r.json()\\n    span.set_attribute(\\'response\\', data)\\n  if data:\\n    return {\\'lat\\': data[0][\\'lat\\'], \\'lng\\': data[0][\\'lon\\']}\\n  else:\\n    raise ModelRetry(\\'Could not find the location\\')\\n\\n@weather_agent.tool\\nasync defget_weather(ctx: RunContext[Deps], lat: float, lng: float) -> dict[str, Any]:\\n\"\"\"Get the weather at a location.\\n  Args:\\n    ctx: The context.\\n    lat: Latitude of the location.\\n    lng: Longitude of the location.\\n  \"\"\"\\n  if ctx.deps.weather_api_key is None:\\n    # if no API key is provided, return a dummy response\\n    return {\\'temperature\\': \\'21 °C\\', \\'description\\': \\'Sunny\\'}\\n  params = {\\n    \\'apikey\\': ctx.deps.weather_api_key,\\n    \\'location\\': f\\'{lat},{lng}\\',\\n    \\'units\\': \\'metric\\',\\n  }\\n  with logfire.span(\\'calling weather API\\', params=params) as span:\\n    r = await ctx.deps.client.get(\\n      \\'https://api.tomorrow.io/v4/weather/realtime\\', params=params\\n    )\\n    r.raise_for_status()\\n    data = r.json()\\n    span.set_attribute(\\'response\\', data)\\n  values = data[\\'data\\'][\\'values\\']\\n  # https://docs.tomorrow.io/reference/data-layers-weather-codes\\n  code_lookup = {\\n    1000: \\'Clear, Sunny\\',\\n    1100: \\'Mostly Clear\\',\\n    1101: \\'Partly Cloudy\\',\\n    1102: \\'Mostly Cloudy\\',\\n    1001: \\'Cloudy\\',\\n    2000: \\'Fog\\',\\n    2100: \\'Light Fog\\',\\n    4000: \\'Drizzle\\',\\n    4001: \\'Rain\\',\\n    4200: \\'Light Rain\\',\\n    4201: \\'Heavy Rain\\',\\n    5000: \\'Snow\\',\\n    5001: \\'Flurries\\',\\n    5100: \\'Light Snow\\',\\n    5101: \\'Heavy Snow\\',\\n    6000: \\'Freezing Drizzle\\',\\n    6001: \\'Freezing Rain\\',\\n    6200: \\'Light Freezing Rain\\',\\n    6201: \\'Heavy Freezing Rain\\',\\n    7000: \\'Ice Pellets\\',\\n    7101: \\'Heavy Ice Pellets\\',\\n    7102: \\'Light Ice Pellets\\',\\n    8000: \\'Thunderstorm\\',\\n  }\\n  return {\\n    \\'temperature\\': f\\'{values[\"temperatureApparent\"]:0.0f}°C\\',\\n    \\'description\\': code_lookup.get(values[\\'weatherCode\\'], \\'Unknown\\'),\\n  }\\n\\nasync defmain():\\n  async with AsyncClient() as client:\\n    # create a free API key at https://www.tomorrow.io/weather-api/\\n    weather_api_key = os.getenv(\\'WEATHER_API_KEY\\')\\n    # create a free API key at https://geocode.maps.co/\\n    geo_api_key = os.getenv(\\'GEO_API_KEY\\')\\n    deps = Deps(\\n      client=client, weather_api_key=weather_api_key, geo_api_key=geo_api_key\\n    )\\n    result = await weather_agent.run(\\n      \\'What is the weather like in London and in Wiltshire?\\', deps=deps\\n    )\\n    debug(result)\\n    print(\\'Response:\\', result.data)\\n\\nif __name__ == \\'__main__\\':\\n  asyncio.run(main())'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 160, 'url': 'https://pydantic.com'}, page_content='pipinstallgradio>=5.9.0\\npython/uv-run-mpydantic_ai_examples.weather_agent_gradio'),\n",
       " Document(metadata={'type': 'code', 'parent_text_id': 160, 'url': 'https://pydantic.com'}, page_content='from__future__import annotations as _annotations\\nimportjson\\nimportos\\nfromhttpximport AsyncClient\\nfrompydantic_ai.messagesimport ToolCallPart, ToolReturnPart\\nfrompydantic_ai_examples.weather_agentimport Deps, weather_agent\\ntry:\\n  importgradioasgr\\nexcept ImportError as e:\\n  raise ImportError(\\n    \\'Please install gradio with `pip install gradio`. You must use python>=3.10.\\'\\n  ) frome\\nTOOL_TO_DISPLAY_NAME = {\\'get_lat_lng\\': \\'Geocoding API\\', \\'get_weather\\': \\'Weather API\\'}\\nclient = AsyncClient()\\nweather_api_key = os.getenv(\\'WEATHER_API_KEY\\')\\n# create a free API key at https://geocode.maps.co/\\ngeo_api_key = os.getenv(\\'GEO_API_KEY\\')\\ndeps = Deps(client=client, weather_api_key=weather_api_key, geo_api_key=geo_api_key)\\n\\nasync defstream_from_agent(prompt: str, chatbot: list[dict], past_messages: list):\\n  chatbot.append({\\'role\\': \\'user\\', \\'content\\': prompt})\\n  yield gr.Textbox(interactive=False, value=\\'\\'), chatbot, gr.skip()\\n  async with weather_agent.run_stream(\\n    prompt, deps=deps, message_history=past_messages\\n  ) as result:\\n    for message in result.new_messages():\\n      for call in message.parts:\\n        if isinstance(call, ToolCallPart):\\n          call_args = (\\n            call.args.args_json\\n            if hasattr(call.args, \\'args_json\\')\\n            else json.dumps(call.args.args_dict)\\n          )\\n          metadata = {\\n            \\'title\\': f\\'🛠️ Using {TOOL_TO_DISPLAY_NAME[call.tool_name]}\\',\\n          }\\n          if call.tool_call_id is not None:\\n            metadata[\\'id\\'] = {call.tool_call_id}\\n          gr_message = {\\n            \\'role\\': \\'assistant\\',\\n            \\'content\\': \\'Parameters: \\' + call_args,\\n            \\'metadata\\': metadata,\\n          }\\n          chatbot.append(gr_message)\\n        if isinstance(call, ToolReturnPart):\\n          for gr_message in chatbot:\\n            if (\\n              gr_message.get(\\'metadata\\', {}).get(\\'id\\', \\'\\')\\n              == call.tool_call_id\\n            ):\\n              gr_message[\\'content\\'] += (\\n                f\\'\\\\nOutput: {json.dumps(call.content)}\\'\\n              )\\n        yield gr.skip(), chatbot, gr.skip()\\n    chatbot.append({\\'role\\': \\'assistant\\', \\'content\\': \\'\\'})\\n    async for message in result.stream_text():\\n      chatbot[-1][\\'content\\'] = message\\n      yield gr.skip(), chatbot, gr.skip()\\n    past_messages = result.all_messages()\\n    yield gr.Textbox(interactive=True), gr.skip(), past_messages\\n\\nasync defhandle_retry(chatbot, past_messages: list, retry_data: gr.RetryData):\\n  new_history = chatbot[: retry_data.index]\\n  previous_prompt = chatbot[retry_data.index][\\'content\\']\\n  past_messages = past_messages[: retry_data.index]\\n  async for update in stream_from_agent(previous_prompt, new_history, past_messages):\\n    yield update\\n\\ndefundo(chatbot, past_messages: list, undo_data: gr.UndoData):\\n  new_history = chatbot[: undo_data.index]\\n  past_messages = past_messages[: undo_data.index]\\n  return chatbot[undo_data.index][\\'content\\'], new_history, past_messages\\n\\ndefselect_data(message: gr.SelectData) -> str:\\n  return message.value[\\'text\\']\\n\\nwith gr.Blocks() as demo:\\n  gr.HTML(\\n\"\"\"\\n<div style=\"display: flex; justify-content: center; align-items: center; gap: 2rem; padding: 1rem; width: 100%\">\\n  <img src=\"https://ai.pydantic.dev/img/logo-white.svg\" style=\"max-width: 200px; height: auto\">\\n  <div>\\n    <h1 style=\"margin: 0 0 1rem 0\">Weather Assistant</h1>\\n    <h3 style=\"margin: 0 0 0.5rem 0\">\\n      This assistant answer your weather questions.\\n    </h3>\\n  </div>\\n</div>\\n\"\"\"\\n  )\\n  past_messages = gr.State([])\\n  chatbot = gr.Chatbot(\\n    label=\\'Packing Assistant\\',\\n    type=\\'messages\\',\\n    avatar_images=(None, \\'https://ai.pydantic.dev/img/logo-white.svg\\'),\\n    examples=[\\n      {\\'text\\': \\'What is the weather like in Miami?\\'},\\n      {\\'text\\': \\'What is the weather like in London?\\'},\\n    ],\\n  )\\n  with gr.Row():\\n    prompt = gr.Textbox(\\n      lines=1,\\n      show_label=False,\\n      placeholder=\\'What is the weather like in New York City?\\',\\n    )\\n  generation = prompt.submit(\\n    stream_from_agent,\\n    inputs=[prompt, chatbot, past_messages],\\n    outputs=[prompt, chatbot, past_messages],\\n  )\\n  chatbot.example_select(select_data, None, [prompt])\\n  chatbot.retry(\\n    handle_retry, [chatbot, past_messages], [prompt, chatbot, past_messages]\\n  )\\n  chatbot.undo(undo, [chatbot, past_messages], [prompt, chatbot, past_messages])\\n\\nif __name__ == \\'__main__\\':\\n  demo.launch()')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-huggingface\n",
      "  Using cached langchain_huggingface-0.1.2-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting langchain-chroma\n",
      "  Using cached langchain_chroma-0.2.2-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from langchain-huggingface) (0.30.0)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from langchain-huggingface) (0.3.49)\n",
      "Collecting sentence-transformers>=2.6.0 (from langchain-huggingface)\n",
      "  Using cached sentence_transformers-4.0.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from langchain-huggingface) (0.21.1)\n",
      "Requirement already satisfied: transformers>=4.39.0 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from langchain-huggingface) (4.50.3)\n",
      "Collecting numpy<2.0.0,>=1.26.2 (from langchain-chroma)\n",
      "  Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "Collecting chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0 (from langchain-chroma)\n",
      "  Using cached chromadb-0.6.3-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting build>=1.0.3 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Using cached build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma) (2.11.1)\n",
      "Collecting chroma-hnswlib==0.7.6 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Using cached chroma_hnswlib-0.7.6-cp312-cp312-win_amd64.whl\n",
      "Collecting fastapi>=0.95.2 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Using cached uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting posthog>=2.4.0 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading posthog-3.23.0-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma) (4.13.0)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Using cached onnxruntime-1.21.0-cp312-cp312-win_amd64.whl.metadata (4.9 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading opentelemetry_api-1.31.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.31.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.52b1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading opentelemetry_sdk-1.31.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting pypika>=0.48.9 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Using cached PyPika-0.48.9-py2.py3-none-any.whl\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma) (4.67.1)\n",
      "Collecting overrides>=7.3.1 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Using cached overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting importlib-resources (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Using cached importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting grpcio>=1.58.0 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Using cached grpcio-1.71.0-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Using cached bcrypt-4.3.0-cp39-abi3-win_amd64.whl.metadata (10 kB)\n",
      "Collecting typer>=0.9.0 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Using cached typer-0.15.2-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Using cached kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma) (9.0.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma) (6.0.2)\n",
      "Collecting mmh3>=4.0.1 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Using cached mmh3-5.1.0-cp312-cp312-win_amd64.whl.metadata (16 kB)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma) (3.10.16)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma) (0.28.1)\n",
      "Collecting rich>=10.11.0 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2025.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (24.2)\n",
      "Requirement already satisfied: requests in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2.32.3)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.3.19)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.33)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.15.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (11.1.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from transformers>=4.39.0->langchain-huggingface) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from transformers>=4.39.0->langchain-huggingface) (0.5.3)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Using cached pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from build>=1.0.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma) (0.4.6)\n",
      "Collecting starlette<0.47.0,>=0.40.0 (from fastapi>=0.95.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Using cached starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (3.0.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma) (2.9.0.post0)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Using cached google_auth-2.38.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Using cached websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma) (2.3.0)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Using cached durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.23.0)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading protobuf-6.30.2-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: sympy in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma) (1.13.1)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Using cached Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting importlib-metadata<8.7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Using cached importlib_metadata-8.6.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading googleapis_common_protos-1.69.2-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.31.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.31.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-proto==1.31.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading opentelemetry_proto-1.31.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading protobuf-5.29.4-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.52b1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-instrumentation==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading opentelemetry_instrumentation-0.52b1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading opentelemetry_semantic_conventions-0.52b1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-util-http==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading opentelemetry_util_http-0.52b1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting wrapt<2.0.0,>=1.0.0 (from opentelemetry-instrumentation==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Using cached wrapt-1.17.2-cp312-cp312-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Using cached asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Using cached monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting distro>=1.5.0 (from posthog>=2.4.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from pydantic>=1.9->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.0 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from pydantic>=1.9->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma) (2.33.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from pydantic>=1.9->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (3.4.1)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from rich>=10.11.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma) (2.19.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (78.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma) (1.3.0)\n",
      "Collecting click>=8.0.0 (from typer>=0.9.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Using cached httptools-0.6.4-cp312-cp312-win_amd64.whl.metadata (3.7 kB)\n",
      "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Using cached watchfiles-1.0.4-cp312-cp312-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Using cached websockets-15.0.1-cp312-cp312-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (3.6.0)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Using cached cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting zipp>=3.20 (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Using cached zipp-3.21.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from anyio->httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma) (1.3.1)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.0.2)\n",
      "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Using cached pyreadline3-3.5.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Using cached langchain_huggingface-0.1.2-py3-none-any.whl (21 kB)\n",
      "Using cached langchain_chroma-0.2.2-py3-none-any.whl (11 kB)\n",
      "Using cached chromadb-0.6.3-py3-none-any.whl (611 kB)\n",
      "Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl (15.5 MB)\n",
      "Using cached sentence_transformers-4.0.1-py3-none-any.whl (340 kB)\n",
      "Using cached bcrypt-4.3.0-cp39-abi3-win_amd64.whl (152 kB)\n",
      "Using cached build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
      "Using cached grpcio-1.71.0-cp312-cp312-win_amd64.whl (4.3 MB)\n",
      "Using cached kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\n",
      "Using cached mmh3-5.1.0-cp312-cp312-win_amd64.whl (41 kB)\n",
      "Using cached onnxruntime-1.21.0-cp312-cp312-win_amd64.whl (11.8 MB)\n",
      "Downloading opentelemetry_api-1.31.1-py3-none-any.whl (65 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_grpc-1.31.1-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.31.1-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.31.1-py3-none-any.whl (55 kB)\n",
      "Downloading opentelemetry_instrumentation_fastapi-0.52b1-py3-none-any.whl (12 kB)\n",
      "Downloading opentelemetry_instrumentation-0.52b1-py3-none-any.whl (31 kB)\n",
      "Downloading opentelemetry_instrumentation_asgi-0.52b1-py3-none-any.whl (16 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.52b1-py3-none-any.whl (183 kB)\n",
      "Downloading opentelemetry_util_http-0.52b1-py3-none-any.whl (7.3 kB)\n",
      "Downloading opentelemetry_sdk-1.31.1-py3-none-any.whl (118 kB)\n",
      "Using cached overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Downloading posthog-3.23.0-py2.py3-none-any.whl (84 kB)\n",
      "Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Using cached typer-0.15.2-py3-none-any.whl (45 kB)\n",
      "Using cached uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "Using cached importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Using cached Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
      "Using cached google_auth-2.38.0-py2.py3-none-any.whl (210 kB)\n",
      "Downloading googleapis_common_protos-1.69.2-py3-none-any.whl (293 kB)\n",
      "Using cached httptools-0.6.4-cp312-cp312-win_amd64.whl (88 kB)\n",
      "Downloading importlib_metadata-8.6.1-py3-none-any.whl (26 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Downloading protobuf-5.29.4-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached starlette-0.46.1-py3-none-any.whl (71 kB)\n",
      "Using cached watchfiles-1.0.4-cp312-cp312-win_amd64.whl (285 kB)\n",
      "Using cached websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Using cached websockets-15.0.1-cp312-cp312-win_amd64.whl (176 kB)\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Using cached pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Using cached cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Using cached wrapt-1.17.2-cp312-cp312-win_amd64.whl (38 kB)\n",
      "Using cached zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Using cached pyreadline3-3.5.4-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: pypika, monotonic, flatbuffers, durationpy, zipp, wrapt, websockets, websocket-client, shellingham, python-dotenv, pyreadline3, pyproject_hooks, pyasn1, protobuf, overrides, opentelemetry-util-http, oauthlib, numpy, mmh3, mdurl, importlib-resources, httptools, grpcio, distro, click, cachetools, bcrypt, backoff, asgiref, watchfiles, uvicorn, starlette, rsa, requests-oauthlib, pyasn1-modules, posthog, opentelemetry-proto, markdown-it-py, importlib-metadata, humanfriendly, googleapis-common-protos, deprecated, chroma-hnswlib, build, rich, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, google-auth, fastapi, coloredlogs, typer, opentelemetry-semantic-conventions, onnxruntime, kubernetes, sentence-transformers, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, langchain-huggingface, opentelemetry-instrumentation-fastapi, chromadb, langchain-chroma\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.4\n",
      "    Uninstalling numpy-2.2.4:\n",
      "      Successfully uninstalled numpy-2.2.4\n",
      "Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.3.0 build-1.2.2.post1 cachetools-5.5.2 chroma-hnswlib-0.7.6 chromadb-0.6.3 click-8.1.8 coloredlogs-15.0.1 deprecated-1.2.18 distro-1.9.0 durationpy-0.9 fastapi-0.115.12 flatbuffers-25.2.10 google-auth-2.38.0 googleapis-common-protos-1.69.2 grpcio-1.71.0 httptools-0.6.4 humanfriendly-10.0 importlib-metadata-8.6.1 importlib-resources-6.5.2 kubernetes-32.0.1 langchain-chroma-0.2.2 langchain-huggingface-0.1.2 markdown-it-py-3.0.0 mdurl-0.1.2 mmh3-5.1.0 monotonic-1.6 numpy-1.26.4 oauthlib-3.2.2 onnxruntime-1.21.0 opentelemetry-api-1.31.1 opentelemetry-exporter-otlp-proto-common-1.31.1 opentelemetry-exporter-otlp-proto-grpc-1.31.1 opentelemetry-instrumentation-0.52b1 opentelemetry-instrumentation-asgi-0.52b1 opentelemetry-instrumentation-fastapi-0.52b1 opentelemetry-proto-1.31.1 opentelemetry-sdk-1.31.1 opentelemetry-semantic-conventions-0.52b1 opentelemetry-util-http-0.52b1 overrides-7.7.0 posthog-3.23.0 protobuf-5.29.4 pyasn1-0.6.1 pyasn1-modules-0.4.2 pypika-0.48.9 pyproject_hooks-1.2.0 pyreadline3-3.5.4 python-dotenv-1.1.0 requests-oauthlib-2.0.0 rich-14.0.0 rsa-4.9 sentence-transformers-4.0.1 shellingham-1.5.4 starlette-0.46.1 typer-0.15.2 uvicorn-0.34.0 watchfiles-1.0.4 websocket-client-1.8.0 websockets-15.0.1 wrapt-1.17.2 zipp-3.21.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-huggingface langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yogesh\\Documents\\Documentation RAG\\myvenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "db_dir = os.path.join(current_dir, \"db\")\n",
    "document_dir = os.path.join(current_dir, 'documents')\n",
    "persistent_directory = os.path.join(db_dir, \"chroma_db_with_metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_chroma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Chroma\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m db = \u001b[43mChroma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_text_chunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpersistent_directory\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yogesh\\Documents\\Documentation RAG\\myvenv\\Lib\\site-packages\\langchain_chroma\\vectorstores.py:1239\u001b[39m, in \u001b[36mChroma.from_documents\u001b[39m\u001b[34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[39m\n\u001b[32m   1237\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1238\u001b[39m     ids = [doc.id \u001b[38;5;28;01mif\u001b[39;00m doc.id \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(uuid.uuid4()) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m-> \u001b[39m\u001b[32m1239\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1240\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1241\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1242\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1243\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1244\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1245\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1246\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1247\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1248\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1249\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1250\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yogesh\\Documents\\Documentation RAG\\myvenv\\Lib\\site-packages\\langchain_chroma\\vectorstores.py:1192\u001b[39m, in \u001b[36mChroma.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[39m\n\u001b[32m   1184\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbatch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_batches\n\u001b[32m   1186\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m create_batches(\n\u001b[32m   1187\u001b[39m         api=chroma_collection._client,\n\u001b[32m   1188\u001b[39m         ids=ids,\n\u001b[32m   1189\u001b[39m         metadatas=metadatas,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   1190\u001b[39m         documents=texts,\n\u001b[32m   1191\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1192\u001b[39m         \u001b[43mchroma_collection\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m   1195\u001b[39m \u001b[43m            \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1198\u001b[39m     chroma_collection.add_texts(texts=texts, metadatas=metadatas, ids=ids)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yogesh\\Documents\\Documentation RAG\\myvenv\\Lib\\site-packages\\langchain_chroma\\vectorstores.py:532\u001b[39m, in \u001b[36mChroma.add_texts\u001b[39m\u001b[34m(self, texts, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m    530\u001b[39m texts = \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embedding_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[32m    534\u001b[39m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[32m    535\u001b[39m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[32m    536\u001b[39m     length_diff = \u001b[38;5;28mlen\u001b[39m(texts) - \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yogesh\\Documents\\Documentation RAG\\myvenv\\Lib\\site-packages\\langchain_huggingface\\embeddings\\huggingface.py:85\u001b[39m, in \u001b[36mHuggingFaceEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts)\u001b[39m\n\u001b[32m     83\u001b[39m     sentence_transformers.SentenceTransformer.stop_multi_process_pool(pool)\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m     89\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(embeddings, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m     93\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mExpected embeddings to be a Tensor or a numpy array, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     94\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mgot a list instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yogesh\\Documents\\Documentation RAG\\myvenv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:557\u001b[39m, in \u001b[36mSentenceTransformer.encode\u001b[39m\u001b[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[39m\n\u001b[32m    554\u001b[39m features.update(extra_features)\n\u001b[32m    556\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m     out_features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    558\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device.type == \u001b[33m\"\u001b[39m\u001b[33mhpu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    559\u001b[39m         out_features = copy.deepcopy(out_features)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yogesh\\Documents\\Documentation RAG\\myvenv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:624\u001b[39m, in \u001b[36mSentenceTransformer.forward\u001b[39m\u001b[34m(self, input, **kwargs)\u001b[39m\n\u001b[32m    622\u001b[39m     module_kwarg_keys = \u001b[38;5;28mself\u001b[39m.module_kwargs.get(module_name, [])\n\u001b[32m    623\u001b[39m     module_kwargs = {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys}\n\u001b[32m--> \u001b[39m\u001b[32m624\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yogesh\\Documents\\Documentation RAG\\myvenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yogesh\\Documents\\Documentation RAG\\myvenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yogesh\\Documents\\Documentation RAG\\myvenv\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:442\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, features, **kwargs)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Returns token_embeddings, cls_token\"\"\"\u001b[39;00m\n\u001b[32m    436\u001b[39m trans_features = {\n\u001b[32m    437\u001b[39m     key: value\n\u001b[32m    438\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features.items()\n\u001b[32m    439\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtoken_type_ids\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33minputs_embeds\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    440\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m output_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    443\u001b[39m output_tokens = output_states[\u001b[32m0\u001b[39m]\n\u001b[32m    445\u001b[39m \u001b[38;5;66;03m# If the AutoModel is wrapped with a PeftModelForFeatureExtraction, then it may have added virtual tokens\u001b[39;00m\n\u001b[32m    446\u001b[39m \u001b[38;5;66;03m# We need to extend the attention mask to include these virtual tokens, or the pooling will fail\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yogesh\\Documents\\Documentation RAG\\myvenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yogesh\\Documents\\Documentation RAG\\myvenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yogesh\\Documents\\Documentation RAG\\myvenv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:544\u001b[39m, in \u001b[36mMPNetModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[39m\n\u001b[32m    542\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m    543\u001b[39m embedding_output = \u001b[38;5;28mself\u001b[39m.embeddings(input_ids=input_ids, position_ids=position_ids, inputs_embeds=inputs_embeds)\n\u001b[32m--> \u001b[39m\u001b[32m544\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    552\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    553\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yogesh\\Documents\\Documentation RAG\\myvenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yogesh\\Documents\\Documentation RAG\\myvenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yogesh\\Documents\\Documentation RAG\\myvenv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:334\u001b[39m, in \u001b[36mMPNetEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[39m\n\u001b[32m    331\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    332\u001b[39m     all_hidden_states = all_hidden_states + (hidden_states,)\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    342\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    344\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yogesh\\Documents\\Documentation RAG\\myvenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yogesh\\Documents\\Documentation RAG\\myvenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yogesh\\Documents\\Documentation RAG\\myvenv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:293\u001b[39m, in \u001b[36mMPNetLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, position_bias, output_attentions, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    285\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    286\u001b[39m     hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    291\u001b[39m     **kwargs,\n\u001b[32m    292\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m     self_attention_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    300\u001b[39m     attention_output = self_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    301\u001b[39m     outputs = self_attention_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add self attentions if we output attention weights\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yogesh\\Documents\\Documentation RAG\\myvenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yogesh\\Documents\\Documentation RAG\\myvenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yogesh\\Documents\\Documentation RAG\\myvenv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:234\u001b[39m, in \u001b[36mMPNetAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, position_bias, output_attentions, **kwargs)\u001b[39m\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    226\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    227\u001b[39m     hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    232\u001b[39m     **kwargs,\n\u001b[32m    233\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m     self_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     attention_output = \u001b[38;5;28mself\u001b[39m.LayerNorm(\u001b[38;5;28mself\u001b[39m.dropout(self_outputs[\u001b[32m0\u001b[39m]) + hidden_states)\n\u001b[32m    242\u001b[39m     outputs = (attention_output,) + self_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yogesh\\Documents\\Documentation RAG\\myvenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yogesh\\Documents\\Documentation RAG\\myvenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yogesh\\Documents\\Documentation RAG\\myvenv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:194\u001b[39m, in \u001b[36mMPNetSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, position_bias, output_attentions, **kwargs)\u001b[39m\n\u001b[32m    191\u001b[39m new_c_shape = c.size()[:-\u001b[32m2\u001b[39m] + (\u001b[38;5;28mself\u001b[39m.all_head_size,)\n\u001b[32m    192\u001b[39m c = c.view(*new_c_shape)\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m o = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    196\u001b[39m outputs = (o, attention_probs) \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;28;01melse\u001b[39;00m (o,)\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yogesh\\Documents\\Documentation RAG\\myvenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yogesh\\Documents\\Documentation RAG\\myvenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yogesh\\Documents\\Documentation RAG\\myvenv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "db = Chroma.from_documents(\n",
    "        total_text_chunks, embeddings, persist_directory=persistent_directory\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma(persist_directory=persistent_directory, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Relevent Documents ----\n",
      "Documents 0:\n",
      " Running the Example  \n",
      "  *  Example Code  \n",
      "  *  Running the UI  \n",
      "  *  UI Code  \n",
      "\n",
      "  1.  Introduction  \n",
      "  2.  Examples  \n",
      "\n",
      "Version Notice\n",
      "This documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \n",
      "# Weather agent\n",
      "Example of PydanticAI with multiple tools which the LLM needs to call in turn to answer a question.\n",
      "Demonstrates:\n",
      "  * tools\n",
      "  * agent dependencies\n",
      "  * streaming text responses\n",
      "  * Building a Gradio UI for the agent\n",
      "\n",
      "In this case the idea is a \"weather\" agent — the user can ask for the weather in multiple locations, the agent will use the `get_lat_lng` tool to get the latitude and longitude of the locations, then use the `get_weather` tool to get the weather for those locations.\n",
      "## Running the Example\n",
      "To run this example properly, you might want to add two extra API keys **(Note if either key is missing, the code will fall back to dummy data, so they're not required)** :\n",
      "  * A weather API key from tomorrow.io set via `WEATHER_API_KEY`\n",
      "  * A geocoding API key from geocode.maps.co set via `GEO_API_KEY`\n",
      "\n",
      "With dependencies installed and environment variables set, run:\n",
      "pipuv\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "## Example Code\n",
      "pydantic_ai_examples/weather_agent.py\n",
      "\n",
      "## Running the UI\n",
      "You can build multi-turn chat applications for your agent with Gradio, a framework for building AI web applications entirely in python. Gradio comes with built-in chat components and agent support so the entire UI will be implemented in a single python file!\n",
      "Here's what the UI looks like for the weather agent:\n",
      "Note, to run the UI, you'll need Python 3.10+.\n",
      "\n",
      "\n",
      "## UI Code\n",
      "pydantic_ai_examples/weather_agent_gradio.py\n",
      "\n",
      "© Pydantic Services Inc. 2024 to present\n",
      "\n",
      "Documents 1:\n",
      " You can sign up and start using Logfire in a couple of minutes.\n",
      "PydanticAI has built-in (but optional) support for Logfire via the `logfire-api` no-op package.\n",
      "That means if the `logfire` package is installed and configured, detailed information about agent runs is sent to Logfire. But if the `logfire` package is not installed, there's virtually no overhead and nothing is sent.\n",
      "Here's an example showing details of running the Weather Agent in Logfire:\n",
      "\n",
      "## Using Logfire\n",
      "To use logfire, you'll need a logfire account, and logfire installed:\n",
      "pipuv\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Then authenticate your local environment with logfire:\n",
      "pipuv\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "And configure a project to send data to:\n",
      "pipuv\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(Or use an existing project with `logfire projects use`)\n",
      "The last step is to add logfire to your code:\n",
      "adding_logfire.py\n",
      "\n",
      "The logfire documentation has more details on how to use logfire, including how to instrument other libraries like Pydantic, HTTPX and FastAPI.\n",
      "Since Logfire is build on OpenTelemetry, you can use the Logfire Python SDK to send data to any OpenTelemetry collector.\n",
      "Once you have logfire set up, there are two primary ways it can help you understand your application:\n",
      "  * **Debugging** — Using the live view to see what's happening in your application in real-time.\n",
      "  * **Monitoring** — Using SQL and dashboards to observe the behavior of your application, Logfire is effectively a SQL database that stores information about how your application is running.\n",
      "\n",
      "### Debugging\n",
      "To demonstrate how Logfire can let you visualise the flow of a PydanticAI run, here's the view you get from Logfire while running the chat app examples:\n",
      "### Monitoring Performance\n",
      "We can also query data with SQL in Logfire to monitor the performance of an application. Here's a real world example of using Logfire to monitor PydanticAI runs inside Logfire itself:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retriever = db.as_retriever(\n",
    "    search_type = \"similarity_score_threshold\",\n",
    "    search_kwargs = {'k':3, 'score_threshold':0.2}\n",
    ")\n",
    "results = retriever.invoke(\"how to build weather agent?\")\n",
    "\n",
    "print(\"\\n---- Relevent Documents ----\")\n",
    "for i, docs in enumerate(results):\n",
    "    print(f\"Documents {i}:\\n {docs.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id='965377c0-455e-4433-b8cb-5971aab014f3', metadata={'chunk_id': 160, 'num_code_blocks': 5, 'type': 'text', 'url': 'https://pydantic.com'}, page_content='Running the Example  \\n  *  Example Code  \\n  *  Running the UI  \\n  *  UI Code  \\n\\n  1.  Introduction  \\n  2.  Examples  \\n\\nVersion Notice\\nThis documentation is ahead of the last release by 21 commits. You may see documentation for features not yet supported in the latest release v0.0.24 2025-02-12. \\n# Weather agent\\nExample of PydanticAI with multiple tools which the LLM needs to call in turn to answer a question.\\nDemonstrates:\\n  * tools\\n  * agent dependencies\\n  * streaming text responses\\n  * Building a Gradio UI for the agent\\n\\nIn this case the idea is a \"weather\" agent — the user can ask for the weather in multiple locations, the agent will use the `get_lat_lng` tool to get the latitude and longitude of the locations, then use the `get_weather` tool to get the weather for those locations.\\n## Running the Example\\nTo run this example properly, you might want to add two extra API keys **(Note if either key is missing, the code will fall back to dummy data, so they\\'re not required)** :\\n  * A weather API key from tomorrow.io set via `WEATHER_API_KEY`\\n  * A geocoding API key from geocode.maps.co set via `GEO_API_KEY`\\n\\nWith dependencies installed and environment variables set, run:\\npipuv\\n\\n\\n\\n\\n## Example Code\\npydantic_ai_examples/weather_agent.py\\n\\n## Running the UI\\nYou can build multi-turn chat applications for your agent with Gradio, a framework for building AI web applications entirely in python. Gradio comes with built-in chat components and agent support so the entire UI will be implemented in a single python file!\\nHere\\'s what the UI looks like for the weather agent:\\nNote, to run the UI, you\\'ll need Python 3.10+.\\n\\n\\n## UI Code\\npydantic_ai_examples/weather_agent_gradio.py\\n\\n© Pydantic Services Inc. 2024 to present')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'type': 'code', 'parent_text_id': 1, 'url': 'https://pydantic.com'}, page_content='...\\nfrombank_databaseimport DatabaseConn\\nimportlogfire\\nlogfire.configure() \\nConfigure logfire, this will fail if project is not set up.\\n\\nlogfire.instrument_asyncpg() \\nIn our demo, DatabaseConn uses asyncpg to connect to a PostgreSQL database, so logfire.instrument_asyncpg() is used to log the database queries.\\n\\n...')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_code_chunks[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='python-mpydantic_ai_examples.weather_agent' metadata={'type': 'code', 'parent_text_id': 160, 'url': 'https://pydantic.com'}\n",
      "page_content='uvrun-mpydantic_ai_examples.weather_agent' metadata={'type': 'code', 'parent_text_id': 160, 'url': 'https://pydantic.com'}\n",
      "page_content='from__future__import annotations as _annotations\n",
      "importasyncio\n",
      "importos\n",
      "fromdataclassesimport dataclass\n",
      "fromtypingimport Any\n",
      "importlogfire\n",
      "fromdevtoolsimport debug\n",
      "fromhttpximport AsyncClient\n",
      "frompydantic_aiimport Agent, ModelRetry, RunContext\n",
      "# 'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured\n",
      "logfire.configure(send_to_logfire='if-token-present')\n",
      "\n",
      "@dataclass\n",
      "classDeps:\n",
      "  client: AsyncClient\n",
      "  weather_api_key: str | None\n",
      "  geo_api_key: str | None\n",
      "\n",
      "weather_agent = Agent(\n",
      "  'openai:gpt-4o',\n",
      "  # 'Be concise, reply with one sentence.' is enough for some models (like openai) to use\n",
      "  # the below tools appropriately, but others like anthropic and gemini require a bit more direction.\n",
      "  system_prompt=(\n",
      "    'Be concise, reply with one sentence.'\n",
      "    'Use the `get_lat_lng` tool to get the latitude and longitude of the locations, '\n",
      "    'then use the `get_weather` tool to get the weather.'\n",
      "  ),\n",
      "  deps_type=Deps,\n",
      "  retries=2,\n",
      ")\n",
      "\n",
      "@weather_agent.tool\n",
      "async defget_lat_lng(\n",
      "  ctx: RunContext[Deps], location_description: str\n",
      ") -> dict[str, float]:\n",
      "\"\"\"Get the latitude and longitude of a location.\n",
      "  Args:\n",
      "    ctx: The context.\n",
      "    location_description: A description of a location.\n",
      "  \"\"\"\n",
      "  if ctx.deps.geo_api_key is None:\n",
      "    # if no API key is provided, return a dummy response (London)\n",
      "    return {'lat': 51.1, 'lng': -0.1}\n",
      "  params = {\n",
      "    'q': location_description,\n",
      "    'api_key': ctx.deps.geo_api_key,\n",
      "  }\n",
      "  with logfire.span('calling geocode API', params=params) as span:\n",
      "    r = await ctx.deps.client.get('https://geocode.maps.co/search', params=params)\n",
      "    r.raise_for_status()\n",
      "    data = r.json()\n",
      "    span.set_attribute('response', data)\n",
      "  if data:\n",
      "    return {'lat': data[0]['lat'], 'lng': data[0]['lon']}\n",
      "  else:\n",
      "    raise ModelRetry('Could not find the location')\n",
      "\n",
      "@weather_agent.tool\n",
      "async defget_weather(ctx: RunContext[Deps], lat: float, lng: float) -> dict[str, Any]:\n",
      "\"\"\"Get the weather at a location.\n",
      "  Args:\n",
      "    ctx: The context.\n",
      "    lat: Latitude of the location.\n",
      "    lng: Longitude of the location.\n",
      "  \"\"\"\n",
      "  if ctx.deps.weather_api_key is None:\n",
      "    # if no API key is provided, return a dummy response\n",
      "    return {'temperature': '21 °C', 'description': 'Sunny'}\n",
      "  params = {\n",
      "    'apikey': ctx.deps.weather_api_key,\n",
      "    'location': f'{lat},{lng}',\n",
      "    'units': 'metric',\n",
      "  }\n",
      "  with logfire.span('calling weather API', params=params) as span:\n",
      "    r = await ctx.deps.client.get(\n",
      "      'https://api.tomorrow.io/v4/weather/realtime', params=params\n",
      "    )\n",
      "    r.raise_for_status()\n",
      "    data = r.json()\n",
      "    span.set_attribute('response', data)\n",
      "  values = data['data']['values']\n",
      "  # https://docs.tomorrow.io/reference/data-layers-weather-codes\n",
      "  code_lookup = {\n",
      "    1000: 'Clear, Sunny',\n",
      "    1100: 'Mostly Clear',\n",
      "    1101: 'Partly Cloudy',\n",
      "    1102: 'Mostly Cloudy',\n",
      "    1001: 'Cloudy',\n",
      "    2000: 'Fog',\n",
      "    2100: 'Light Fog',\n",
      "    4000: 'Drizzle',\n",
      "    4001: 'Rain',\n",
      "    4200: 'Light Rain',\n",
      "    4201: 'Heavy Rain',\n",
      "    5000: 'Snow',\n",
      "    5001: 'Flurries',\n",
      "    5100: 'Light Snow',\n",
      "    5101: 'Heavy Snow',\n",
      "    6000: 'Freezing Drizzle',\n",
      "    6001: 'Freezing Rain',\n",
      "    6200: 'Light Freezing Rain',\n",
      "    6201: 'Heavy Freezing Rain',\n",
      "    7000: 'Ice Pellets',\n",
      "    7101: 'Heavy Ice Pellets',\n",
      "    7102: 'Light Ice Pellets',\n",
      "    8000: 'Thunderstorm',\n",
      "  }\n",
      "  return {\n",
      "    'temperature': f'{values[\"temperatureApparent\"]:0.0f}°C',\n",
      "    'description': code_lookup.get(values['weatherCode'], 'Unknown'),\n",
      "  }\n",
      "\n",
      "async defmain():\n",
      "  async with AsyncClient() as client:\n",
      "    # create a free API key at https://www.tomorrow.io/weather-api/\n",
      "    weather_api_key = os.getenv('WEATHER_API_KEY')\n",
      "    # create a free API key at https://geocode.maps.co/\n",
      "    geo_api_key = os.getenv('GEO_API_KEY')\n",
      "    deps = Deps(\n",
      "      client=client, weather_api_key=weather_api_key, geo_api_key=geo_api_key\n",
      "    )\n",
      "    result = await weather_agent.run(\n",
      "      'What is the weather like in London and in Wiltshire?', deps=deps\n",
      "    )\n",
      "    debug(result)\n",
      "    print('Response:', result.data)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "  asyncio.run(main())' metadata={'type': 'code', 'parent_text_id': 160, 'url': 'https://pydantic.com'}\n",
      "page_content='pipinstallgradio>=5.9.0\n",
      "python/uv-run-mpydantic_ai_examples.weather_agent_gradio' metadata={'type': 'code', 'parent_text_id': 160, 'url': 'https://pydantic.com'}\n",
      "page_content='from__future__import annotations as _annotations\n",
      "importjson\n",
      "importos\n",
      "fromhttpximport AsyncClient\n",
      "frompydantic_ai.messagesimport ToolCallPart, ToolReturnPart\n",
      "frompydantic_ai_examples.weather_agentimport Deps, weather_agent\n",
      "try:\n",
      "  importgradioasgr\n",
      "except ImportError as e:\n",
      "  raise ImportError(\n",
      "    'Please install gradio with `pip install gradio`. You must use python>=3.10.'\n",
      "  ) frome\n",
      "TOOL_TO_DISPLAY_NAME = {'get_lat_lng': 'Geocoding API', 'get_weather': 'Weather API'}\n",
      "client = AsyncClient()\n",
      "weather_api_key = os.getenv('WEATHER_API_KEY')\n",
      "# create a free API key at https://geocode.maps.co/\n",
      "geo_api_key = os.getenv('GEO_API_KEY')\n",
      "deps = Deps(client=client, weather_api_key=weather_api_key, geo_api_key=geo_api_key)\n",
      "\n",
      "async defstream_from_agent(prompt: str, chatbot: list[dict], past_messages: list):\n",
      "  chatbot.append({'role': 'user', 'content': prompt})\n",
      "  yield gr.Textbox(interactive=False, value=''), chatbot, gr.skip()\n",
      "  async with weather_agent.run_stream(\n",
      "    prompt, deps=deps, message_history=past_messages\n",
      "  ) as result:\n",
      "    for message in result.new_messages():\n",
      "      for call in message.parts:\n",
      "        if isinstance(call, ToolCallPart):\n",
      "          call_args = (\n",
      "            call.args.args_json\n",
      "            if hasattr(call.args, 'args_json')\n",
      "            else json.dumps(call.args.args_dict)\n",
      "          )\n",
      "          metadata = {\n",
      "            'title': f'🛠️ Using {TOOL_TO_DISPLAY_NAME[call.tool_name]}',\n",
      "          }\n",
      "          if call.tool_call_id is not None:\n",
      "            metadata['id'] = {call.tool_call_id}\n",
      "          gr_message = {\n",
      "            'role': 'assistant',\n",
      "            'content': 'Parameters: ' + call_args,\n",
      "            'metadata': metadata,\n",
      "          }\n",
      "          chatbot.append(gr_message)\n",
      "        if isinstance(call, ToolReturnPart):\n",
      "          for gr_message in chatbot:\n",
      "            if (\n",
      "              gr_message.get('metadata', {}).get('id', '')\n",
      "              == call.tool_call_id\n",
      "            ):\n",
      "              gr_message['content'] += (\n",
      "                f'\\nOutput: {json.dumps(call.content)}'\n",
      "              )\n",
      "        yield gr.skip(), chatbot, gr.skip()\n",
      "    chatbot.append({'role': 'assistant', 'content': ''})\n",
      "    async for message in result.stream_text():\n",
      "      chatbot[-1]['content'] = message\n",
      "      yield gr.skip(), chatbot, gr.skip()\n",
      "    past_messages = result.all_messages()\n",
      "    yield gr.Textbox(interactive=True), gr.skip(), past_messages\n",
      "\n",
      "async defhandle_retry(chatbot, past_messages: list, retry_data: gr.RetryData):\n",
      "  new_history = chatbot[: retry_data.index]\n",
      "  previous_prompt = chatbot[retry_data.index]['content']\n",
      "  past_messages = past_messages[: retry_data.index]\n",
      "  async for update in stream_from_agent(previous_prompt, new_history, past_messages):\n",
      "    yield update\n",
      "\n",
      "defundo(chatbot, past_messages: list, undo_data: gr.UndoData):\n",
      "  new_history = chatbot[: undo_data.index]\n",
      "  past_messages = past_messages[: undo_data.index]\n",
      "  return chatbot[undo_data.index]['content'], new_history, past_messages\n",
      "\n",
      "defselect_data(message: gr.SelectData) -> str:\n",
      "  return message.value['text']\n",
      "\n",
      "with gr.Blocks() as demo:\n",
      "  gr.HTML(\n",
      "\"\"\"\n",
      "<div style=\"display: flex; justify-content: center; align-items: center; gap: 2rem; padding: 1rem; width: 100%\">\n",
      "  <img src=\"https://ai.pydantic.dev/img/logo-white.svg\" style=\"max-width: 200px; height: auto\">\n",
      "  <div>\n",
      "    <h1 style=\"margin: 0 0 1rem 0\">Weather Assistant</h1>\n",
      "    <h3 style=\"margin: 0 0 0.5rem 0\">\n",
      "      This assistant answer your weather questions.\n",
      "    </h3>\n",
      "  </div>\n",
      "</div>\n",
      "\"\"\"\n",
      "  )\n",
      "  past_messages = gr.State([])\n",
      "  chatbot = gr.Chatbot(\n",
      "    label='Packing Assistant',\n",
      "    type='messages',\n",
      "    avatar_images=(None, 'https://ai.pydantic.dev/img/logo-white.svg'),\n",
      "    examples=[\n",
      "      {'text': 'What is the weather like in Miami?'},\n",
      "      {'text': 'What is the weather like in London?'},\n",
      "    ],\n",
      "  )\n",
      "  with gr.Row():\n",
      "    prompt = gr.Textbox(\n",
      "      lines=1,\n",
      "      show_label=False,\n",
      "      placeholder='What is the weather like in New York City?',\n",
      "    )\n",
      "  generation = prompt.submit(\n",
      "    stream_from_agent,\n",
      "    inputs=[prompt, chatbot, past_messages],\n",
      "    outputs=[prompt, chatbot, past_messages],\n",
      "  )\n",
      "  chatbot.example_select(select_data, None, [prompt])\n",
      "  chatbot.retry(\n",
      "    handle_retry, [chatbot, past_messages], [prompt, chatbot, past_messages]\n",
      "  )\n",
      "  chatbot.undo(undo, [chatbot, past_messages], [prompt, chatbot, past_messages])\n",
      "\n",
      "if __name__ == '__main__':\n",
      "  demo.launch()' metadata={'type': 'code', 'parent_text_id': 160, 'url': 'https://pydantic.com'}\n"
     ]
    }
   ],
   "source": [
    "for i in total_code_chunks:\n",
    "    if i.metadata['parent_text_id']==160:\n",
    "        with open(\"code.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"New Document \\n\")\n",
    "            f.write(i.page_content)\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-groq\n",
      "  Downloading langchain_groq-0.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.47 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from langchain-groq) (0.3.49)\n",
      "Collecting groq<1,>=0.4.1 (from langchain-groq)\n",
      "  Downloading groq-0.20.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (2.11.1)\n",
      "Requirement already satisfied: sniffio in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (4.13.0)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.47->langchain-groq) (0.3.19)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.47->langchain-groq) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.47->langchain-groq) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.47->langchain-groq) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.47->langchain-groq) (24.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.47->langchain-groq) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-groq) (3.10.16)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-groq) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-groq) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-groq) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.0 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (2.33.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-groq) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yogesh\\documents\\documentation rag\\myvenv\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-groq) (2.3.0)\n",
      "Downloading langchain_groq-0.3.1-py3-none-any.whl (15 kB)\n",
      "Downloading groq-0.20.0-py3-none-any.whl (124 kB)\n",
      "Installing collected packages: groq, langchain-groq\n",
      "Successfully installed groq-0.20.0 langchain-groq-0.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "load_dotenv()\n",
    "model = ChatGroq(\n",
    "    model=\"llama3-8b-8192\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code provided is an example of a weather agent using Pydantic AI, which is a Python library for building conversational AI applications. The code defines a weather agent that can answer questions about the weather in different locations.\n",
      "\n",
      "Here's a breakdown of the code:\n",
      "\n",
      "1. The first part of the code defines a `Deps` class that represents the dependencies required by the weather agent. This includes an `AsyncClient` object, a weather API key, and a geocoding API key.\n",
      "2. The `weather_agent` class is defined, which represents the weather agent itself. It has a `tool` method that defines the tools that the agent can use to answer questions. In this case, the tools are `get_lat_lng` and `get_weather`.\n",
      "3. The `get_lat_lng` tool is used to get the latitude and longitude of a location. It takes a `location_description` argument and returns a dictionary with the latitude and longitude.\n",
      "4. The `get_weather` tool is used to get the weather at a location. It takes a `lat` and `lng` argument and returns a dictionary with the weather information.\n",
      "5. The `main` function is defined, which runs the weather agent and prints the result.\n",
      "6. The second part of the code defines a Gradio UI for the weather agent. It uses the `gradio` library to create a chatbot that can interact with the user and display the results of the weather agent.\n",
      "\n",
      "The code is quite long, so I'll highlight the main points:\n",
      "\n",
      "* The weather agent uses two tools: `get_lat_lng` and `get_weather`.\n",
      "* The `get_lat_lng` tool gets the latitude and longitude of a location.\n",
      "* The `get_weather` tool gets the weather at a location.\n",
      "* The weather agent runs the tools in sequence to answer the user's question.\n",
      "* The Gradio UI allows the user to interact with the weather agent and display the results.\n",
      "\n",
      "Note that this code is just an example and may require modifications to work with your specific use case.\n"
     ]
    }
   ],
   "source": [
    "query = \"what is code for weather agent in pydantic?\"\n",
    "\n",
    "retriever = db.as_retriever(\n",
    "    search_type = \"similarity_score_threshold\",\n",
    "    search_kwargs = {'k':2, 'score_threshold':0.2}\n",
    ")\n",
    "\n",
    "results = retriever.invoke(query)\n",
    "\n",
    "text = \"\"\n",
    "parent_doc = \"\"\n",
    "for i, docs in enumerate(results):\n",
    "    text += docs.page_content\n",
    "    if i==0:\n",
    "        parent_doc = docs.metadata['chunk_id']\n",
    "    \n",
    "code_content = \"\"\n",
    "for i in total_code_chunks:\n",
    "    if i.metadata['parent_text_id']==parent_doc:\n",
    "        code_content += i.page_content\n",
    "        \n",
    "context = (\n",
    "    \"Here are some documents that might help answer the question:\"\n",
    "    + query\n",
    "    + \"\\n\\nRelevant Text from pydantic documentation:\\n\"\n",
    "    + text\n",
    "    + \"\\n\\nRelevant code\"\n",
    "    + code_content\n",
    "    + \"The code might contain syntax errors so correct it as needed\"    \n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are python expert and you can we understand errors in code and able to provide right syntax. Your task is to go through code and text from documentation you are provided with as content and able to answer user's question about code documentation. Help him explore documentation easily and explain the code you are provided with\"),\n",
    "    HumanMessage(content=context)\n",
    "]\n",
    "\n",
    "result = model.invoke(messages)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1416"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(code_content.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_input = (\n",
    "    \"Here are some documents that might help answer the question: \"\n",
    "    + query\n",
    "    + \"\\n\\nRelevant Documents:\\n\"\n",
    "    + \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "    + \"\\n\\nPlease provide a rough answer based only on the provided documents. If the answer is not found in the documents, respond with 'I'm not sure'.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\"system\", \"You are python expert and you can we understand errors in code and able to provide right syntax. Your task is to go through code and text from documentation you are provided with as content and able to answer user's question about code documentation. Help him explore documentation easily and explain the code you are provided with\"),\n",
    "    ('human', )\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
